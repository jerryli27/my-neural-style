It was on the last day of 2016 that I discovered the PaintsChainer blog. At that time I was still working on the Neural style project but I was almost at the end of that project and wanted to start something new. I was inspired by the "Neural Doodle" project and was thinking of some project where I can give some simple guides as input and the output would be some good looking anime-like images. The PaintsChainer project attracted me because I never have imagined that the network can learn from simple sketches, interpret its semantic meanings, and color the output accordingly. The result looked really good.

I thought the project would be easy. Because after all it's just like the "Neural Style" project where someone came up with an idea, they share their network structure, and I reimplement it using Tensorflow. But it turned out to be a long project that I've been working on for almost 3 months now.

The first difficulty I met was that the author "Taizan"-san was really unclear about the technical details in his blog. He was not clear for how he trained the network, what setting he used, and where the dataset came from. Therefore first I was stuck with having little training data and had to believe that the sketch generation method he mentioned in the blog was exactly what he used. After I got enough training data, I was then stuck on having outputs that looks like old pictures -- most of the objects are colored black and white. Taizan-san had mentioned such problem in his blog so I followed his suggestion and added the hint and the GAN. I at first was not sure how I should train the thing. That is, should I train GAN along with l2 loss or should I train l2 loss then GAN? At that time I did not know about pix2pix or Scribbler. I thought this work was just by itself.

Another huge difficulty I met was my own code. Although I wrote it mostly by myself, it was mostly adapted for the neural style project and when I took the same code and use it for PaintsChainer reimplementation, there are some issues because the overall layout is just different. (For example, originally the l2 loss was not per-pixel but sum of all pixels, and there were just too many ambiguities in the code.)

Therefore after many failed attempts on using my own code, I started to look into other methods. At that time, the author did not upload his github repo yet and I thought that by looking into other people's similar projects, I can get some inspiration. I did, and I tried to implement one of the techniques mentioned in the papers, but that did not help either.

At this point I should mention that on my old code, the training was really slow. It usually takes like 4-5 seconds per batch. That was because I was not parallelizing the image loading code with the training code. I did not thought of the tensorflow pipeline because I did not know that it automatically parallelize that process, making training much much faster. That was one of the major bottlenecks that I was facing. I did not realize that I can look up such problems. I just thought it was normal, because that was how I trained things in the past...

On 1/30 when I was struggling to understand what went wrong (I can't even colorize the bw images using caffe code from git repo that others have used to color real-life bw images.), I found out that Taizan-san released an online-demo with the source code. I was super excited, thinking that finally I can reimplement it in Tensorflow, because I now have the source code. Only later when I talked to Taizan that I found out it was not exactly how he trained the network. He changed things around several times during the training, and since he cannot release the dataset he was using or the training settings, I have no way to reproduce his result from the github source code.

Then a week later I found out about pix2pix, and it was working better than any other implementations I had, including the chainer code. So that's how I get to the current repo I'm using. With pix2pix, I found out about the tensorflow data loading pipeline and that greatly increased my speed of debugging. Reimplementing the hint update under tensorflow pipeline took some time. Implementing WGAN also took some time and before I realized, I was spending 2 months on it already. The first month was mostly wasted because of the slow debugging process (I still do not know where the bug was in my recycled code.). The second month was spent adapting the pix2pix code and exploring random other projects that sounded interesting.

The last two problem that I encountered up to this point was: using the sketches I previously generated, I was unable to make it work on the real sketches on pixiv. I believe that was because the two sketches were fundamentally different and I worked on getting a dataset with sketch-colored image pairs. This problem is not yet entirely solved yet. The old sketch has more information than the new one, so the result was better. The downside is it cannot work on real sketches. The new sketch works on real sketches, but it contains less information and therefore produced worse outputs. I am now working on trying to combine the two.
The other problem was that the output was not colorful enough. So far the most successful attempt to solve this problem is to increase the GAN weight, but I think it only works when I have a small GAN weight at first, then increase it. I have yet to prove this theory, but one previous experiment I ran with larger GAN weight just resulted in messy colors.
Another small problem was with the training dataset. I found that by cleaning it properly I can get results with much higher quality. But how to clean it is a tricky thing...
