This is the notebook for recording experiment results, discoveries, and next steps.

It is crucial to have a low enough learning rate. If you have a high one and decrease it over time, it still won't work.

MRF is confirmed not to work with feed forward neural network. But mrf is not really absolutely necessary. I might need
it for semantic related masking stuff. But it has also been shown that masking works with gram loss and feed forward network.

Here are two unsolved problems:
The size of the features/styles. The image genearted when input is 100 x 100 versus 1000 x 1000 is drastically different.
The generated image is limited to the structure of the original painting. In art, you can draw something larger or
smaller than usual to achieve some effect. There's no such thing here because the loss is localized
(You can't just change the absolute position of objects in content image.)


Two ideas: artists "zoom in" to a spot when they want to paint it in more detail. Is it possible to do so in our architecture?

Second idea: if the generator network can generate styled images, why can't it be used to reformat the feature layers?
Turn the feature layers into something that look like other artforms (without loosing too much originality)
and generate ... But how's that different from just use the generator network to generate the image? Not really

I want something other than difference squared loss. I thought about nearest neighbor loss of the normalized feature
layers. That one is invariant to translation.


Trying two things now: one is to directly observe what is going on in each loss function for each feature layer.
Another is to feed multiple styles into the network while modifying the same parameters.

simply feed 15000 shirobako images as style didn't work.

I tried the mask again using hand-segmented shirobako mask (mrf loss). Surprisingly it's sort of working. Maybe I don't need the
content loss. I can just create a drawing from scratch. Maybe that would be too hard.
One more difficulty if I choose to go on this direction. I don't have hand-segmented masks for me to train.
Even if I do, I don't know how to incorporate say 10 masked style images together. Simple nearest neighbor matching on
the 10 images would require 10x memory in the gpu.

One idea for automatically label images: start with example image and a hand-labeled mask. Now for each input
we compute the nearest neighbor of say conv4-2 (any high level layer) and assign the nearest neighbor's mask
to the input layer. Then add the constraint that nearby pixels should preferrably have the same label.
Then do deconv to get pixel-wise label for the original image. (Sounds like a plan, but this requires the
high level layers of the style and content to be similar, which may well not be the case. 20161122)

Texture and placement of objects are two different things. Texture is given a mask, how to fill in the colors
so that the style loss is the least. That is already solved. Placement of object is another issue. How to
place objects relative to each other so that it is the most probable.

I tried the nn loss. NN is not differentiable so it has no gradient. I should've realized earlier. Now I probably have
to get around this problem.

One thing is, the reason why mrf can perform better is because it has those patches that captures info about nearby
cells. Gram simply multiply them together and add all cells. There's no interaction between a cell and the cell on its
side. On the otherhand, the mrf is expensive to calculate because of the nn step.

After some thinking, it comes down to knowing the feature layers first. (like each of the conv layers, when they're
activated and compare two inputs, content and style, see when their conv layers look similar.)

Adversarial generative network may be worth investigating into. But don't do that just yet. I still have to finish up
this experiment.

We can make the additional semantic masks learnable. Just add a loss for too much deviation from original (more
complicated loss can be added later)

Things learned from the experiment: the conv layer won't look close to the original image. So we can't hope to morph
one image into another in the conv layers. Also, making semantic masks learnable will need further refinement.

I thought maybe finding the spatial correlation of semantic layers might be helpful. Now it is only finding the
correlation in-place (that is, the same pixel times the same pixel in another layer). What happens if we shift it by
say half of the width and find the correlation? The hope is that , for example we have two eyes, one on the left
and one on the right. By finding the correlation after shifting, we may find: ah whenever there's an
eye here, there will be on on its right with distance half screen away. That's my hope.


I was correct that correlation after shifting one layer encodes the relative positional information. Now the problem is
1. It was not perfect. I hope the problem can be solved after adding a content loss.
2. It was spatially too similar to the style image. ie. no shifting etc. There was some shift for subfeatures, like
the mouth was on the right first and slowly came to the center. But overall the head position was still the same, the
position of everything was the same as in the style. This is not what we want. We just want to modify the content image
a little so that the eyes become bigger, or the nose become less noticeable and things like that.
I want to have two very simple images as input and see how it goes.

Not going well so far.

Side projects during thxgiving:
Run multi-style feed forward network.
Shit.. I realized it actually doesn't make sense in the current frame work. I have to separate each style by itself
instead of feeding them in as batches. ( Otherwise I can't set the image placeholder so I can't train scale and offset
individually. I'll try to fix this tomorrow by going back to the master branch and merge the two..
I don't know if I should keep the batch style... Maybe I should.

Tested overnight on claude monet's paintings. Now I'm sure I can't just merge styles like that. It doesn't work.

Other future directions: feed forward neural doodle.


As I think of it, maybe we don't need the content image at all? There are many things that are hard to find in real life
but exist in drawings. I know that a feed forward network with masks can be trained as long as I have enough training
data. The only problem is where to get that semantically labeled training data. I can get that from 3d model... I think
I can look into that direction. No content image. Content loss comes directly from thousands of style images. Details
of how content loss can come from those may be filled in later but I think this is the right direction. We don't have
to worry about distorting content image into the style image, which is a big challenge.

Before that, let me modify the current code so that it can accept only semantic style images and semantic masks and
learn them using feed-forward nn. the loss... we can use content loss or style loss. try both I guess. I need to
copy the previous code into a new file because the change will make the code incompatible with the previous version.
(since there no longer need to be a content image). Actually, I can use the same file... Since I may use that framework
to learn auto generating textures.

Basically I will implement feed forward neural doodle, but better than what they've done. I need not only rgb mask, but
any number of masks for each style image.

2016/12/09
I implemented fast neural doodle. Testing it. It is running super slow for some reason... I don't get it. I should test
running the normal version... (without masks and doodles.)

2016/12/10
Fixed some bugs, but the thing is still super slow, even if I don't turn on masks. Was it like that from the very
beginning? I think so because it takes the whole night to train around 20000 rounds, so it's like one round per second.

Also the loss for style is unnaturally high only after I apply masks. I found that was because I was doing dot product
so each number is dotted with 255. lol.

But the result was still unsatisfactory.

2016/12/11
What can I do that will have some originality instead of repeating what people has done? What is the limit of the
current system?

If working on masked images:
    You must first have a mask, whether it is based on real images or not.
    The mask cannot be too detailed, or it will use up too much memory (grows linearly) and time it takes to create
    such a mask is prohibitive.

If not working on masked images:
    The model clearly does not have an idea of scale. If you train it on a 256x256 image, you can't just put that
    on a 1024x1024 image because the patterns created will be too small.
    It has no understanding of the relative size/location of objects (unless the two objects are close enough that they
    become in the same cell after convolution. That is a problem for vgg, but since we're using gram matrix, the
    problem is also applicable in style generation. For this reason, it is still impossible to tell it to "paint a cat"
    and expect it to just work.
    It also has no idea about different style between different objects. See comparison between mask and no mask.
    It

--
The thing is not working, at least not on johnson network... I don't know what's going on, but my guess is that johnson
might not be the best choice for texture generation with masks. I'll try the pyramid network...

Planning to try pyramid network
read blog and source code on feed forward network and see why mine did not work.
The johnson feed forward network is definitely working. It just takes a while to train. It get's faster(to a tolerable
level) when I decrease the batch size from 8 to 1. I'll try that on the masks.

2016/12/12
on going work:
Trying pyramid network without masks. Just normal content input.
Planning to try johnson network with style generation only. No masks. -- It worked!

I'll try to replace input to "johnson network with masks" with random
noise instead of the masks themselves. It probably will get better after intensive training... Usually generating image
with random content/mask takes a while to train. Ok there's definitely something wrong with the masks. The masks
changed their meanings in the middle of training, so we're definitely loading them wrong. If I'm lucky, it might just
be the output but not the training (judging from the quality of generated images).
I tested the output. The output's order is correct. So it's the training process... But I don't get what part went
wrong. I tested getting the files. That part was correct. ... Plan: debug this. I don't know what it will look like
after more training, but now it's not so satisfactory yet.
It is interesting to see how each mask acts. when value of a mask is 0, it doesn't mean that it will be black. Instead
since it indicates that it is not a certain mask, it becomes the color of other masks.
But when only one mask has all 255s and the rest 0s, the result just becomes the same as no masks for some reason..
I don't understand why, but it seems the network is doing something wierd.


Also working on finding out why pyramid generator did not work so well. The image generated was just too blue...
I first thought it might be that I forgot to preprocess the image or something like that. But I checked and it's fine.
So I thought it might be the generator net. I want to test that with simple "style-only" task. That failed ... old
problem.

2016/12/13
Re-reading the feed-forward paper and trying to figure out why my network did not work.
Differences:
    1. They used a uniform noise distribution instead of the whatever distribution numpy offers. (fixed, changed all to
    uniform random)
    2. according to the original paper, normalization is done also before concatenation layer. (while in another paper
    it says otherwise. Personally I think it is not necessary to have normalization there, but I want to at least
    reproduce the result of the author. (fixed)
    3. Initializations (fixed)

Humm... Where's the problem???
Maybe it's not in the generator network??
Alright I give up... I don't know why it doesn't work. Pushing this off to tomorrow.

Task 2: Johnson network with masks:

Task 3: improving slow version.
    I was comparing the texture generation network (with and without mrf, with and without masks). I found that the key
    lies in the loss function, ***so if I want to make something new, I need to invent a new loss function. ***

    I tested style generation (no content image) for mrf loss and gram loss. It turns out that both are effective,
    although mrf loss is better at preserving local structure and information. gram loss knows little about the
    information in the current pixel compared to say five pixel away. mrf handles that pretty well (maybe too well, I
    suspect that it is most of the time just copying instead of learning).

    I then tested semantic masks
    mrf failed when I simply append the mask to each layer. I think it is because the magnitude of the image (0~255)
    and the vgg layers are different, so nearest neighbor didn't work so well (that's why the author said it is ok to
    simply pass the mask into the vgg as an image. Since this will guarantee that they have the same magnitude as
    all other feature layers.
    I tried to dot the feature layers with the mask in mrf. That also worked, but then the gram matrix and the nn
    convolution becomes pretty large. Both make sense though. Dotting means that: each semantic mask is independent to
    other layers: the style of the eye has nothing to do with the style of say the hair. On the other hand, appending
    means that: it is still possible in nearest neighbor that: although one pixel belongs to another semantic mask,
    it can use the style of the other semantic mask if vgg thinks it is similar enough to another pixel... Considering
    the inaccuracy of human labeling and the ram, the second one is more preferrable. #Plan: I will make the change
    and basically delete resizing. (before that, I need to test if appending is really preferrable to the dot product.)

    Another problem is: Since we can't use nn in feed-forward nn (It's probably too hard for a generator network to
    learn nn), is there a better loss function or any modification I can make to the gram loss function? I came up with
    the "gram stack" and showed that it can reconstruct local structures to a good extent, given enough stacks. Can I
    use that? (like have stack = 5, 2 left, 1 center and 2 right?)

1. Fix resizing/dot/feed into network and append. Compare the three throughly.
2. Find alternative loss function that gets the good side from both gram and mrf.

Tasks not done yet:
1. pyramid generator... still doesn't work. it should be something simple that I'm missing
2. Johnson network does not work with masks.
3. A better loss function.

2016/12/14

1. Johnson network:
    I looked at the network itself. THe first layer has a filter of size 9. That is huge. That's probably why the
    result looks so blurry. And that size is the limit to the size of the features. Notice the image it generated
    (without masks) when I train it for texture generation. The features are always of a certain size, not larger nor
    smaller. That is probably one limit of the network.
    I also guessed why the network no longer works when I added masks. The input I used was noise + masks, basically
    appending the masks as additional features. That didn;t work and I guessed that it is probably too hard for the
    network to learn the dot product. So I thought I can just do it for the network. So now the input would be
    dot(noise, masks) and the dimension would be W x H x (3 * num_masks). Let me try if that works.

    So far it looks a lot better than before... With more training it can probably get even better.
    Uh. it's sort of working but not really. It just looks ugly as hell and the houses layer doesn't look like houses
    at all.

2. Alternative loss function.
    I modified the "gram_stack" so that it now calculates the gram between the original layer and the shifted layer
    where the shift is applied within a window (so if it's a 3x3 window, there will be 9 gram matrices)
    This works, to some extent. The image is definitely better than before. Now we can clearly see the houses and the
    doors whereas before it was just a blur. The feature size is limited to the size of the shift though...

    Also a side note, this might be way down the road, but how do we make sure the sky looks consistent? I mean the
    swirls are going everywhere in different directions, but in original graph they are consistent in one direction.
    Should I do something like add tv loss to the feature layers? ( Not just the final output) But Again that might not
    be a good idea if the feature layers are more like an edge detector (it would have huge tv loss). It's worth trying
    though.

2016/12/15
1. Johnson network:
    The network just doesn't behave like what it should be doing. That's why the style generated looked so poor. When I
    feed in different masks, the output doesn't look like it contains only the content that is supposed to be in that
    mask. Also the output changes when I add say a bunch of zeros in the upper left corner. As I mentioned before, it
    seems like the mask is just remembering whatever is not inside the mask. That's why when I give it zeros, it will be
    filled with the texture not in the mask, instead of just blank.

2. Alternative loss function.
    I compared the style generated by mrf versus my new loss function. It seems that mine is overfit on the small
    details. I thought maybe by changing the style layer that we're fitting it to, the result can get better.
    I did it. By changing the style layers to the one mrf uses (relu3_1 and relu4_1), and by changing shift size to 4,
    I got a much better style regeneration than the previous ones. It was able to recover some of the objects in the
    original style image. One small problem is: the layout seems to resemble the layout of the original style image
    a little (not noticeable unless you pay attention to it). Like the buildings are always on the left side,
    resembling the original style image.
    Tried to fix normalization but ended up using the current one.. It works good while the other ones don't.

    I want to try this on content images. Style/texture regenration was successful.
    Oh shit I was using the wrong masks.. I also need to increase the weight of the styles.

3. Pyramid genration network.
    It seems like the author did not use the pyrammid generation network in his github. He used a conv-deconv model,
    similar to Johnson, but with small modifications. He added a skip layer and appended those after each conv step.
    He also added a lot of noise layers along with each skip layer. The downsampling layers have 8 layers but the
    noise layers have 16...

    That one is guaranteed to work, but I still would like to figure out why mine didn't. Let me try to increase the
    number of noise layers... Don't know if that will help.
2016/12/16
Alternative loss function
The loss with respect to the shift size is kind of wierd...
6:1.80673e+08
5:1.41603e+08
4:1.05345e+08
3:7.28106e+07
2:4.48457e+07
1:2.26033e+07
I guess if we keep the shift size constant, it's not a big problem.
Yesterday when I compared my new loss with the old one, I think I had some bugs
and I should compare them again.

My implementation results in image that is too detailed. (No interesting features on
a grand scale, just local ones). I then tried to change the lr and it worked... I don't know why
but the features now became of a larger scale. What I mean is the brushes looks larger and more
coherent. This may be just an accident, because I don't think the system is designed that way. I need
to find a way to make coherent brushes.

I tested my new loss function. Although there is noticeable difference when I do texture generation,
when I added the content image, there was essentially no difference between the new loss function and
the gram loss. Maybe that is because we're using layer 1-5 and layer 1 is just too detailed? (So if we fit a layer that
looks almost like the original image, then the result would look like the original image as well...) I'll try the
layer 3-5.
... Nah it is just hard to tell the difference. The old problem still exists and the variance in the result each time
is just too large. The biggest problem is still the inability to recognize objects and paint them accordingly. If I
want to paint the sky in some way, I don't want to paint the ocean in the same way as well (unless I want to do so
intentionally) because now the person will have no idea where the boundary is. But that can be solved by adding masks,
and the more detailed the mask is, the better the painting would be. One question would be: what if the thing i want to
paint is not in the style image? Using neural network sort of solves that image but using masks won't. Also relying on
a single image almost guarantees that you can't draw something that is not in that image.
So the solution I can think of:
1. Use a hierarchy of objects. If I want to paint a human, but there is no human in the kb, we find the next closest
level: mammals, animals etc and paint the human like that. But that would require a kb of how to draw different things
as well as their semantic relations.
2. Enable training on lots of masked images, not just one. This is essentially building a kb. But I don't know how to
even train one feed-forward network on one masked image yet... So this now becomes my priority...


Feed-forward texture generation with masks.
    I read the source code in torch and compared against my code. One big difference I found was that it had one gram
    matrix for each coloring. I had one huge gram matrix for all the coloring. Theoretically mine should not affect the
    result (because no color overlaps so most of the gram matrix would be 0), but I'm going to rewrite mine just in
    case.

    Link to the torch source code: https://github.com/DmitryUlyanov/fast-neural-doodle/blob/master/fast_neural_doodle.lua

2016/12/17
1.Testing gramian_with_mask
    It works fine in back-propagation mode.
    The feed forward is still not working after training for 5000 rounds. This is fustrating...
    check the result for texture generation without masks and contintue training? It seems to be working. More
    training will result in better result.
    I will code the new feed-forward network and hope that saves the day.
2. New loss function.
    Ok why my loss function didn't work? It worked perfectly without masks. Then it starts to break down a little after
    I added the masks. Then it completely becomes the old gram loss after I added the content images. What happened in
    between?
3. Refactoring the code: Went till line 224 in n_style_feedforward_net.


2016/12/18
1. Testing gramian with mask
    Continued training from yesterday.
    Hum.. the network also sort of failed... The result just looks super wierd.
    Trying to repeat the experiment using content images...
    Ok The skip_noise_4 network is working. At least it is working when it generates the style without masks or content
    images. *

    Conclusion:
        johnson network sort of works to generate textures. The boarder looks wierd though and the features are small,
        probably because I trained using 128x128 instead of 256x256.

        skip_noise_4 generate textures successfully (tested two times by accident)
2. New loss function.

2016/12/19
1. Testing feed-forward with mask
    I don't understand what's going on. The new skip_noise_4 network now had the same type of errors as johnson. The
    masks seem to switch from training to training. The network applies this texture to this network during this round
    and the other texture to the other network in the next round... I'm really lost and I don't understand what is
    going on.

    Tried to have input mask be constant... Still the texture generation didn't work. That doesn't make sense...

    Style generation seems to work. But it looked different from the ones online. Maybe the style loss was too low.
    I set it to 25 because 100 seemed too high. Plan: test using style weight 50.

    I guess it's time to download some repos online and see what they have.

    No online repo found on fast neural doodle, except for the Dmitry one. I am planning to read his whole source code.
    I found that the noise he feeds in has three parts: mask, mask times uniform noise, and noise. I only fed in noise.
    That is fed into the network's very first layer.. He also have some additional noises in skip layers. I'm not sure
    how those work yet. I assume that they're all uniform random.

    Still didn't work after 30000 iterations... GG. There;s something else to the network.


2. New loss function
    No work done.

3. Back prop MRF with style as real images.
    According to https://github.com/jcjohnson/neural-style/wiki/Similar-to-Neural-Style it seems to work well when
    style is real images. Trying that... That worked pretty well when the style weight is kept low and when the content
    and the style image have similar contents (Mostly because then you can't tell if I replaced one object in the
    content with another object in the style image. Again, mrf does mostly copying but not learning.)

    Trying to stylize some other image. I found that there were some wierd horizontal lines in the generated image.
    I may need to fix that. I first thought that it was because of the resized style image. Then I resized them
    manually and the problem was still there. So it must be caused by something else. Maybe the dimensions were not
    divisible by 2?

4. Future direction
    I was thinking about what to do next. I thought before that I can train a feed-forward network on multiple style
    images so that it can draw a wider range of stuff that might not be present in one single style image. But then I
    realized that there are limits to the masks. I mean you can say "A man facing the sun" is a kind of mask and "a
    man facing away from the sun" is another kind of mask. But then that requires too many masks in total. So I need
    some extra parameters to train... I need a semantic tree and I need a smarter way to represent the same object in
    different environments.

    Maybe also reimplement multiple styles feed-forward. But that's not so necesssary.


2016/12/20
1. Feed-forward with mask
    Only works on the boarder line. Don't know why.

2016/12/21
1. Feed-forward Style genration with johnson network.
    style weight = 50 seems to work fine.

2. feed-forward with mask
    I thought it was working... I replaced .jpg with .png masks and at first it seemed to be working... Then it just
    stopped becoming better after round 10000. The sky looks a little bit like the given style but the rest didn't
    look anything like the original style image. I think I've met the same problem before when using johnson network.
    So it has nothing to do with the network or the inputted mask images. Interesting. I tested feeding in masks
    where only one mask is 255 and all others are 0. It outputted the same result as before when I used johnson
    network...

    If this still didn't work... I don't want to get stuck at the same place for a week. So I'm going to move on.
    Maybe I should change the output so that it has not 3 channels, but 3 * num_masks channels and I manually dot
    the output with each mask and combine the final result.
    ...
    Wait, how do I calculate the loss again? I dot the gram matrix with each mask... Hum... There must be a problem
    there. That's one of the two possiblity. The other one is I'm not feeding in the right noise.
    Nah there can't be a problem there because I'm using the same loss function in the slow version.

    I just realized that the version online didn't use any other input except the mask themselves! The code is there
    to support additional noises and noise dot mask but he didn't use them (he set the parameters to 0)... gg


3. Back prop slow version
    I want to try the non-mrf version and compare them. The biggest problem with mrf is: it is too aggressive and it
    does not recognize objects well. It is agressive because it usually either does not change the object at all or
    completely changes it to another objecct in the style image. It does not recognize objects well because, well, the
    limits in vgg network. It does a poor job recognizing, say a black contour of a human compared to that of a tree.
    We're helpless unless we label each part. But then most of the time the images are too complicated to mark what
    objects are in there.
4. Future direction
    Since the feed-forward part is not going so well, I'm thinking things I can do with the slow version.
    I thought of attention network. So I paint one area at a time or something like that. That can replace the need of
    masks because masks are essentially manually added information on object recognition (which can be done by the
    network itself in theory if the network is good enough.)
    Now the bottle neck is that the vgg can't recognize objects well, and if it can't do that, I don't have any way to
    improve the current model except to feed in manually labeled data...

2016/12/23
2. feed-forward with mask
    I noticed that I did not normalize the gram matrix by the mean of mask values. maybe that is causing the problem
    (how do you expect two gram to be the same when you don't even know the number of pixels marked by the mask?)
    That's why the feed-forward network works without masks...
    Yep it's finally working... Plan: I still need to change the input back. The model probably isn't using the extra
    input anyway.

2. Future direction
    I thought about the style of an image. What does that mean? To me its meaning is two-fold. The first one is local
    statistic relations. This one is well captured by the gram matrix. The other one is more logical. If I always paint
    a man's face like that, then it must have some logic behind it (or not) and we can also call that "style". That's
    why the current neural style need a 'content image' whereas in real life people don't need a real image and make
    change on that. One is more on a small scale and the other one on a grand scale.

    What I can then improve? I implement removing the grid-like noises in the generated image. That has been done
    before.

    I can improve the gram loss so that it is no longer just on a local scale. That had some progress but I stopped. I
    should maybe continue that. I need to understand the gram matrix more. I mean I know how mathematically works, but
    why it works? I have no idea.

    I had an idea of how to generate masks for any image (as long as it is moving). There are object tracking
    techniques that can mark one object in a stream of images. Ah I can think about this later. It depends largely on
    object tracking. *

     I know my final goal: to make programs that can draw like humans, with some instruction from human.

3. Back prop slow version
    After I fixed how to calculate the grams, the "stack grams" with the masks, the result looks super impressive. It's
    even better than the mrf. The color looks more like the original and it definitely have some originality.
    After applying that to the content image, it lost its charm... Plan: figure out why.

2016/12/24
1. Back prop slow version
    I tried a different mask for output. It still works :) I doubted that it tend to generate images similar to the
    style image in terms of where objects are placed. Apparently when I'm using masks and generate only texture, that
    is not the case. But I tested with texture generation without mask and that seems to be the case. The texture
    generate had a similar layout as the original style image.

    But a bigger problem is it no longer works on content image. If I set the style weight to be too high, then it no
    longer looks anything like the content image. If I set it too low, then it is unable to generate those large-scale
    features like it did in texture generation. It is a hard balance between the two... That can be solved by applying
    different weights on different areas. The program is then instructed to be "more creative" in some areas and
    adhere to the content image more in other areas. That may be achieved by adding a "weight" to each mask. Currently
    all masks are treated equally.

    Or I can just dot another mask with the current feature layers. That way I'm no longer confined to having a uniform
    weight for one mask. I can apply as much/little style to one area as I want. This can also be used in the
    fast-forward version. (Although I guess it's a little bit trickier.) I can imagine one having a brush and as they
    put more "style paint" onto one area, the output changes accordingly. That would be amazing.

    Hum that sounds like a plan. I'm doing it. i need another branch... It is not so related to improving the new loss
    function (In theory it will make the new loss as well as the old one look better), but whatever.

    1.1 Style weight mask.
        It worked almost as expected. There is no style applied to the area where the mask masks as 0. One thing about
        that is: Since I do convolution on the mask, even if one area is masked as "do not apply style", after average
        conv, it is still possible to apply style on that area. I don't know if that is desirable or not yet. if not I
        can simply get rid of the average conv and do resizing. Anyway, so far it works with texture generation.
        I'm checking if it still works with style generation.
        Yes! It totally worked!!! It is almost unnoticeable that I applied different amount of styles to the same image.
        It looked so nice (way nicer than the previous one). I now have control over exactly where and how much style
        I'd like to apply to the content image. This is so nice.
        Next step is probably do feed-forward version of that so that the user can see instantly how the change affect
        the outcome. This is exciting work. :) Nice christmas present for myself I guess.

        I probably shouldn't get so excited because this is just the simplest change... But nevertheless it is
        effective.

        Plan: add the style weight mask to the feed forward network.
        Plan: delete the pyramid generation network. It's no longer useful. Everything else is working fine.

    1.2 No longer automatically resize style images.
        Resizing them is causing some issues with style/texture generation. If the width:height ratio was changed, then
        the generated style/texture will also change, making it look less like the style given.

Plan: Test the style weight mask in the feed forward network... Uh maybe the style weight 200 was too high. Or maybe the style weight mask was messing everything up.
Plan: Try generating styles with content image with skip noise 4 without style weight mask. It looks wierd for the first 3000 rounds

2016/12/25
1. Style weight mask-- feed forward version
    It is not working when I simply feed that into the johnson network in addtion to the images. The result is just not
    what I expected. I tried feeding in both completely random noise as well as mask generated by the diamond square
    algorithm.
    Nah. skip_noise_4 can't even learn the content image. Or it's learning it super slowly compared to how fast it's
    learning the style.
    # Plan: figure out a solution.

2. Scale_offset_only
    I realized that it may be done through just adjusting the scale and offset variables (according to the google paper)
    instead of feeding the entire mask as another layer of input besides rgb. Therefore I decided to test the
    multi-style training again.

    Oh right I forgot. I need to have two sets of those variables and I'm only outputting one image currently...

2016/12/26
1. Scale_offset_only for multi-style feed forward network.
    I fixed the network. Now at least Johnson network works fine with multi style.
    I re-read the google paper "A Learned Representation For Artistic Style". The pictures they provided for several
    styles were not particularly satisfying. I think the slow back prop would do a much better job on those pictures.
    The problem was still mainly on applying styles on places that it shouldn't. Plan: I will compare the feed-forward
    model, the multi-style feed forward model, and the back prop models and see what's the difference between them.

    It will take a while so i'll do that overnight through a script. style weight = 50 seems to be too low for some
    styles so I'll increase it to 100. The network seems to work as expected (meaning it doesn't work so well yet. lol)
    and the styles can be nicely described by only the normalization factors. I still need to understand why before I
    move on though. The two biggest question I have is: why Gram matrix work? Why normalization work as separating
    between styles?

2. back prop.
    Plan: Will test between the new and old loss systematically.

2016/12/27
1. Back prop New loss.
    There is no noticeable difference between the new one and the old one when it comes to applying styles to content
    image. As noted before, there is a huge difference when I use it to do texture generation.

    I thought the size of the content image may affect how the patterns are generated (or rather how well the patterns
    are generated. Currently they're just general patterns. They don't have any objects. But in the texture generation
    it can learn to generate larger patterns/objects as well.
    After testing with size 128,256, and 512, I found that the pattern generated is not affected by the size of content
    image so much. Then I remember reading yesterday that the initial value is very important to texture/style
    generation. Now I'm trying to add manually some patterns to the content image and see what the result is. I may
    have to also try to modify the noise I feed in (fill the noise with more patterns for example)
    Nope... Simply drawing lines to guide the style doesn't work. lol. I need to figure out how gram matrix really works.

2. More meaningful parameters:
    I read an article on GAN yesterday and they mentioned a way to make the "noise parameters" fed into the model mean
    something.

2016/12/28
1, Scale_offset_only
    I ran two trials with scale offset only = on and off for each one. It seems that scale offset only = off will have
    a better regeneration of the content image without losing its ability to regenerate the style (I mean it's as
    good/bad as scale_offset_only = True, most of the time clearly even better than that). That means the scale and
    offsets may not be the only factors affecting the style? Or it could be that the thing just works better when the
    generated image have a smaller content loss (because in theory when scale_offset_only = False we're essentially
    training the rest of the parameters n-1 times more compared to scale_offset_only = True.

    For both setting, there are artifacts in the generated image. Sometimes it's an completely black area and
    sometimes it's completely white (the color is consistent for each setting). I think it may have something to do
    with the scale and offset. I knew that there are some bugs in tensorflow's standard deviation calculation.
    Maybe I should try to add the absolute value. It is consistantly appearing for every single training. Maybe the
    learning rate is still too large. I'm guessing what can cause such artifacts with usually an oval shape. The
    artifact appears at the same place and shape for all styles so it has nothing to do with scale and offset.
    Maybe I should print all variables and see their values? Find the abnormal one? I'm guessing there might be a
    nan somewhere.

    Also the feature size seems to be too small compared to the picture. Maybe I should do training first with small
    image size then increase that??? I've seen people do it in one git repo. Ok the feature size has nothing to do with
    the size of the style image. I was mistaken. I thought because the convolution kernel size and stride are fixed,
    the interpretation of the cnn on style image of different style would be different. I was wrong. But why???

    Also the artifacts are still there... It's not the mean and standard deviation...
    Maybe it's relu!! Maybe I should use leaky relu or elu or something. But the same artifact did not exist in
    feed forward texture generation.

    Hum... I think the artifact is somewhere in the convolution part. It can't be caused by having a large learning
    rate (because then it would also be a problem in texture synthesis). Also it's probably not dividing by zero
    because you can see non-white pixels in the artifact area.

    but there is no such problem before I made the multi-style change... So... the problem must be in the multi-style
    part that is shared by all styles...


    Plan: fix the feature size and the artifacts (artifacts first)
    Plan(done): test the effect of style image with different size on the output result.
    Plan: test the effect of changing output size and see if the patterns generated become bigger... I don't think so

2. Study the model.
    Plan: read the GAN article on making meaningful noise parameters. I read it. The math was pretty complicated but
    I got a broad picture of how it works. It is inspiring. However I don't see how to connect that to my current work
    yet.
    I can't find an explaination of why gram matrix works. It's magic.
    Plan: Can we build an even better loss function? Can we build a neural network that basically classify styles and hope
    that it automatically contains enough information to build a better loss function?

3. back prop
    I thought the masks were not working. but it turns out that the backprop is not working on only one part of the
    content image. I'm testing the effect of other masks, but this is really wierd.
    It's definitely the output mask or the style mask... Even if I change the image, the same part/area is still not working...
    But even if I change the content semantic mask it's still not working...
    If I change the size then it's working...Something wierd is going on... it must be something stupid I'm doing wrong.

4. new loss function
    No progress made.

5. Read nijigen stuff

2016/12/29
1. Scale_offset_only
    Artifacts: Plan: trying elu instead of relu... Don't think it will help though. Hum it seems elu fixed it.
    (or it could be I decreased the number of style images, but in the past i've seen those artifacts even when I only
    have two style images.)


2. Study the model
    I need more control over the generation process. I have no control over how the patterns are generated or what
    size those patterns should be.

    Plan: figure out what controls the size of patterns using back prop first then generalize to feed forward
    (like the size of brush or size of triangles etc.)... Wait the back prop and the feed forward are different.
    Feed forward seems to be more dependent on the size of content image fed in during training.Need more testing to
    confirm that.

    I tested the effect of content and style images with different styles. I'm now 100% certain that *The size of
    content and style images does not matter. The features generated have the same features sizes.

    I realized that it may be related to the kernel size of the vgg layers. The style comes from dotting each pixel in
    feature layers with the same pixel in another feature layer. The value of that pixel is determined by the algebraic
    computations in the vgg layer. I thought the vgg layer will get activated differently if i change the size of the
    style image or the content image, but it seems that it is largely the same (this may be explained by saying that
    if you draw a square and change its size, then no matter how you change it, unless you change it to a tiny 3x3 size,
    a conv layer with kernel size = 3 will have the same activation for most of the pixels no matter how you shrink it.

    So the feature size is dependent only on the size of the kernels in the vgg network and that's why no matter how we
    change the image size, the resulting texture won't change a lot.

    Hum...


3. back prop
    I think the bug I met yesterday was caused by .png masks. Although invisible, there are layers covered by the top
    layer and those are messing around things. If I change it to jpg it should be fine.
    Ok it's definitely something wierd with the mask for content image. If I make a brand new one then it's fine. But
    if I try to reuse the old one in any way it breaks.
    yep that's the bug... I still don't know what caused it but well. I found the solution. It took me a whole day
    to figure out...

4. New loss function
    I kind of knows now how I can get a larger pattern. (rather than a local one). That lies in the correlation between
    one feature and the features in nearby cells.
    Plan: testing the effect of stride and shift on the new loss.
    ** Larger stride results in textures becoming more and more like the style image itself. That is probably
    due to the fact that we used lower layers as well as upper ones. What if we only apply the larger stride on upper
    layer? (The reason behind that is: lower layers represent local details and overall layout. If I use large strides
    on lower layers, then the overall layout will be also set as the same as style image... Let me test this.
    I was wrong. I was only using layers 3 and 4. Now I'm trying layers 1 and 2.

    Using layers 1 to 4, it seems that smaller stride will result in slightly smaller features/objects. Larger shift
    and larger shift will result in textures looking more and more like the style image in terms of layout (stride more
    than shift) This is a pretty large draw back...

    Plan: use layer 1-3, 1-2, 1... and look at the difference between them while using different shift and strides.
    It turns out that using more layers (especially the upper layer) is better for generating larger patterns.

    Also I haven't yet figured out why the patterns disappeared when I use a content image. But that's probably less
    important now. The new loss function is just too buggy and hard to control.



5. nijigen
    I haven't read the article, but I started to think how to use my stuff to draw something given a sketch.
    So let's say I want to draw a tree. I marks where the leaves should be and where the trunk should be. Then I let
    the program to do the rest for me. Does that sound like a reasonable goal? Yes and I've already accomplished it I
    think. I haven't tested on how well it works on more complicated styles (like drawing a human), but if it works,
    then I should start building the website to let people mark their own masks and use their own images...
    I should also if it works as well in feed forward mode.

    Also, it comes to me that: what happens if I choose my training data for the feed forward net according to the
    content of the style image. So if the style image is a human face, then most of the training data would also be
    human faces. Would that make the result look better than training the style for a human face on any random image?
    This is just a random thought.

    #Plan: testing mia.

    Neither mrf nor gram loss is perfect... gram loss is more flexible but too crude. The mrf has higher quality but
    looks too much like the original one without the content loss. I would try guiding the image with the result from
    mrf. First let me try to guide it with the style image (since I'm using the same masks for style and output and the
    best result mrf can get is to be exactly the same as the style image).
    # Plan: guide gram.
6. From previous todos:
    The skip_noise_4 network can't learn style generation with content image.


2016/12/31
1. Nijigen
    I tested the grided loss functions. With guidance mrf performed a lot better than gram. I'll try a different mask
    (different from the mask of the style image) and see if mrf still does well. I don't think it will though.
    Nope just as expected. lol.

    Also If I want to generate genuine looking faces instead of just copying the current style, I probably need
    feed forward network. Maybe I need GAN? That seems a little more promising than using style generation to generate
    contents...

    I found a blog that had amazing result for coloring sketches. I'll probably spend lots of time on that...

    TODO: what is deconv layer (or conv_transpose) ?http://www.matthewzeiler.com/pubs/cvpr2010/cvpr2010.pdf

    Ok the majority of the implementations are done. The only thing left is training data. I tried it on real life
    images. it turns out it is too hard for it to learn (understandably. There are way too many edges...)

2. Scale_offset_only
    Testing if elu really worked. Also testing at the same time the effect of training first on smaller images then
    on larger ones.
    Shit it's giving the artifacts again... So it's not the elu that's causing the problem.

2017/01/01
1. Nijigen
    Uh it sort of works but not really. The color was not 'vivid looking' at all. Low contrast is the technical word.
    Also adversarial net wasn't working so well either. I changed some parameters around but I don't think that will
    really fix things. I'm also downloading more training data.

    After training the net for a while (without adversarial net), it seems to be doing well at least on the training
    data. If I feed it brand new data, it can at least tell the background apart from the rest... But it didn't do much
    other than that...
    I forgot to separate the training from the test so I did that. I probably will try the new training data
    tomorrow if I finish downloading them tonight.

    Plan: I also need to ask the author what he used for the training data of inputs with addtional "helper
    information"

2017/01/02
1. Nijigen
    I finished translating it to Chinese. The color was still a problem, even after I added the hint. The hint doesn't
    seem to be working as it was supposed to. It had the kind of "old photo" color.
    I'm trying the adversarial network...

2017/01/03
 1. Nijigen
    I confirmed that no matter if I use the discriminator or not, it will not color thing correctly. Everything looks
    like old photos. Maybe the adversarial network is not set up correctly? I should take a look at other
    implementations of gan and its relatives.

    I found that I implemented several things wrong in the adv network. First I normalized the first input layer, which
    according to the dcgan paper will cause oscillation. I also got the loss function wrong. I naively thought that
    my implementation is equivalent to the more formal cross entropy loss. That doesn't seem to be the case.

    I did not increase the batch size (the paper used a huge batch size) because I want to quickly check if the result
    is working. I will switch to larger batch size once I know it's working.

    If it is still not working, then it might be the training data. I am downloading more training data from pixiv.
    I will have around 200g of data to train, which should be more than enough.

    I'll go ahead and post the chinese translation while working on the english one.


    Nah it's not working... It is generating wierd colors.Did I get the loss wrong or is this the natural reaction...
    It doesn't seem natural.
    After checking the change in loss after each training round, I found that the learning rate was too large. Smaller
    learning rate or larger batch size will generate more stable result. That's why only training through adv did not
    generate satisfying-looking images even after 50000 rounds of training. This sanity check was helpful...
    The following test confirmed my guess. I tried to force the network to learn the l2 loss for the first 10000 round
    and let go after that. But the network went back to the old messy looking image after I let go. So the network
    actually prefer the messy looking one, which is completely nonsense. It is probably the adv network was too weak
    (as shown in the sanity check...) the generator network always outlearns the adversarial network for some reason.


    I think the sketch generation has a 'bug': when it generate the sketch, I forgot to set a threshold. Therefore,
    most of the area is actually filled with almost invisible but nonzero values. This may provide hints to the cnn.
    That's why the cnn can learn the shades so successfully... GG. Plan: I need to rerun the tests after changing the
    sketch util to set a lower threshold for sketches.

    Doesn't seem to be the case actually. I ran the network using the new sketch generation method and it still looked
    fine (as good/bad as before).

    I'm trying to filter training images where there are too many lines (the sketch that takes more than 20% of the
    white space is filtered). I might end up with not enough training images...

    I found that the training set had surprisingly few number of images... I'll reupload the training data. I will
    also definitely download the larger dataset and train on that... I believe the method itself is fine... I hope.

    But why the adversarial network is not working? The only difference I found between my implementation and the one
    online is that we use different networks and they use a much larger batch size... Also they probably had more
    training data...

2017/01/05
1. Nijigen
    Higher batch size is way more important than I thought... It provided much stabler training process and better
    result simply by raising batch from 1 to 8. The hint didn't work as well as the original author had shown, but it's
    better than the previous versions. Given more training time and data, maybe it can get even better.

    The adversarial network was still not performing well... It's training super slowly and it's getting no where..


2.  Text dcgan
    This is the idea I came up with last night. Instead of feeding in images, I can feed in text in certain encoding
    and see what happens. I may come up with a good enough sentence generator.

3. Random thought
    I just thought of one thing. Why can't I have a network that take two inputs: the style and the content, and
    train that on the gram loss? So the structure would be similar to unet, except that the conv will take the style as
    the input and the content is fed in by concatenating the resized version of that to each deconv layer. (So
    the structure changed slightly from the unet to take two input images). Maybe that can work... lol.
    But I'm not so positive. I haven't seen the network work fully as described by the blog yet. The color is just not
    satisfying. I was hoping that much more training data could save the day (or higher resolution. 128 was just too
    hard even for a human). We'll see.


2017/01/06
1. Nijigen
    Still downloading the dataset.

2. Text dcgan
    I found a great explaination for why text dcgan won' work:
    https://www.reddit.com/r/MachineLearning/comments/40ldq6/generative_adversarial_networks_for_text/
    I need something continuous. Then I thought of chinese characters.

2017/01/07
1. Summary
Try to solve problems not try to use new tech

The problem I am facing now is I can't color the sketches. The colors look like those from the old pictures.
I used three methods. One is train against l2 loss. The second is train with 5x5 random hints on the color to be used
against the same loss. The third is train using GAN. The first and second is some what successful but not satisfactory.
The third is completely shit. The reasons I can think of is as follows
The first one fails mainly because it is almost impossible to predict what color one used simply based on the sketch.
The result is therefore reasonable.
The second one still failed. I thought it was because the images were too small. Even if I gave the hint, it might
still be too hard. Maybe the way I am giving hint was wrong.  The third possibility was the lack of training data.
The third one failed I think mainly because of network design. I am not training it correctly. I may want to test if
the adv network can tell apart anime from real life images. If it can't do that. Well there's something wrong with the
discriminator. If it can then there is something wrong with how we train.
So this is my plan.
Regarding the neural style, I've done most of what the papers have mentioned so far. They looked as expected. I was
planning to improve applying styles differently on different objects but it seemed hard with feed forward network. I
need to improve the network design if I want to do that. Also, I should probably try different networks other than VGG.
This is the most direct way to improve object recognition because vgg is not trained on art works.
As of the NLP plus GAN, I am still experimenting. The biggest difference Between image and language is that language is
more discrete due to its abstract nature. I need to think more on how to approach it. Chinese characters are
interesting, but are they also discrete?

2. Nijigen
    a. No hint no GAN
        Waiting for training data
    b. Hint
        Waiting for training data
    c. GAN
        The Adv network works on discriminating between anime and real life. So it's not the adv network
        The generator network... I don't know. I need to log the loss.
        I implemented loss logging. The loss seems to be working fine. I think the network did not work due to a lack
        of training data. The DCGAN paper used 1 million + images for each training task. Mine had 20000... This is a
        huge difference. I will get the larger set of training data asap and I will not try training it further...

3. Text DCGAN
    The generator can't produce satisfying kanji. The adversarial net was too good.
    The problem still exist for kanji. There is definitely more smooth transition than pure english alphabet, but it's
    still kind of discrete. I see that the generator mastered some of the strikes, but the overall structure of the
    kanji generated was just bad.

4. Random thoughts
    I was thinking if I can use the techniques in style generation in nijigen stuff. I know how to mimic the 'style' of
    any image. What happens if I train a network on anime images and use that instead of vgg? Would the gram loss still
    work on that network?

2017/01/08
1. Nijigen
    Finally finished downloading. Decompressing will take a while...
2. Text DCGAN,
    I also realized the problem for kanji. There are too few of them. The DCGAN requires 1 million + examples at least.
    I realized yesterday that although natural language is inheritly discrete, it can be turned into a continuous
    problem(otherwise no ml approach would've existed for nlp). The problem is how. If I have a loss function and a
    generator network, then I don't need the adversarial network (yet). I can google CNN for nlp
3. Neural style
    a. Feature size
        I just found through training the back prop version that the feature is indeed only related to the vgg network.
        I reached the wrong conclusion previously because I used a style image that was not so obvious. If I choose one
        correctly, then it is obvious that the feature size has nothing to do with how large things are in the content
        image.
        One remedy I can think of is to directly change the conv layers of the vgg network. If I want the features look
        twice as big, I just need to change the stride from 1 to 2 or from 2 to 4. Generating smaller features would be
        tricker but still possible. I can provide an image twice as large, run neural style on it, and then reshape it
        back to the original size.
        Plan: Let me actually do this. I didn't see people do that yet.

        I tried several experiments. Simply doubling the stride size in either conv or max pool layers did not work.
        I then tried to resize the filter so that they become twice as large. That worked to some extent but the image
        produced was blurry. Wait of course it is blurry. The network is not trained on doubling the size of the
        filters at all!

        Imagining the activation of one to two layers would make sense. If you try to make the activations be the same
        after you enlarge the image to twice as large, then it is clear that you should also make the weight matrix
        twice the size and keep the max pool the same. If you change the max pool, then you change the relationship
        between pixels and the masks trained is no longer valid.  Multiplying the ksize and stride of the max pool
        layer is more like making the shape of the features smaller. The result is usually not so satisfactory because
        the features should be already pretty small.

        Anyway it is not so satisfactory yet. The best I can do is to produce a blurry distored version of the enlarged
        style.

        I tried varying the style image size again, and again apart from sharper looking image, the size of the
        features did not change with regard to the style image size. This result is the same as I've tried before.

        Anyway, this is good for now. The main reason of failure I suspect is that resizing the filters is not good
        enough for interpreting the styles accurately. The distortion was too large and the style no longer looks like
        the original one. I know how to make the features look smaller though: just run it on larger content image and
        resize the result afterwards. :)



    b. Code refactoring
        I refactored the code a little and deleted unnecessary files/functions. It looked a little bit cleaner now.

    c. Fast forward both style and content.
        Hum... I need a lot of style images. Well I have them right? The pixiv dataset.
        Plan: This is also worth trying.

        The first experiment would be simply use the johnson network but change the last dimension of the input to 6.

        Well the result was: the image looked almost like the style image except with black and white color. You can
        see a tiny shade of the content image. I didn't think that the network can use the style image directly instead
        of learning some complicated loss function.. lol that was dumb.
        I can't feed the style image as an input then... otherwise the network will just use it directly on the output
        image.

2017/01/08
1. Nijigen
    Training using the new dataset. The new dataset was smaller than I thought. It is 10x larger than the previous one,
    but still did not reach 1 million. The size was 100 thousand.

2. Text DCGAN,
    After changing the generator to a more complicated structure and training for the whole night, it was clear that
    the generator did not to a good job. The best it could do was to produce the word "the" and "one" from time to time.
    I therefore decided to change the task to generating english-looking words.

    still failed,,,
    I realized that there are still definitely no more than 1 million english words in the training set. Probably on
    the level of 10 thousands, which is not enough. I'm planning to work on something else for now.

3. Neural style
    Still thinking

Plan:
    Run text dcgan for word generation
    Run color_sketches_runscript with hint
    resume color_sketches_resume_runscript to see if the network can learn anything without hint

2017/01/09
1. Nijigen
    After training for around 50000 rounds, the result was still not satisfactory. The coloring just isn't right, even
    with the hint. When using 256x256, the hint can be generalized to a larger area successfully but that property
    disappeared when I try to generate image of larger size using the network trained on 256x256...

    I Also tried to retrain a network that has been trained previously without adversarial network with adv=True. The
    result was not satisfying. The adversarial losses for both adversarial network and the genrator was decreasing
    slowly but the l2 loss became larger. I don't know if training the adv network along side the l2 loss would help or
    not. I don't think so personally... But I'll give it a try.

2. Neural style
    Planning to separate the nijigen stuff out from the neural style. The neural style project's reimplementation part
    is almost done. I just need more refactoring and that's it. So spend the time refactoring code.

    Refactor status:
        general_util: Done
        neural_doodle_util: Done except for adding some unit tests.
        neural_util: Done except for adding some unit tests.
        vgg: No need for refactoring because it's mostly other's code.
        mrf_util: Done.

        stylize: Done
        stylize_examples: Done writing large-scale tests for stylize that can also serve as examples.
        neural_style: Done

        conv_util: Done. Maybe add more test cases... Too lazy.
        skip_noise_4_feedforward_net: Done, don't forget to delete commented out part after testing.
        johnson_feedforward_net_util: Done
        n_style_feedforward_net: Done, but it's still a mess.
        feed_forward_neural_style: Done
        real_time_neural_style:
        generate_masks


    There's another thing that I noticed. The texture seems to also change with respect to the size of the style image.
    TODO: I will investigate throughly.

2017/01/12
    worked on refactoring.

2017/01/13
    1. Nijigen
        Added function to get rid of black and white images from the training dataset. Now I'll try to retrain the whole
        thing on 512x512 resolution with batch size 8. It will take a while...
        I also got the "pixivutil2". If I found that it was the problem of not enough data, I will use that to download
        more data.

        Regarding the original author's suggestion to first train for several epochs without adversarial network then
        add the adversarial network and set the loss to around 1/10~1/20 of the l2 loss, I'll give that a try. The
        wierd thing is, previously I had adversarial part's loss be around 1/100000 of the l2 loss and it still was
        having reasonable changes to the model. I don't know what will happen if I time that change by 10000...
    2. Neural style
        The refactoring on back propagation part was almost done. I still have one stylize example to do but that was
        it. Next step is to refactor the feed forward part... But I want to put off that for a while. I haven't worked
        on something challenging for a while... Refactoring was more like a break.

2017/01/14
    1. Nijigen
        More data, more data... I used the PixivUtil2 program. Hope pixiv doesn't block amazon's ip forever...
        So far so good. It's getting around 60 gigs in the past 9 hours. By this time tomorrow I will have around
        600000 training images... I think I might be able to do things even more interesting than filling in colors
        given this dataset.

        TODO: check network structure. Maybe there were some problems there.
    2. Neural style
        More refactoring...
        Don't forget to investigate pattern size.

    3. CNN relation extraction
        Well I found several tensorflow repos but I forgot to bring data to try it out...
        My project is a little different though. I lack training data. I have to use active learning to gather more.
        The most naive way for active learning is after training for a while, put all possible combinations of
        pairs of phrases into the classifier, select the most uncertain one and ask about that. A little less naive
        way is to sample say 1000 pairs randomly and do the same thing. That will speed things up a little.
        Another thing I probably don't have to deal with is that the key phrases I have not may not be accurate (it is
        certainly not complete) and I may have to create a vector for the phrase by adding up the vectors for each
        word, but I don't know if that would make learning harder -- balanced tree is quite different from the word
        "balanced" and "tree" trained on other corpora.

        Anyway this looks much better than the situation I was in before.


2017/01/15
    1. Nijigen
        Still the same issues with the adversarial network... Fluctuating colors. Clearning up the dataset or
        increasing the weight didn't fix it.
        My last resorts are: increase the dataset size (which also means even longer training time), and decreasing the
        learning rate only for the adversarial network. I'm trying the latter...
    2. Neural style
        More refactoring.

    3. CNN relation extraction
        I'm done with modifying the cnn repo for my project's purpose. All I need is more data and adding active
        learning.

2017/01/15
    1. Nijigen
        After training for such a long time (110000 batches), it still looked the same as if I've trained it on a
        smaller dataset at a much smaller size for the same number of batches.

        Although I do have the larger dataset, I don't want to work on this further until i find out why... It costs
        money to train this thing overnight.
        Cleaning up training data will take a while so I'm letting that run. Maybe I should also try to save the
        preprocessed and resized images as .npy files and save them in the memory ready to be used. Don't know how much
        speed up this will provide. It should be considerable enough though...


        I tested the resize function. It was pretty slow (3s on my laptop per image). Going to try to add preprocessed
        data with test_multi_style.py. If that speed things up significantly, I will switch to using preprocessed ver.
        Before switch, each batch with batch size 1, hw = 512, and 9 style images takes 8 seconds.

        After modification: each batch with batch size 32, hw = 128, and 12 style images take 23 seconds.
        each batch with batch size = 4 hw=512, and 12 style images takes 50 sec... Not so fast I guess..


     2. Neural style
        Refactoring... maybe not today
        Maybe I should try this: pretrain the model on say 8 style images, then use this as a base to train on
        other style images only on the scale and offset variables.
     3. CNN relation extraction.
        I will keep the notes of this in the NewMaster project.


2017/01/16
    1. Nijigen
        TODO: study the other coloring methods that uses cnn.
    2. Neural style
        Note: I'll not be able to use the pretrained network in the example because I changed things around afterwards.