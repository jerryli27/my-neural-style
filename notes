This is the notebook for recording experiment results, discoveries, and next steps.

It is crucial to have a low enough learning rate. If you have a high one and decrease it over time, it still won't work.

MRF is confirmed not to work with feed forward neural network. But mrf is not really absolutely necessary. I might need
it for semantic related masking stuff. But it has also been shown that masking works with gram loss and feed forward network.

Here are two unsolved problems:
The size of the features/styles. The image genearted when input is 100 x 100 versus 1000 x 1000 is drastically different.
The generated image is limited to the structure of the original painting. In art, you can draw something larger or
smaller than usual to achieve some effect. There's no such thing here because the loss is localized
(You can't just change the absolute position of objects in content image.)


Two ideas: artists "zoom in" to a spot when they want to paint it in more detail. Is it possible to do so in our architecture?

Second idea: if the generator network can generate styled images, why can't it be used to reformat the feature layers?
Turn the feature layers into something that look like other artforms (without loosing too much originality)
and generate ... But how's that different from just use the generator network to generate the image? Not really

I want something other than difference squared loss. I thought about nearest neighbor loss of the normalized feature
layers. That one is invariant to translation.


Trying two things now: one is to directly observe what is going on in each loss function for each feature layer.
Another is to feed multiple styles into the network while modifying the same parameters.

simply feed 15000 shirobako images as style didn't work.

I tried the mask again using hand-segmented shirobako mask (mrf loss). Surprisingly it's sort of working. Maybe I don't need the
content loss. I can just create a drawing from scratch. Maybe that would be too hard.
One more difficulty if I choose to go on this direction. I don't have hand-segmented masks for me to train.
Even if I do, I don't know how to incorporate say 10 masked style images together. Simple nearest neighbor matching on
the 10 images would require 10x memory in the gpu.

One idea for automatically label images: start with example image and a hand-labeled mask. Now for each input
we compute the nearest neighbor of say conv4-2 (any high level layer) and assign the nearest neighbor's mask
to the input layer. Then add the constraint that nearby pixels should preferrably have the same label.
Then do deconv to get pixel-wise label for the original image. (Sounds like a plan, but this requires the
high level layers of the style and content to be similar, which may well not be the case. 20161122)

Texture and placement of objects are two different things. Texture is given a mask, how to fill in the colors
so that the style loss is the least. That is already solved. Placement of object is another issue. How to
place objects relative to each other so that it is the most probable.

I tried the nn loss. NN is not differentiable so it has no gradient. I should've realized earlier. Now I probably have
to get around this problem.

One thing is, the reason why mrf can perform better is because it has those patches that captures info about nearby
cells. Gram simply multiply them together and add all cells. There's no interaction between a cell and the cell on its
side. On the otherhand, the mrf is expensive to calculate because of the nn step.

After some thinking, it comes down to knowing the feature layers first. (like each of the conv layers, when they're
activated and compare two inputs, content and style, see when their conv layers look similar.)

Adversarial generative network may be worth investigating into. But don't do that just yet. I still have to finish up
this experiment.

We can make the additional semantic masks learnable. Just add a loss for too much deviation from original (more
complicated loss can be added later)

Things learned from the experiment: the conv layer won't look close to the original image. So we can't hope to morph
one image into another in the conv layers. Also, making semantic masks learnable will need further refinement.

I thought maybe finding the spatial correlation of semantic layers might be helpful. Now it is only finding the
correlation in-place (that is, the same pixel times the same pixel in another layer). What happens if we shift it by
say half of the width and find the correlation? The hope is that , for example we have two eyes, one on the left
and one on the right. By finding the correlation after shifting, we may find: ah whenever there's an
eye here, there will be on on its right with distance half screen away. That's my hope.


I was correct that correlation after shifting one layer encodes the relative positional information. Now the problem is
1. It was not perfect. I hope the problem can be solved after adding a content loss.
2. It was spatially too similar to the style image. ie. no shifting etc. There was some shift for subfeatures, like
the mouth was on the right first and slowly came to the center. But overall the head position was still the same, the
position of everything was the same as in the style. This is not what we want. We just want to modify the content image
a little so that the eyes become bigger, or the nose become less noticeable and things like that.
I want to have two very simple images as input and see how it goes.

Not going well so far.

Side projects during thxgiving:
Run multi-style feed forward network.
Shit.. I realized it actually doesn't make sense in the current frame work. I have to separate each style by itself
instead of feeding them in as batches. ( Otherwise I can't set the image placeholder so I can't train scale and offset
individually. I'll try to fix this tomorrow by going back to the master branch and merge the two..
I don't know if I should keep the batch style... Maybe I should.

Tested overnight on claude monet's paintings. Now I'm sure I can't just merge styles like that. It doesn't work.

Other future directions: feed forward neural doodle.


As I think of it, maybe we don't need the content image at all? There are many things that are hard to find in real life
but exist in drawings. I know that a feed forward network with masks can be trained as long as I have enough training
data. The only problem is where to get that semantically labeled training data. I can get that from 3d model... I think
I can look into that direction. No content image. Content loss comes directly from thousands of style images. Details
of how content loss can come from those may be filled in later but I think this is the right direction. We don't have
to worry about distorting content image into the style image, which is a big challenge.

Before that, let me modify the current code so that it can accept only semantic style images and semantic masks and
learn them using feed-forward nn. the loss... we can use content loss or style loss. try both I guess. I need to
copy the previous code into a new file because the change will make the code incompatible with the previous version.
(since there no longer need to be a content image). Actually, I can use the same file... Since I may use that framework
to learn auto generating textures.

Basically I will implement feed forward neural doodle, but better than what they've done. I need not only rgb mask, but
any number of masks for each style image.

2016/12/09
I implemented fast neural doodle. Testing it. It is running super slow for some reason... I don't get it. I should test
running the normal version... (without masks and doodles.)

2016/12/10
Fixed some bugs, but the thing is still super slow, even if I don't turn on masks. Was it like that from the very
beginning? I think so because it takes the whole night to train around 20000 rounds, so it's like one round per second.

Also the loss for style is unnaturally high only after I apply masks. I found that was because I was doing dot product
so each number is dotted with 255. lol.

But the result was still unsatisfactory.