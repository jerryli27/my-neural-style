This is the notebook for recording experiment results, discoveries, and next steps.

It is crucial to have a low enough learning rate. If you have a high one and decrease it over time, it still won't work.

MRF is confirmed not to work with feed forward neural network. But mrf is not really absolutely necessary. I might need
it for semantic related masking stuff. But it has also been shown that masking works with gram loss and feed forward network.

Here are two unsolved problems:
The size of the features/styles. The image genearted when input is 100 x 100 versus 1000 x 1000 is drastically different.
The generated image is limited to the structure of the original painting. In art, you can draw something larger or
smaller than usual to achieve some effect. There's no such thing here because the loss is localized
(You can't just change the absolute position of objects in content image.)


Two ideas: artists "zoom in" to a spot when they want to paint it in more detail. Is it possible to do so in our architecture?

Second idea: if the generator network can generate styled images, why can't it be used to reformat the feature layers?
Turn the feature layers into something that look like other artforms (without loosing too much originality)
and generate ... But how's that different from just use the generator network to generate the image? Not really

I want something other than difference squared loss. I thought about nearest neighbor loss of the normalized feature
layers. That one is invariant to translation.


Trying two things now: one is to directly observe what is going on in each loss function for each feature layer.
Another is to feed multiple styles into the network while modifying the same parameters.

simply feed 15000 shirobako images as style didn't work.

I tried the mask again using hand-segmented shirobako mask (mrf loss). Surprisingly it's sort of working. Maybe I don't need the
content loss. I can just create a drawing from scratch. Maybe that would be too hard.
One more difficulty if I choose to go on this direction. I don't have hand-segmented masks for me to train.
Even if I do, I don't know how to incorporate say 10 masked style images together. Simple nearest neighbor matching on
the 10 images would require 10x memory in the gpu.

One idea for automatically label images: start with example image and a hand-labeled mask. Now for each input
we compute the nearest neighbor of say conv4-2 (any high level layer) and assign the nearest neighbor's mask
to the input layer. Then add the constraint that nearby pixels should preferrably have the same label.
Then do deconv to get pixel-wise label for the original image. (Sounds like a plan, but this requires the
high level layers of the style and content to be similar, which may well not be the case. 20161122)

Texture and placement of objects are two different things. Texture is given a mask, how to fill in the colors
so that the style loss is the least. That is already solved. Placement of object is another issue. How to
place objects relative to each other so that it is the most probable.

I tried the nn loss. NN is not differentiable so it has no gradient. I should've realized earlier. Now I probably have
to get around this problem.

One thing is, the reason why mrf can perform better is because it has those patches that captures info about nearby
cells. Gram simply multiply them together and add all cells. There's no interaction between a cell and the cell on its
side. On the otherhand, the mrf is expensive to calculate because of the nn step.

After some thinking, it comes down to knowing the feature layers first. (like each of the conv layers, when they're
activated and compare two inputs, content and style, see when their conv layers look similar.)

Adversarial generative network may be worth investigating into. But don't do that just yet. I still have to finish up
this experiment.

We can make the additional semantic masks learnable. Just add a loss for too much deviation from original (more
complicated loss can be added later)

Things learned from the experiment: the conv layer won't look close to the original image. So we can't hope to morph
one image into another in the conv layers. Also, making semantic masks learnable will need further refinement.

I thought maybe finding the spatial correlation of semantic layers might be helpful. Now it is only finding the
correlation in-place (that is, the same pixel times the same pixel in another layer). What happens if we shift it by
say half of the width and find the correlation? The hope is that , for example we have two eyes, one on the left
and one on the right. By finding the correlation after shifting, we may find: ah whenever there's an
eye here, there will be on on its right with distance half screen away. That's my hope.


I was correct that correlation after shifting one layer encodes the relative positional information. Now the problem is
1. It was not perfect. I hope the problem can be solved after adding a content loss.
2. It was spatially too similar to the style image. ie. no shifting etc. There was some shift for subfeatures, like
the mouth was on the right first and slowly came to the center. But overall the head position was still the same, the
position of everything was the same as in the style. This is not what we want. We just want to modify the content image
a little so that the eyes become bigger, or the nose become less noticeable and things like that.
I want to have two very simple images as input and see how it goes.

Not going well so far.

Side projects during thxgiving:
Run multi-style feed forward network.
Shit.. I realized it actually doesn't make sense in the current frame work. I have to separate each style by itself
instead of feeding them in as batches. ( Otherwise I can't set the image placeholder so I can't train scale and offset
individually. I'll try to fix this tomorrow by going back to the master branch and merge the two..
I don't know if I should keep the batch style... Maybe I should.

Tested overnight on claude monet's paintings. Now I'm sure I can't just merge styles like that. It doesn't work.

Other future directions: feed forward neural doodle.


As I think of it, maybe we don't need the content image at all? There are many things that are hard to find in real life
but exist in drawings. I know that a feed forward network with masks can be trained as long as I have enough training
data. The only problem is where to get that semantically labeled training data. I can get that from 3d model... I think
I can look into that direction. No content image. Content loss comes directly from thousands of style images. Details
of how content loss can come from those may be filled in later but I think this is the right direction. We don't have
to worry about distorting content image into the style image, which is a big challenge.

Before that, let me modify the current code so that it can accept only semantic style images and semantic masks and
learn them using feed-forward nn. the loss... we can use content loss or style loss. try both I guess. I need to
copy the previous code into a new file because the change will make the code incompatible with the previous version.
(since there no longer need to be a content image). Actually, I can use the same file... Since I may use that framework
to learn auto generating textures.

Basically I will implement feed forward neural doodle, but better than what they've done. I need not only rgb mask, but
any number of masks for each style image.

2016/12/09
I implemented fast neural doodle. Testing it. It is running super slow for some reason... I don't get it. I should test
running the normal version... (without masks and doodles.)

2016/12/10
Fixed some bugs, but the thing is still super slow, even if I don't turn on masks. Was it like that from the very
beginning? I think so because it takes the whole night to train around 20000 rounds, so it's like one round per second.

Also the loss for style is unnaturally high only after I apply masks. I found that was because I was doing dot product
so each number is dotted with 255. lol.

But the result was still unsatisfactory.

2016/12/11
What can I do that will have some originality instead of repeating what people has done? What is the limit of the
current system?

If working on masked images:
    You must first have a mask, whether it is based on real images or not.
    The mask cannot be too detailed, or it will use up too much memory (grows linearly) and time it takes to create
    such a mask is prohibitive.

If not working on masked images:
    The model clearly does not have an idea of scale. If you train it on a 256x256 image, you can't just put that
    on a 1024x1024 image because the patterns created will be too small.
    It has no understanding of the relative size/location of objects (unless the two objects are close enough that they
    become in the same cell after convolution. That is a problem for vgg, but since we're using gram matrix, the
    problem is also applicable in style generation. For this reason, it is still impossible to tell it to "paint a cat"
    and expect it to just work.
    It also has no idea about different style between different objects. See comparison between mask and no mask.
    It

--
The thing is not working, at least not on johnson network... I don't know what's going on, but my guess is that johnson
might not be the best choice for texture generation with masks. I'll try the pyramid network...

Planning to try pyramid network
read blog and source code on feed forward network and see why mine did not work.
The johnson feed forward network is definitely working. It just takes a while to train. It get's faster(to a tolerable
level) when I decrease the batch size from 8 to 1. I'll try that on the masks.

2016/12/12
on going work:
Trying pyramid network without masks. Just normal content input.
Planning to try johnson network with style generation only. No masks. -- It worked!

I'll try to replace input to "johnson network with masks" with random
noise instead of the masks themselves. It probably will get better after intensive training... Usually generating image
with random content/mask takes a while to train. Ok there's definitely something wrong with the masks. The masks
changed their meanings in the middle of training, so we're definitely loading them wrong. If I'm lucky, it might just
be the output but not the training (judging from the quality of generated images).
I tested the output. The output's order is correct. So it's the training process... But I don't get what part went
wrong. I tested getting the files. That part was correct. ... Plan: debug this. I don't know what it will look like
after more training, but now it's not so satisfactory yet.
It is interesting to see how each mask acts. when value of a mask is 0, it doesn't mean that it will be black. Instead
since it indicates that it is not a certain mask, it becomes the color of other masks.
But when only one mask has all 255s and the rest 0s, the result just becomes the same as no masks for some reason..
I don't understand why, but it seems the network is doing something wierd.


Also working on finding out why pyramid generator did not work so well. The image generated was just too blue...
I first thought it might be that I forgot to preprocess the image or something like that. But I checked and it's fine.
So I thought it might be the generator net. I want to test that with simple "style-only" task. That failed ... old
problem.

2016/12/13
Re-reading the feed-forward paper and trying to figure out why my network did not work.
Differences:
    1. They used a uniform noise distribution instead of the whatever distribution numpy offers. (fixed, changed all to
    uniform random)
    2. according to the original paper, normalization is done also before concatenation layer. (while in another paper
    it says otherwise. Personally I think it is not necessary to have normalization there, but I want to at least
    reproduce the result of the author. (fixed)
    3. Initializations (fixed)

Humm... Where's the problem???
Maybe it's not in the generator network??
Alright I give up... I don't know why it doesn't work. Pushing this off to tomorrow.

Task 2: Johnson network with masks:

Task 3: improving slow version.
    I was comparing the texture generation network (with and without mrf, with and without masks). I found that the key
    lies in the loss function, ***so if I want to make something new, I need to invent a new loss function. ***

    I tested style generation (no content image) for mrf loss and gram loss. It turns out that both are effective,
    although mrf loss is better at preserving local structure and information. gram loss knows little about the
    information in the current pixel compared to say five pixel away. mrf handles that pretty well (maybe too well, I
    suspect that it is most of the time just copying instead of learning).

    I then tested semantic masks
    mrf failed when I simply append the mask to each layer. I think it is because the magnitude of the image (0~255)
    and the vgg layers are different, so nearest neighbor didn't work so well (that's why the author said it is ok to
    simply pass the mask into the vgg as an image. Since this will guarantee that they have the same magnitude as
    all other feature layers.
    I tried to dot the feature layers with the mask in mrf. That also worked, but then the gram matrix and the nn
    convolution becomes pretty large. Both make sense though. Dotting means that: each semantic mask is independent to
    other layers: the style of the eye has nothing to do with the style of say the hair. On the other hand, appending
    means that: it is still possible in nearest neighbor that: although one pixel belongs to another semantic mask,
    it can use the style of the other semantic mask if vgg thinks it is similar enough to another pixel... Considering
    the inaccuracy of human labeling and the ram, the second one is more preferrable. #Plan: I will make the change
    and basically delete resizing. (before that, I need to test if appending is really preferrable to the dot product.)

    Another problem is: Since we can't use nn in feed-forward nn (It's probably too hard for a generator network to
    learn nn), is there a better loss function or any modification I can make to the gram loss function? I came up with
    the "gram stack" and showed that it can reconstruct local structures to a good extent, given enough stacks. Can I
    use that? (like have stack = 5, 2 left, 1 center and 2 right?)

1. Fix resizing/dot/feed into network and append. Compare the three throughly.
2. Find alternative loss function that gets the good side from both gram and mrf.

Tasks not done yet:
1. pyramid generator... still doesn't work. it should be something simple that I'm missing
2. Johnson network does not work with masks.
3. A better loss function.

2016/12/14

1. Johnson network:
    I looked at the network itself. THe first layer has a filter of size 9. That is huge. That's probably why the
    result looks so blurry. And that size is the limit to the size of the features. Notice the image it generated
    (without masks) when I train it for texture generation. The features are always of a certain size, not larger nor
    smaller. That is probably one limit of the network.
    I also guessed why the network no longer works when I added masks. The input I used was noise + masks, basically
    appending the masks as additional features. That didn;t work and I guessed that it is probably too hard for the
    network to learn the dot product. So I thought I can just do it for the network. So now the input would be
    dot(noise, masks) and the dimension would be W x H x (3 * num_masks). Let me try if that works.

    So far it looks a lot better than before... With more training it can probably get even better.
    Uh. it's sort of working but not really. It just looks ugly as hell and the houses layer doesn't look like houses
    at all.

2. Alternative loss function.
    I modified the "gram_stack" so that it now calculates the gram between the original layer and the shifted layer
    where the shift is applied within a window (so if it's a 3x3 window, there will be 9 gram matrices)
    This works, to some extent. The image is definitely better than before. Now we can clearly see the houses and the
    doors whereas before it was just a blur. The feature size is limited to the size of the shift though...

    Also a side note, this might be way down the road, but how do we make sure the sky looks consistent? I mean the
    swirls are going everywhere in different directions, but in original graph they are consistent in one direction.
    Should I do something like add tv loss to the feature layers? ( Not just the final output) But Again that might not
    be a good idea if the feature layers are more like an edge detector (it would have huge tv loss). It's worth trying
    though.

2016/12/15
1. Johnson network:
    The network just doesn't behave like what it should be doing. That's why the style generated looked so poor. When I
    feed in different masks, the output doesn't look like it contains only the content that is supposed to be in that
    mask. Also the output changes when I add say a bunch of zeros in the upper left corner. As I mentioned before, it
    seems like the mask is just remembering whatever is not inside the mask. That's why when I give it zeros, it will be
    filled with the texture not in the mask, instead of just blank.

2. Alternative loss function.
    I compared the style generated by mrf versus my new loss function. It seems that mine is overfit on the small
    details. I thought maybe by changing the style layer that we're fitting it to, the result can get better.
    I did it. By changing the style layers to the one mrf uses (relu3_1 and relu4_1), and by changing shift size to 4,
    I got a much better style regeneration than the previous ones. It was able to recover some of the objects in the
    original style image. One small problem is: the layout seems to resemble the layout of the original style image
    a little (not noticeable unless you pay attention to it). Like the buildings are always on the left side,
    resembling the original style image.
    Tried to fix normalization but ended up using the current one.. It works good while the other ones don't.

    I want to try this on content images. Style/texture regenration was successful.
    Oh shit I was using the wrong masks.. I also need to increase the weight of the styles.

3. Pyramid genration network.
    It seems like the author did not use the pyrammid generation network in his github. He used a conv-deconv model,
    similar to Johnson, but with small modifications. He added a skip layer and appended those after each conv step.
    He also added a lot of noise layers along with each skip layer. The downsampling layers have 8 layers but the
    noise layers have 16...

    That one is guaranteed to work, but I still would like to figure out why mine didn't. Let me try to increase the
    number of noise layers... Don't know if that will help.
2016/12/16
Alternative loss function
The loss with respect to the shift size is kind of wierd...
6:1.80673e+08
5:1.41603e+08
4:1.05345e+08
3:7.28106e+07
2:4.48457e+07
1:2.26033e+07
I guess if we keep the shift size constant, it's not a big problem.
Yesterday when I compared my new loss with the old one, I think I had some bugs
and I should compare them again.

My implementation results in image that is too detailed. (No interesting features on
a grand scale, just local ones). I then tried to change the lr and it worked... I don't know why
but the features now became of a larger scale. What I mean is the brushes looks larger and more
coherent. This may be just an accident, because I don't think the system is designed that way. I need
to find a way to make coherent brushes.

I tested my new loss function. Although there is noticeable difference when I do texture generation,
when I added the content image, there was essentially no difference between the new loss function and
the gram loss. Maybe that is because we're using layer 1-5 and layer 1 is just too detailed? (So if we fit a layer that
looks almost like the original image, then the result would look like the original image as well...) I'll try the
layer 3-5.
... Nah it is just hard to tell the difference. The old problem still exists and the variance in the result each time
is just too large. The biggest problem is still the inability to recognize objects and paint them accordingly. If I
want to paint the sky in some way, I don't want to paint the ocean in the same way as well (unless I want to do so
intentionally) because now the person will have no idea where the boundary is. But that can be solved by adding masks,
and the more detailed the mask is, the better the painting would be. One question would be: what if the thing i want to
paint is not in the style image? Using neural network sort of solves that image but using masks won't. Also relying on
a single image almost guarantees that you can't draw something that is not in that image.
So the solution I can think of:
1. Use a hierarchy of objects. If I want to paint a human, but there is no human in the kb, we find the next closest
level: mammals, animals etc and paint the human like that. But that would require a kb of how to draw different things
as well as their semantic relations.
2. Enable training on lots of masked images, not just one. This is essentially building a kb. But I don't know how to
even train one feed-forward network on one masked image yet... So this now becomes my priority...


Feed-forward texture generation with masks.
    I read the source code in torch and compared against my code. One big difference I found was that it had one gram
    matrix for each coloring. I had one huge gram matrix for all the coloring. Theoretically mine should not affect the
    result (because no color overlaps so most of the gram matrix would be 0), but I'm going to rewrite mine just in
    case.

    Link to the torch source code: https://github.com/DmitryUlyanov/fast-neural-doodle/blob/master/fast_neural_doodle.lua

2016/12/17
1.Testing gramian_with_mask
    It works fine in back-propagation mode.
    The feed forward is still not working after training for 5000 rounds. This is fustrating...
    check the result for texture generation without masks and contintue training? It seems to be working. More
    training will result in better result.
    I will code the new feed-forward network and hope that saves the day.
2. New loss function.
    Ok why my loss function didn't work? It worked perfectly without masks. Then it starts to break down a little after
    I added the masks. Then it completely becomes the old gram loss after I added the content images. What happened in
    between?
3. Refactoring the code: Went till line 224 in n_style_feedforward_net.


2016/12/18
1. Testing gramian with mask
    Continued training from yesterday.
    Hum.. the network also sort of failed... The result just looks super wierd.
    Trying to repeat the experiment using content images...
    Ok The skip_noise_4 network is working. At least it is working when it generates the style without masks or content
    images. *

    Conclusion:
        johnson network sort of works to generate textures. The boarder looks wierd though and the features are small,
        probably because I trained using 128x128 instead of 256x256.

        skip_noise_4 generate textures successfully (tested two times by accident)
2. New loss function.

2016/12/19
1. Testing feed-forward with mask
    I don't understand what's going on. The new skip_noise_4 network now had the same type of errors as johnson. The
    masks seem to switch from training to training. The network applies this texture to this network during this round
    and the other texture to the other network in the next round... I'm really lost and I don't understand what is
    going on.

    Tried to have input mask be constant... Still the texture generation didn't work. That doesn't make sense...

    Style generation seems to work. But it looked different from the ones online. Maybe the style loss was too low.
    I set it to 25 because 100 seemed too high. Plan: test using style weight 50.

    I guess it's time to download some repos online and see what they have.

    No online repo found on fast neural doodle, except for the Dmitry one. I am planning to read his whole source code.
    I found that the noise he feeds in has three parts: mask, mask times uniform noise, and noise. I only fed in noise.
    That is fed into the network's very first layer.. He also have some additional noises in skip layers. I'm not sure
    how those work yet. I assume that they're all uniform random.

    Still didn't work after 30000 iterations... GG. There;s something else to the network.


2. New loss function
    No work done.

3. Back prop MRF with style as real images.
    According to https://github.com/jcjohnson/neural-style/wiki/Similar-to-Neural-Style it seems to work well when
    style is real images. Trying that... That worked pretty well when the style weight is kept low and when the content
    and the style image have similar contents (Mostly because then you can't tell if I replaced one object in the
    content with another object in the style image. Again, mrf does mostly copying but not learning.)

    Trying to stylize some other image. I found that there were some wierd horizontal lines in the generated image.
    I may need to fix that. I first thought that it was because of the resized style image. Then I resized them
    manually and the problem was still there. So it must be caused by something else. Maybe the dimensions were not
    divisible by 2?

4. Future direction
    I was thinking about what to do next. I thought before that I can train a feed-forward network on multiple style
    images so that it can draw a wider range of stuff that might not be present in one single style image. But then I
    realized that there are limits to the masks. I mean you can say "A man facing the sun" is a kind of mask and "a
    man facing away from the sun" is another kind of mask. But then that requires too many masks in total. So I need
    some extra parameters to train... I need a semantic tree and I need a smarter way to represent the same object in
    different environments.

    Maybe also reimplement multiple styles feed-forward. But that's not so necesssary.


2016/12/20
1. Feed-forward with mask
    Only works on the boarder line. Don't know why.

2016/12/21
1. Feed-forward Style genration with johnson network.
    style weight = 50 seems to work fine.

2. feed-forward with mask
    I thought it was working... I replaced .jpg with .png masks and at first it seemed to be working... Then it just
    stopped becoming better after round 10000. The sky looks a little bit like the given style but the rest didn't
    look anything like the original style image. I think I've met the same problem before when using johnson network.
    So it has nothing to do with the network or the inputted mask images. Interesting. I tested feeding in masks
    where only one mask is 255 and all others are 0. It outputted the same result as before when I used johnson
    network...

    If this still didn't work... I don't want to get stuck at the same place for a week. So I'm going to move on.
    Maybe I should change the output so that it has not 3 channels, but 3 * num_masks channels and I manually dot
    the output with each mask and combine the final result.
    ...
    Wait, how do I calculate the loss again? I dot the gram matrix with each mask... Hum... There must be a problem
    there. That's one of the two possiblity. The other one is I'm not feeding in the right noise.
    Nah there can't be a problem there because I'm using the same loss function in the slow version.

    I just realized that the version online didn't use any other input except the mask themselves! The code is there
    to support additional noises and noise dot mask but he didn't use them (he set the parameters to 0)... gg


3. Back prop slow version
    I want to try the non-mrf version and compare them. The biggest problem with mrf is: it is too aggressive and it
    does not recognize objects well. It is agressive because it usually either does not change the object at all or
    completely changes it to another objecct in the style image. It does not recognize objects well because, well, the
    limits in vgg network. It does a poor job recognizing, say a black contour of a human compared to that of a tree.
    We're helpless unless we label each part. But then most of the time the images are too complicated to mark what
    objects are in there.
4. Future direction
    Since the feed-forward part is not going so well, I'm thinking things I can do with the slow version.
    I thought of attention network. So I paint one area at a time or something like that. That can replace the need of
    masks because masks are essentially manually added information on object recognition (which can be done by the
    network itself in theory if the network is good enough.)
    Now the bottle neck is that the vgg can't recognize objects well, and if it can't do that, I don't have any way to
    improve the current model except to feed in manually labeled data...

2016/12/23
2. feed-forward with mask
    I noticed that I did not normalize the gram matrix by the mean of mask values. maybe that is causing the problem
    (how do you expect two gram to be the same when you don't even know the number of pixels marked by the mask?)
    That's why the feed-forward network works without masks...
    Yep it's finally working... Plan: I still need to change the input back. The model probably isn't using the extra
    input anyway.

2. Future direction
    I thought about the style of an image. What does that mean? To me its meaning is two-fold. The first one is local
    statistic relations. This one is well captured by the gram matrix. The other one is more logical. If I always paint
    a man's face like that, then it must have some logic behind it (or not) and we can also call that "style". That's
    why the current neural style need a 'content image' whereas in real life people don't need a real image and make
    change on that. One is more on a small scale and the other one on a grand scale.

    What I can then improve? I implement removing the grid-like noises in the generated image. That has been done
    before.

    I can improve the gram loss so that it is no longer just on a local scale. That had some progress but I stopped. I
    should maybe continue that. I need to understand the gram matrix more. I mean I know how mathematically works, but
    why it works? I have no idea.

    I had an idea of how to generate masks for any image (as long as it is moving). There are object tracking
    techniques that can mark one object in a stream of images. Ah I can think about this later. It depends largely on
    object tracking. *

     I know my final goal: to make programs that can draw like humans, with some instruction from human.

3. Back prop slow version
    After I fixed how to calculate the grams, the "stack grams" with the masks, the result looks super impressive. It's
    even better than the mrf. The color looks more like the original and it definitely have some originality.
    After applying that to the content image, it lost its charm... Plan: figure out why.

2016/12/24
1. Back prop slow version
    I tried a different mask for output. It still works :) I doubted that it tend to generate images similar to the
    style image in terms of where objects are placed. Apparently when I'm using masks and generate only texture, that
    is not the case. But I tested with texture generation without mask and that seems to be the case. The texture
    generate had a similar layout as the original style image.

    But a bigger problem is it no longer works on content image. If I set the style weight to be too high, then it no
    longer looks anything like the content image. If I set it too low, then it is unable to generate those large-scale
    features like it did in texture generation. It is a hard balance between the two... That can be solved by applying
    different weights on different areas. The program is then instructed to be "more creative" in some areas and
    adhere to the content image more in other areas. That may be achieved by adding a "weight" to each mask. Currently
    all masks are treated equally.

    Or I can just dot another mask with the current feature layers. That way I'm no longer confined to having a uniform
    weight for one mask. I can apply as much/little style to one area as I want. This can also be used in the
    fast-forward version. (Although I guess it's a little bit trickier.) I can imagine one having a brush and as they
    put more "style paint" onto one area, the output changes accordingly. That would be amazing.

    Hum that sounds like a plan. I'm doing it. i need another branch... It is not so related to improving the new loss
    function (In theory it will make the new loss as well as the old one look better), but whatever.

    1.1 Style weight mask.
        It worked almost as expected. There is no style applied to the area where the mask masks as 0. One thing about
        that is: Since I do convolution on the mask, even if one area is masked as "do not apply style", after average
        conv, it is still possible to apply style on that area. I don't know if that is desirable or not yet. if not I
        can simply get rid of the average conv and do resizing. Anyway, so far it works with texture generation.
        I'm checking if it still works with style generation.
        Yes! It totally worked!!! It is almost unnoticeable that I applied different amount of styles to the same image.
        It looked so nice (way nicer than the previous one). I now have control over exactly where and how much style
        I'd like to apply to the content image. This is so nice.
        Next step is probably do feed-forward version of that so that the user can see instantly how the change affect
        the outcome. This is exciting work. :) Nice christmas present for myself I guess.

        I probably shouldn't get so excited because this is just the simplest change... But nevertheless it is
        effective.

        Plan: add the style weight mask to the feed forward network.
        Plan: delete the pyramid generation network. It's no longer useful. Everything else is working fine.

    1.2 No longer automatically resize style images.
        Resizing them is causing some issues with style/texture generation. If the width:height ratio was changed, then
        the generated style/texture will also change, making it look less like the style given.

Plan: Test the style weight mask in the feed forward network... Uh maybe the style weight 200 was too high. Or maybe the style weight mask was messing everything up.
Plan: Try generating styles with content image with skip noise 4 without style weight mask. It looks wierd for the first 3000 rounds

2016/12/25
1. Style weight mask-- feed forward version
    It is not working when I simply feed that into the johnson network in addtion to the images. The result is just not
    what I expected. I tried feeding in both completely random noise as well as mask generated by the diamond square
    algorithm.
    Nah. skip_noise_4 can't even learn the content image. Or it's learning it super slowly compared to how fast it's
    learning the style.
    # Plan: figure out a solution.

2. Scale_offset_only
    I realized that it may be done through just adjusting the scale and offset variables (according to the google paper)
    instead of feeding the entire mask as another layer of input besides rgb. Therefore I decided to test the
    multi-style training again.

    Oh right I forgot. I need to have two sets of those variables and I'm only outputting one image currently...

2016/12/26
1. Scale_offset_only for multi-style feed forward network.
    I fixed the network. Now at least Johnson network works fine with multi style.
    I re-read the google paper "A Learned Representation For Artistic Style". The pictures they provided for several
    styles were not particularly satisfying. I think the slow back prop would do a much better job on those pictures.
    The problem was still mainly on applying styles on places that it shouldn't. Plan: I will compare the feed-forward
    model, the multi-style feed forward model, and the back prop models and see what's the difference between them.

    It will take a while so i'll do that overnight through a script. style weight = 50 seems to be too low for some
    styles so I'll increase it to 100. The network seems to work as expected (meaning it doesn't work so well yet. lol)
    and the styles can be nicely described by only the normalization factors. I still need to understand why before I
    move on though. The two biggest question I have is: why Gram matrix work? Why normalization work as separating
    between styles?

2. back prop.
    Plan: Will test between the new and old loss systematically.

2016/12/27
1. Back prop New loss.
    There is no noticeable difference between the new one and the old one when it comes to applying styles to content
    image. As noted before, there is a huge difference when I use it to do texture generation.

    I thought the size of the content image may affect how the patterns are generated (or rather how well the patterns
    are generated. Currently they're just general patterns. They don't have any objects. But in the texture generation
    it can learn to generate larger patterns/objects as well.
    After testing with size 128,256, and 512, I found that the pattern generated is not affected by the size of content
    image so much. Then I remember reading yesterday that the initial value is very important to texture/style
    generation. Now I'm trying to add manually some patterns to the content image and see what the result is. I may
    have to also try to modify the noise I feed in (fill the noise with more patterns for example)
    Nope... Simply drawing lines to guide the style doesn't work. lol. I need to figure out how gram matrix really works.

2. More meaningful parameters:
    I read an article on GAN yesterday and they mentioned a way to make the "noise parameters" fed into the model mean
    something.

2016/12/28
1, Scale_offset_only
    I ran two trials with scale offset only = on and off for each one. It seems that scale offset only = off will have
    a better regeneration of the content image without losing its ability to regenerate the style (I mean it's as
    good/bad as scale_offset_only = True, most of the time clearly even better than that). That means the scale and
    offsets may not be the only factors affecting the style? Or it could be that the thing just works better when the
    generated image have a smaller content loss (because in theory when scale_offset_only = False we're essentially
    training the rest of the parameters n-1 times more compared to scale_offset_only = True.

    For both setting, there are artifacts in the generated image. Sometimes it's an completely black area and
    sometimes it's completely white (the color is consistent for each setting). I think it may have something to do
    with the scale and offset. I knew that there are some bugs in tensorflow's standard deviation calculation.
    Maybe I should try to add the absolute value. It is consistantly appearing for every single training. Maybe the
    learning rate is still too large. I'm guessing what can cause such artifacts with usually an oval shape. The
    artifact appears at the same place and shape for all styles so it has nothing to do with scale and offset.
    Maybe I should print all variables and see their values? Find the abnormal one? I'm guessing there might be a
    nan somewhere.

    Also the feature size seems to be too small compared to the picture. Maybe I should do training first with small
    image size then increase that??? I've seen people do it in one git repo. Ok the feature size has nothing to do with
    the size of the style image. I was mistaken. I thought because the convolution kernel size and stride are fixed,
    the interpretation of the cnn on style image of different style would be different. I was wrong. But why???

    Also the artifacts are still there... It's not the mean and standard deviation...
    Maybe it's relu!! Maybe I should use leaky relu or elu or something. But the same artifact did not exist in
    feed forward texture generation.

    Hum... I think the artifact is somewhere in the convolution part. It can't be caused by having a large learning
    rate (because then it would also be a problem in texture synthesis). Also it's probably not dividing by zero
    because you can see non-white pixels in the artifact area.

    but there is no such problem before I made the multi-style change... So... the problem must be in the multi-style
    part that is shared by all styles...


    Plan: fix the feature size and the artifacts (artifacts first)
    Plan(done): test the effect of style image with different size on the output result.
    Plan: test the effect of changing output size and see if the patterns generated become bigger... I don't think so

2. Study the model.
    Plan: read the GAN article on making meaningful noise parameters. I read it. The math was pretty complicated but
    I got a broad picture of how it works. It is inspiring. However I don't see how to connect that to my current work
    yet.
    I can't find an explaination of why gram matrix works. It's magic.
    Plan: Can we build an even better loss function? Can we build a neural network that basically classify styles and hope
    that it automatically contains enough information to build a better loss function?

3. back prop
    I thought the masks were not working. but it turns out that the backprop is not working on only one part of the
    content image. I'm testing the effect of other masks, but this is really wierd.
    It's definitely the output mask or the style mask... Even if I change the image, the same part/area is still not working...
    But even if I change the content semantic mask it's still not working...
    If I change the size then it's working...Something wierd is going on... it must be something stupid I'm doing wrong.

4. new loss function
    No progress made.

5. Read nijigen stuff

2016/12/29
1. Scale_offset_only
    Artifacts: Plan: trying elu instead of relu... Don't think it will help though. Hum it seems elu fixed it.
    (or it could be I decreased the number of style images, but in the past i've seen those artifacts even when I only
    have two style images.)


2. Study the model
    I need more control over the generation process. I have no control over how the patterns are generated or what
    size those patterns should be.

    Plan: figure out what controls the size of patterns using back prop first then generalize to feed forward
    (like the size of brush or size of triangles etc.)... Wait the back prop and the feed forward are different.
    Feed forward seems to be more dependent on the size of content image fed in during training.Need more testing to
    confirm that.

    I tested the effect of content and style images with different styles. I'm now 100% certain that *The size of
    content and style images does not matter. The features generated have the same features sizes.

    I realized that it may be related to the kernel size of the vgg layers. The style comes from dotting each pixel in
    feature layers with the same pixel in another feature layer. The value of that pixel is determined by the algebraic
    computations in the vgg layer. I thought the vgg layer will get activated differently if i change the size of the
    style image or the content image, but it seems that it is largely the same (this may be explained by saying that
    if you draw a square and change its size, then no matter how you change it, unless you change it to a tiny 3x3 size,
    a conv layer with kernel size = 3 will have the same activation for most of the pixels no matter how you shrink it.

    So the feature size is dependent only on the size of the kernels in the vgg network and that's why no matter how we
    change the image size, the resulting texture won't change a lot.

    Hum...


3. back prop
    I think the bug I met yesterday was caused by .png masks. Although invisible, there are layers covered by the top
    layer and those are messing around things. If I change it to jpg it should be fine.
    Ok it's definitely something wierd with the mask for content image. If I make a brand new one then it's fine. But
    if I try to reuse the old one in any way it breaks.
    yep that's the bug... I still don't know what caused it but well. I found the solution. It took me a whole day
    to figure out...

4. New loss function
    I kind of knows now how I can get a larger pattern. (rather than a local one). That lies in the correlation between
    one feature and the features in nearby cells.
    Plan: testing the effect of stride and shift on the new loss.
    ** Larger stride results in textures becoming more and more like the style image itself. That is probably
    due to the fact that we used lower layers as well as upper ones. What if we only apply the larger stride on upper
    layer? (The reason behind that is: lower layers represent local details and overall layout. If I use large strides
    on lower layers, then the overall layout will be also set as the same as style image... Let me test this.
    I was wrong. I was only using layers 3 and 4. Now I'm trying layers 1 and 2.

    Using layers 1 to 4, it seems that smaller stride will result in slightly smaller features/objects. Larger shift
    and larger shift will result in textures looking more and more like the style image in terms of layout (stride more
    than shift) This is a pretty large draw back...

    Plan: use layer 1-3, 1-2, 1... and look at the difference between them while using different shift and strides.
    It turns out that using more layers (especially the upper layer) is better for generating larger patterns.

    Also I haven't yet figured out why the patterns disappeared when I use a content image. But that's probably less
    important now. The new loss function is just too buggy and hard to control.



5. nijigen
    I haven't read the article, but I started to think how to use my stuff to draw something given a sketch.
    So let's say I want to draw a tree. I marks where the leaves should be and where the trunk should be. Then I let
    the program to do the rest for me. Does that sound like a reasonable goal? Yes and I've already accomplished it I
    think. I haven't tested on how well it works on more complicated styles (like drawing a human), but if it works,
    then I should start building the website to let people mark their own masks and use their own images...
    I should also if it works as well in feed forward mode.

    Also, it comes to me that: what happens if I choose my training data for the feed forward net according to the
    content of the style image. So if the style image is a human face, then most of the training data would also be
    human faces. Would that make the result look better than training the style for a human face on any random image?
    This is just a random thought.

    #Plan: testing mia.

    Neither mrf nor gram loss is perfect... gram loss is more flexible but too crude. The mrf has higher quality but
    looks too much like the original one without the content loss. I would try guiding the image with the result from
    mrf. First let me try to guide it with the style image (since I'm using the same masks for style and output and the
    best result mrf can get is to be exactly the same as the style image).
    # Plan: guide gram.
6. From previous todos:
    The skip_noise_4 network can't learn style generation with content image.


2016/12/31
1. Nijigen
    I tested the grided loss functions. With guidance mrf performed a lot better than gram. I'll try a different mask
    (different from the mask of the style image) and see if mrf still does well. I don't think it will though.
    Nope just as expected. lol.

    Also If I want to generate genuine looking faces instead of just copying the current style, I probably need
    feed forward network. Maybe I need GAN? That seems a little more promising than using style generation to generate
    contents...

    I found a blog that had amazing result for coloring sketches. I'll probably spend lots of time on that...

    Plan: what is deconv layer (or conv_transpose) ?http://www.matthewzeiler.com/pubs/cvpr2010/cvpr2010.pdf

    Ok the majority of the implementations are done. The only thing left is training data. I tried it on real life
    images. it turns out it is too hard for it to learn (understandably. There are way too many edges...)

2. Scale_offset_only
    Testing if elu really worked. Also testing at the same time the effect of training first on smaller images then
    on larger ones.
    Shit it's giving the artifacts again... So it's not the elu that's causing the problem.

2017/01/01
1. Nijigen
    Uh it sort of works but not really. The color was not 'vivid looking' at all. Low contrast is the technical word.
    Also adversarial net wasn't working so well either. I changed some parameters around but I don't think that will
    really fix things. I'm also downloading more training data.

    After training the net for a while (without adversarial net), it seems to be doing well at least on the training
    data. If I feed it brand new data, it can at least tell the background apart from the rest... But it didn't do much
    other than that...
    I forgot to separate the training from the test so I did that. I probably will try the new training data
    tomorrow if I finish downloading them tonight.

    Plan: I also need to ask the author what he used for the training data of inputs with addtional "helper
    information"

2017/01/02
1. Nijigen
    I finished translating it to Chinese. The color was still a problem, even after I added the hint. The hint doesn't
    seem to be working as it was supposed to. It had the kind of "old photo" color.
    I'm trying the adversarial network...

2017/01/03
 1. Nijigen
    I confirmed that no matter if I use the discriminator or not, it will not color thing correctly. Everything looks
    like old photos. Maybe the adversarial network is not set up correctly? I should take a look at other
    implementations of gan and its relatives.

    I found that I implemented several things wrong in the adv network. First I normalized the first input layer, which
    according to the dcgan paper will cause oscillation. I also got the loss function wrong. I naively thought that
    my implementation is equivalent to the more formal cross entropy loss. That doesn't seem to be the case.

    I did not increase the batch size (the paper used a huge batch size) because I want to quickly check if the result
    is working. I will switch to larger batch size once I know it's working.

    If it is still not working, then it might be the training data. I am downloading more training data from pixiv.
    I will have around 200g of data to train, which should be more than enough.

    I'll go ahead and post the chinese translation while working on the english one.


    Nah it's not working... It is generating wierd colors.Did I get the loss wrong or is this the natural reaction...
    It doesn't seem natural.
    After checking the change in loss after each training round, I found that the learning rate was too large. Smaller
    learning rate or larger batch size will generate more stable result. That's why only training through adv did not
    generate satisfying-looking images even after 50000 rounds of training. This sanity check was helpful...
    The following test confirmed my guess. I tried to force the network to learn the l2 loss for the first 10000 round
    and let go after that. But the network went back to the old messy looking image after I let go. So the network
    actually prefer the messy looking one, which is completely nonsense. It is probably the adv network was too weak
    (as shown in the sanity check...) the generator network always outlearns the adversarial network for some reason.


    I think the sketch generation has a 'bug': when it generate the sketch, I forgot to set a threshold. Therefore,
    most of the area is actually filled with almost invisible but nonzero values. This may provide hints to the cnn.
    That's why the cnn can learn the shades so successfully... GG. Plan: I need to rerun the tests after changing the
    sketch util to set a lower threshold for sketches.

    Doesn't seem to be the case actually. I ran the network using the new sketch generation method and it still looked
    fine (as good/bad as before).

    I'm trying to filter training images where there are too many lines (the sketch that takes more than 20% of the
    white space is filtered). I might end up with not enough training images...

    I found that the training set had surprisingly few number of images... I'll reupload the training data. I will
    also definitely download the larger dataset and train on that... I believe the method itself is fine... I hope.

    But why the adversarial network is not working? The only difference I found between my implementation and the one
    online is that we use different networks and they use a much larger batch size... Also they probably had more
    training data...

2017/01/05
1. Nijigen
    Higher batch size is way more important than I thought... It provided much stabler training process and better
    result simply by raising batch from 1 to 8. The hint didn't work as well as the original author had shown, but it's
    better than the previous versions. Given more training time and data, maybe it can get even better.

    The adversarial network was still not performing well... It's training super slowly and it's getting no where..


2.  Text dcgan
    This is the idea I came up with last night. Instead of feeding in images, I can feed in text in certain encoding
    and see what happens. I may come up with a good enough sentence generator.

3. Random thought
    I just thought of one thing. Why can't I have a network that take two inputs: the style and the content, and
    train that on the gram loss? So the structure would be similar to unet, except that the conv will take the style as
    the input and the content is fed in by concatenating the resized version of that to each deconv layer. (So
    the structure changed slightly from the unet to take two input images). Maybe that can work... lol.
    But I'm not so positive. I haven't seen the network work fully as described by the blog yet. The color is just not
    satisfying. I was hoping that much more training data could save the day (or higher resolution. 128 was just too
    hard even for a human). We'll see.


2017/01/06
1. Nijigen
    Still downloading the dataset.

2. Text dcgan
    I found a great explaination for why text dcgan won' work:
    https://www.reddit.com/r/MachineLearning/comments/40ldq6/generative_adversarial_networks_for_text/
    I need something continuous. Then I thought of chinese characters.

2017/01/07
1. Summary
Try to solve problems not try to use new tech

The problem I am facing now is I can't color the sketches. The colors look like those from the old pictures.
I used three methods. One is train against l2 loss. The second is train with 5x5 random hints on the color to be used
against the same loss. The third is train using GAN. The first and second is some what successful but not satisfactory.
The third is completely shit. The reasons I can think of is as follows
The first one fails mainly because it is almost impossible to predict what color one used simply based on the sketch.
The result is therefore reasonable.
The second one still failed. I thought it was because the images were too small. Even if I gave the hint, it might
still be too hard. Maybe the way I am giving hint was wrong.  The third possibility was the lack of training data.
The third one failed I think mainly because of network design. I am not training it correctly. I may want to test if
the adv network can tell apart anime from real life images. If it can't do that. Well there's something wrong with the
discriminator. If it can then there is something wrong with how we train.
So this is my plan.
Regarding the neural style, I've done most of what the papers have mentioned so far. They looked as expected. I was
planning to improve applying styles differently on different objects but it seemed hard with feed forward network. I
need to improve the network design if I want to do that. Also, I should probably try different networks other than VGG.
This is the most direct way to improve object recognition because vgg is not trained on art works.
As of the NLP plus GAN, I am still experimenting. The biggest difference Between image and language is that language is
more discrete due to its abstract nature. I need to think more on how to approach it. Chinese characters are
interesting, but are they also discrete?

2. Nijigen
    a. No hint no GAN
        Waiting for training data
    b. Hint
        Waiting for training data
    c. GAN
        The Adv network works on discriminating between anime and real life. So it's not the adv network
        The generator network... I don't know. I need to log the loss.
        I implemented loss logging. The loss seems to be working fine. I think the network did not work due to a lack
        of training data. The DCGAN paper used 1 million + images for each training task. Mine had 20000... This is a
        huge difference. I will get the larger set of training data asap and I will not try training it further...

3. Text DCGAN
    The generator can't produce satisfying kanji. The adversarial net was too good.
    The problem still exist for kanji. There is definitely more smooth transition than pure english alphabet, but it's
    still kind of discrete. I see that the generator mastered some of the strikes, but the overall structure of the
    kanji generated was just bad.

4. Random thoughts
    I was thinking if I can use the techniques in style generation in nijigen stuff. I know how to mimic the 'style' of
    any image. What happens if I train a network on anime images and use that instead of vgg? Would the gram loss still
    work on that network?

2017/01/08
1. Nijigen
    Finally finished downloading. Decompressing will take a while...
2. Text DCGAN,
    I also realized the problem for kanji. There are too few of them. The DCGAN requires 1 million + examples at least.
    I realized yesterday that although natural language is inheritly discrete, it can be turned into a continuous
    problem(otherwise no ml approach would've existed for nlp). The problem is how. If I have a loss function and a
    generator network, then I don't need the adversarial network (yet). I can google CNN for nlp
3. Neural style
    a. Feature size
        I just found through training the back prop version that the feature is indeed only related to the vgg network.
        I reached the wrong conclusion previously because I used a style image that was not so obvious. If I choose one
        correctly, then it is obvious that the feature size has nothing to do with how large things are in the content
        image.
        One remedy I can think of is to directly change the conv layers of the vgg network. If I want the features look
        twice as big, I just need to change the stride from 1 to 2 or from 2 to 4. Generating smaller features would be
        tricker but still possible. I can provide an image twice as large, run neural style on it, and then reshape it
        back to the original size.
        Plan: Let me actually do this. I didn't see people do that yet.

        I tried several experiments. Simply doubling the stride size in either conv or max pool layers did not work.
        I then tried to resize the filter so that they become twice as large. That worked to some extent but the image
        produced was blurry. Wait of course it is blurry. The network is not trained on doubling the size of the
        filters at all!

        Imagining the activation of one to two layers would make sense. If you try to make the activations be the same
        after you enlarge the image to twice as large, then it is clear that you should also make the weight matrix
        twice the size and keep the max pool the same. If you change the max pool, then you change the relationship
        between pixels and the masks trained is no longer valid.  Multiplying the ksize and stride of the max pool
        layer is more like making the shape of the features smaller. The result is usually not so satisfactory because
        the features should be already pretty small.

        Anyway it is not so satisfactory yet. The best I can do is to produce a blurry distored version of the enlarged
        style.

        I tried varying the style image size again, and again apart from sharper looking image, the size of the
        features did not change with regard to the style image size. This result is the same as I've tried before.

        Anyway, this is good for now. The main reason of failure I suspect is that resizing the filters is not good
        enough for interpreting the styles accurately. The distortion was too large and the style no longer looks like
        the original one. I know how to make the features look smaller though: just run it on larger content image and
        resize the result afterwards. :)



    b. Code refactoring
        I refactored the code a little and deleted unnecessary files/functions. It looked a little bit cleaner now.

    c. Fast forward both style and content.
        Hum... I need a lot of style images. Well I have them right? The pixiv dataset.
        Plan: This is also worth trying.

        The first experiment would be simply use the johnson network but change the last dimension of the input to 6.

        Well the result was: the image looked almost like the style image except with black and white color. You can
        see a tiny shade of the content image. I didn't think that the network can use the style image directly instead
        of learning some complicated loss function.. lol that was dumb.
        I can't feed the style image as an input then... otherwise the network will just use it directly on the output
        image.

2017/01/08
1. Nijigen
    Training using the new dataset. The new dataset was smaller than I thought. It is 10x larger than the previous one,
    but still did not reach 1 million. The size was 100 thousand.

2. Text DCGAN,
    After changing the generator to a more complicated structure and training for the whole night, it was clear that
    the generator did not to a good job. The best it could do was to produce the word "the" and "one" from time to time.
    I therefore decided to change the task to generating english-looking words.

    still failed,,,
    I realized that there are still definitely no more than 1 million english words in the training set. Probably on
    the level of 10 thousands, which is not enough. I'm planning to work on something else for now.

3. Neural style
    Still thinking

Plan:
    Run text dcgan for word generation
    Run color_sketches_runscript with hint
    resume color_sketches_resume_runscript to see if the network can learn anything without hint

2017/01/09
1. Nijigen
    After training for around 50000 rounds, the result was still not satisfactory. The coloring just isn't right, even
    with the hint. When using 256x256, the hint can be generalized to a larger area successfully but that property
    disappeared when I try to generate image of larger size using the network trained on 256x256...

    I Also tried to retrain a network that has been trained previously without adversarial network with adv=True. The
    result was not satisfying. The adversarial losses for both adversarial network and the genrator was decreasing
    slowly but the l2 loss became larger. I don't know if training the adv network along side the l2 loss would help or
    not. I don't think so personally... But I'll give it a try.

2. Neural style
    Planning to separate the nijigen stuff out from the neural style. The neural style project's reimplementation part
    is almost done. I just need more refactoring and that's it. So spend the time refactoring code.

    Refactor status:
        general_util: Done
        neural_doodle_util: Done except for adding some unit tests.
        neural_util: Done except for adding some unit tests.
        vgg: No need for refactoring because it's mostly other's code.
        mrf_util: Done.

        stylize: Done
        stylize_examples: Done writing large-scale tests for stylize that can also serve as examples.
        neural_style: Done

        conv_util: Done. Maybe add more test cases... Too lazy.
        skip_noise_4_feedforward_net: Done, don't forget to delete commented out part after testing.
        johnson_feedforward_net_util: Done
        n_style_feedforward_net: Done, but it's still a mess.
        feed_forward_neural_style: Done
        real_time_neural_style: Done
        generate_masks: Done


    There's another thing that I noticed. The texture seems to also change with respect to the size of the style image.
    TODO: I will investigate throughly.

2017/01/12
    worked on refactoring.

2017/01/13
    1. Nijigen
        Added function to get rid of black and white images from the training dataset. Now I'll try to retrain the whole
        thing on 512x512 resolution with batch size 8. It will take a while...
        I also got the "pixivutil2". If I found that it was the problem of not enough data, I will use that to download
        more data.

        Regarding the original author's suggestion to first train for several epochs without adversarial network then
        add the adversarial network and set the loss to around 1/10~1/20 of the l2 loss, I'll give that a try. The
        wierd thing is, previously I had adversarial part's loss be around 1/100000 of the l2 loss and it still was
        having reasonable changes to the model. I don't know what will happen if I time that change by 10000...
    2. Neural style
        The refactoring on back propagation part was almost done. I still have one stylize example to do but that was
        it. Next step is to refactor the feed forward part... But I want to put off that for a while. I haven't worked
        on something challenging for a while... Refactoring was more like a break.

2017/01/14
    1. Nijigen
        More data, more data... I used the PixivUtil2 program. Hope pixiv doesn't block amazon's ip forever...
        So far so good. It's getting around 60 gigs in the past 9 hours. By this time tomorrow I will have around
        600000 training images... I think I might be able to do things even more interesting than filling in colors
        given this dataset.

        Plan: check network structure. Maybe there were some problems there.
    2. Neural style
        More refactoring...
        Don't forget to investigate pattern size.

    3. CNN relation extraction
        Well I found several tensorflow repos but I forgot to bring data to try it out...
        My project is a little different though. I lack training data. I have to use active learning to gather more.
        The most naive way for active learning is after training for a while, put all possible combinations of
        pairs of phrases into the classifier, select the most uncertain one and ask about that. A little less naive
        way is to sample say 1000 pairs randomly and do the same thing. That will speed things up a little.
        Another thing I probably don't have to deal with is that the key phrases I have not may not be accurate (it is
        certainly not complete) and I may have to create a vector for the phrase by adding up the vectors for each
        word, but I don't know if that would make learning harder -- balanced tree is quite different from the word
        "balanced" and "tree" trained on other corpora.

        Anyway this looks much better than the situation I was in before.


2017/01/15
    1. Nijigen
        Still the same issues with the adversarial network... Fluctuating colors. Clearning up the dataset or
        increasing the weight didn't fix it.
        My last resorts are: increase the dataset size (which also means even longer training time), and decreasing the
        learning rate only for the adversarial network. I'm trying the latter...
    2. Neural style
        More refactoring.

    3. CNN relation extraction
        I'm done with modifying the cnn repo for my project's purpose. All I need is more data and adding active
        learning.

2017/01/15
    1. Nijigen
        After training for such a long time (110000 batches), it still looked the same as if I've trained it on a
        smaller dataset at a much smaller size for the same number of batches.

        Although I do have the larger dataset, I don't want to work on this further until i find out why... It costs
        money to train this thing overnight.
        Cleaning up training data will take a while so I'm letting that run. Maybe I should also try to save the
        preprocessed and resized images as .npy files and save them in the memory ready to be used. Don't know how much
        speed up this will provide. It should be considerable enough though...


        I tested the resize function. It was pretty slow (3s on my laptop per image). Going to try to add preprocessed
        data with test_multi_style.py. If that speed things up significantly, I will switch to using preprocessed ver.
        Before switch, each batch with batch size 1, hw = 512, and 9 style images takes 8 seconds.

        After modification: each batch with batch size 32, hw = 128, and 12 style images take 23 seconds.
        each batch with batch size = 4 hw=512, and 12 style images takes 50 sec... Not so fast I guess..


     2. Neural style
        Refactoring... maybe not today
        Maybe I should try this: pretrain the model on say 8 style images, then use this as a base to train on
        other style images only on the scale and offset variables.
     3. CNN relation extraction.
        I will keep the notes of this in the NewMaster project.


2017/01/16
    1. Nijigen
        Plan: study the other coloring methods that uses cnn. The one that seems most plausible currently is the
        "Colorful Image Colorization" paper

    2. Neural style
        Note: I'll not be able to use the pretrained network in the example because I changed things around afterwards.

2017/01/17
    1. Nijigen
        I will implement an algorithm similar to the paper "Colorful Image Colorization". One difference between my
        algorithm and his is the choice of color space. Since bw images contains information on the color for each
        pixel while the sketch does not, I can't use LAB color space. I have to use the rgb color space, which in
        principle should work the same but may make the task harder for the network since it is now required to
        predict not 313 classes of color but around 4096 classes. I will start from lower number of colors for now.
        (I will use the 216 color pallate.)

        The reason I was using that was I could not get a satisfying result from the adversarial network. The
        training was slow and I believe that further training will not improve things after training it for a night.
        Also the paper I mentioned above stated the problem I was facing: images generated had very few colors and
        had a sepia tone. It mentioned the reason that caused it: using the l2 loss will result in an optimal
        solution of coloring everything gray if there are ambiguities. It also mentioned the solution, which is to
        modify the loss as the cross entropy between two probability functions.

        The problem I'm currently facing was that there were some parts missing in the model description. He went from
        lower resolution directly to higher one. He said he used up-sampling but mine was giving a checkerboard like
        result. I guessed that it was caused by the generator network and I deleted the softmax layer in the
        generator network because the loss does that already. It seems to be working fine now.


2017/01/18
    1. Nijigen
        For debugging, I'm trying to directly optimize the output image and that helped me to do some sanity checks
        and discover more bugs. ( I forgot to normalize when I convert the rgb grid to image)
        One thing I've found was that I cannot generate a satisfying image using back propagation. It gave me a gray
        image. Only after I manually added a softmax layer to the optimized output image does the training give the
        correct result. The reason was that if I take off the softmax layer, then the output can contain negative
        values, which will create problems during normalization. The two models are essentially the same otherwise.
        So I guess I shouldn't have deleted the softmax layer in the generator network...

        After I made the changes, it turns out that the training last night actually worked! I'm now getting a black
        and white image out of the sketch I gave to the network. To give it color, I need annealed-mean.

        Implemented annealed-mean. Finally saw some color. I think the method is working. So I'm switching to more
        careful training with smaller lr. I also modified the network a little to fit this use case.
        I'm planning to let it train for a whole day while I do other stuff.
        Plan: I need to implement the per-pixel rare color balancing.

2017/01/19
    1. Nijigen
        The generated image was blurry. Trying to increase the resolution...First attempt failed but I think the
        reason was because the number of features in the final layers are too few to encode all info needed for the
        216 color bins.
        No That's not the cause. I increased the embedding to 256 and it still has the checkerboard effect. Maybe
        it's the convolution, but there's no reason why it's showing up now instead of before. I haven't seen such a
        checkerboard effect before and the network architecture hasn't changed that much. Maybe it's the dilation
        layers. I will test it locally. Maybe the generator network is wrong somewhere.

        Other reasons why this may fail:
            It may not be able to recognize the color boundary so the result may look inconsistent.
            Training data could be too complicated and not well selected.
            Training takes too long
                How do I know what lr to use? -- Use back propagation to test what lr can give satisfactory result
                for one single image, then maybe 1/10~1/2 times that would be fine depending on the batch size


    2. Neural style
        Continue refactoring. Finish up stuff.

2017/01/20
    1 Nijigen
        The network result was not so satisfactory. It's even less satisfactory than the unet. Probably that's
        because it did not have direct connection between the layers before and after conv/deconv process. With that
        connection, one can easily learn the identity function get at least the sketches part correct. Now it's
        mostly giving black, white, and gray colors... It has the capacity of giving more colors than that, but
        training is just not going in the direction where I want it to go.

        The network stopped improving after batch 9000. I will test the new unet_mod network. The only modification I
        made to unet is to make its output into 216 bins instead of rgb.

        That should give better results than that of unet. If not, then there's something wierd going on.
        If the result is better but still not satisfactory after the change, it means that the network cannot infer
        from the given information about the color to use. Then I can modify the network (add the dilation layer for
        example) or clean up training data to improve the result. I'm hopeful about this.

        Some other improvements I've made: I added the ability to read from preprocessed npy file into the color
        sketches program. I found that reading the images, resizing them, and generating their sketches are taking
        quite a while. By doing this I'm hoping to have a 25%~50% speed up on training speed.;

2017/01/21
    1. Nijigen
        Summary of what failed and what sort of succeeded:
            a. Failed:
                color sketches modified by adding some more layers to enlarge the result image instead of nn upsample.
                unet modified so that the last layer become rgb bin without changing anything else
                    The output became just white after 2000ish rounds.
                unet modified, just like above, but with the deconv layer being deeper and lower lr. (Got the same
                result as unet mod above)
                unet + adversarial
                Only adversarial
                unet first then only adversarial.
            b. Sort of succeeded:
                unet only
                    The result always had sepia color or only black and white color. This is caused by the inherent
                    uncertainty in the color to use for objects and the optimal choice for l2 loss was the mean.
                color sketches


        Plan:
            Rerunning color sketches on unet with the larger dataset to make sure it was not the larger dataset's
                problem.
                I tried. That had no problem at all. But why did it failed on training the rgb bin? Why did the
                result became just pure white image?

                TODO: think about why...
                    There are several reasons I can think of:
                    First it might be the problem of the training data. I sorted the order of training set and
                    it might happen that all the manga images got clustered together. This is unlikely to be the
                    cause though in my opinion
                    Second, it might be caused by normalization before applying softmax. Again I don't see why
                    mathematically it make sense that way, but it's just a possibility
                    Third, it might be caused by the task being too hard and the most common color in the training
                    images happens to be white...

            Testing color_sketches_net with last layer replaced by rgb layer. This is to test whether I really need
            to add the direct connection between conv-deconv layers

            After that, run the color_sketches_net with directly connected conv-deconv layers.


            colorful_img_network_mod => modified last layer into rgb.
            colorful_img_network_connected => Connected the conv and deconv layers. Substituted the deconv with nn
            resize plus conv layer. Also modified last layer into rgb.
            colorful_img_network_connected_rgbbin => Connected the conv and deconv layers. Substituted the deconv with
            nn resize plus conv layer. Last layer is kept to be rgbbin. NOTE: I've also taken out the norm in the
            last conv layer just before softmax. I thought the norm might be affecting the softmax layer (Don't see
            why it would mathematically since softmax is already a sort of normalization. But who knows)

            Sanity tested most of them.
            Now running colorful_img_connected
2017/01/22
    1. Nijigen
        I tried to train colorful_img_network_connected on l2 loss.
        Result was not satisfactory. It did not even learn where should be brighter and where should be darker. Unet
        gave a better result than this.
        I forgot to add the tanh after the final layer...

        I'm thinking what is causing the difference between my result and taizan's result... There's one thing I have
        yet to try. I did not try to increase the lr to like 0.01 or 0.1. I did not try to adjust the hyperparameters.
        Maybe I should do that... on the unet setting.


        I found a bug... in the conv_net padding thing. After I added the dilation, I checked whether dilation = 1.
        That is the bug. I did it the other way around. It should be "if dilation != 1" but  I wrote "if dilation ==1".
        I found the bug when I was trying to use the johnson network and it gave me an error saying the output size
        and the input size are different... gg. I should've rerun all the tests after I make the changes..

        That's why shit was not working.. Oh god. I should've written more assert clauses.
        But that has nothing to do with the unet though... wait no it does. It turned all strides into 1

        I fixed the bug and tried with colorful_img_connected_rgbbin. The result was way better than I thought it
        would (after seeing so many images with really terrible outcome, an image with "normal looking" black and
        white color was a huge releaf.

        But is it learning colors? I'll find out by changing t close to 0.Hopefully it is, but if it's not, then
        either the network cannot learn color, or it's simply not trained enough.
        Nope, the network is still stubbornly only learning gray images.

        In the colorful image colorization paper, they used batch size 40 and trained for at least 200k iterations.
        That would take forever... And cost a lot. Dunno how long theirs ran... A week?


    2. Neural style
        a. Don't forget to investigate the relationship between texture size and size of content/style images.
        b. The refactoring is largely done. I will now create another project solely for the neural style stuff.

        I was writing the readme file and here's what I've concluded:
        P.S. During this project, I found that sometimes it is hard to know what "looks correct" and what does not.
        It is hard to debug any neural network so it's good to know what you should be expecting beforehand.
        Lots of times I found myself stuck in a situation when I was not sure if the code had a bug, or if the
        learning rate wasn't low enough, or it's just I did not train the model long enough, or I was just using the
        wrong model for the task.
        There are thousands of ways things can go wrong and I don't have a solution to these problems.
        But during those times, I found that looking at what programs can achieve nowadays can keep me motivated and
        push me a little bit further.

2017/01/22
    1. Nijigen
        I just realized... I was training using input mode = bw. That's why the output bw image looked so perfect and
         it did not come from averaging the color bins. That's why adjusting t did not help.
         One lesson learned: connecting conv-deconv made the network lazy and it just spent a night learning identity
          function...
         I was thinking if at least I can color the bw images, it would be a start. Then I forgot about it...
         ..... I thought the network was working and it was smart enough to figure out the black-white coloring of
         the image just within a few thousand iterations... Ah..

         One thing I noticed in the conv_util: I for some reason did not add bias for all conv layers. This is wierd.
         It was probably because one of the repo I checked did not have bias. But it should... I didn't see papers
         saying that it doesn't need bias layers. Could this be the reason??? But it didn't cause any problem before...
        Plan: investigate the bias... I will try to run the color sketches net first without bias then with bias.
        Plan: not a huge deal, but shuffing the training set might be better... But it would require messing with the
        preprocessed npy files...

        Plan: maybe check the initialization method. the nn or whatever...


        The bias did not seem to provide benefits during the first few thousand iterations.


2017/01/23
    1. Nijigen
        Training is slow... I tried to run colorful_img_bias, which is just the original network with bias variables.
        As mentioned above, at first it did not show advantage over the one without bias. After more than 12 hours of
        training, it is still pretty slow in learning at least the identity function. Maybe my lr was too low, but
        that's already higher than what the original paper suggested.

        Should I keep running that? Yes. I want to see how the network works if it works at all. I can work on the
        master project for now.

        Modified unet_mod to include the bias. A small difference between unet_mod and colorful_img_network_bias is
        I added bias in the layer right before softmax in unet_mod but not in colorful_img_network_bias. I first
        thought that adding the bias does not make sense in the last layer before softmax because softmax will just
        sort of normalize the bias added. Now I think that maybe by adding a bias layer it can make the job of previous
        layers easier since it now does not have to produce a negative result for units not activated. It can just
        set the bias to negative.

        Anyway... It will probably be a while before I can test the unet_mod... Hope it will run faster than the
        colorful_img.

2017/01/24
    1. Nijigen
        Training takes too long and I'm impatient... It didn't learn the identity function yet.
        I'm planning to switch back to unet_mod and hope that can improve things a little.

        Caught a bug just in time... I was running unet_mod and it was wierdly slow. Then I found that the upsampling
        layers was modified to all 256... Changed them back to the same as down sampling ones.

        I tried to set the lr to 0.002 and the result became completely white after the second checkpoint. Maybe high
        is the cause to having completely white outputs... But I didn't expect it to saturate so quickly.

        I also realized that I was doing the wrong thing. I was added a softmax layer while if I use the
        tf.nn.softmax_cross_entropy_with_logits loss I don't need that layer since it does it for me. Applying an
        additional softmax layer will make matters worse. gg... so many bugs
            Another reason I did softmax was because this line:
            "rgb_bin_normalized = rgb_bin/np.sum(rgb_bin,axis=3,keepdims=True)"
            Without softmax, the sum can be negative and the probability wouldn't have been correct. But I should've
            just apply the softmax there instead of messing around with the output...


        Plan: I need to implement the color rebalancing since it help learning rare colors...

2017/01/25
    1. Nijigen
        The task i ran last night was not so successful because my learning rate was too low. I trained for 50000
        iterations but it still did not learn how to color bright and dark areas, which is consistent with the
        previous result at 10000 iterations with lr at least 5 times larger (I say at least because I also apply
        lr decay). I am re-running it with much higher lr (lr = 0.005 batch_size = 4)

        I realized the huge difference between the colorization task from bw and from sketches. bw image have
        illumination info for each pixel and sketches do not. Therefore the network for bw images only need to learn a
        very rough coloring for each region and it can then use the illumination to fill in the details.
        That's why my network seemed so slow when I use bw images as input. I told it to output rgb values instead
        of lab or whatever other color spaces they're using. I don't have that kind of luxury in the sketches task.

        The rgb_bin method had a hard time learning the contour or the identity function because each bin is
        independent. So they each have to learn it once. Also, it might not even be a good idea to draw the contour
        for some bins since those contours may not use the color they're representing and they should output 0 instead.

        ....

        Wait a sec. I can train two parts: one outputs the illumination of the network -- which I know already how to
        do... it's just the output that the failed l2 loss would give. I can make that task even easier by reducing the
        number of output layers from 3 to 1. -- and the other outputs the ab values at a smaller resolution which will
        then be upsampled by nearest neighbor. Now each network can do the job that it specializes in. Hah
        I was thinking that the two can share the network upto say the last few upsampling layers, but that I can
        experiment with. Worst case is I just have to use two networks and longer training time. I think this will work
        in theory... But let me first finish the color rebalancing and maybe the K-means initialization in the paper
        "Data-dependent Initializations of Convolutional Neural Networks" (https://arxiv.org/abs/1511.06856)
        K-means initialization is optional so do it only if I have time and waiting for it to train or something.
        (https://github.com/philkr/magic_init)

        I don't think there is a nice way to modify the gradient calculated in tensorflow, so I probably just have to
        change the loss and multiply each category by its weight...
        I finished implementing the color rebalancing. It's a little easier than I thought.


        The training of unet_both went better than I thought. Althought it was slow (since I'm using two neural nets
        instead of one), the result of the first 1500 iterations was better than I thought it would be, largely due to
        the bw network. The unet predicting the bw image from sketch input was doing a great job (as always... but it
        fails to generate color and that's why I'm using the colorful_img network in the first place.). I guess I am
        over-optimistic but I'm just happy to see colors other than bw and sepia... lol.

        TODO: after this project, I want to summarize the signs of all kinds of bugs/errors in cnn, like what are the
        signs of a network unable to digest the training data, what are the signs of learning rate being too large etc.

2017/01/26
    1. Nijigen
        The unet_both ran for 12 hours and it only got through 4500 iterations with batch size = 4. The result was... Uh
        Ok. The l2 loss usually takes around 30000 iterations at that learning rate (0.0002) to learn the bw colors.
        I don't know how long it takes to learn colors for the colorful_img network, but I did a sanity check and found
        that it can never learn how to color the image perfectly (due to producing a tiny image and resizing that to
        a larger size).

        Because the training was so slow with unet_both, I decided to separate the two networks and train each of them
        individually. I can probably combine the two together later into one single pipeline. Now I guess it just takes
        time... and money.

        ...I let the network trained for half a day and it can't learn shit.. The loss was just flat. I think it's the
        learning rate I'm using and the batch size so I'll increase the batch size and decrease the learning rate to
        that suggested by the colorful image colorization paper. I will also modify the preprocessing to include
        lab info as well, although it might take up too much space. The wikipedia said to use 16 bit and floating point.
        The preprocessing should also shuffle the images.

        Some small differences I noticed after reading the paper again:
            1. THe network they've drawn was correct but their table was a little bit off from their git code
            2. they did not have batch norm after relu8_3, which I did. I took it off
            3.


        The training now has speed of 500 iterations with batch size 12. After adjusting the learning speed, it still
        appears that the network saturates at a super fast speed and stayed at a loss of around 3-5. Maybe it's the
        color rebalancing that's causing such an unstable learning? Let me try without color rebalancing

        Could it really be the problem of the training data??? I don't know why things are not working. Maybe I should
        use the github repo and test that.

2017/01/27
    Spent the whole morning trying to install caffe... Finally done, and it does seem that at least the memory use is
    less than tensorflow. It can easily support batch size = 40 and a reasonable training speed of 4s per batch. In
    comparison tensorflow quickly went out of memory after batch size = 20 and I was forced to use 12.

    The colorful_img was training fine. The loss was still steadily decreasing after 17 hours of training (8500
    iterations) The loss got from 6 to around 5. If I just keep training for 200k iterations I can probably get to loss
    around 2.5, which will give me a good enough output. It will take more than 10 days of training though...

2017/01/28
    After 11000 iterations and a day of training, the result wasn't so impressive yet. I guess that was why it would
    take them 200k~300k iterations to get some publishable result... I don't know what I should do now. It's simply
    a little bit too costly to keep machines running for 10 days costing around 200 dollars to get some result in caffe
    that I'm not even sure if I can use or not.

    ... Maybe taizan's method works afterall. I don't know how long he trained. I only know he said several epochs.
    Training cnn is a huge problem especially when it takes several epochs over dataset with size on the order of
    million. It's amazing that people find working cnns at all.

    How long did it take them to train vgg? Yep some reported to take 10 days to train
    (# http://libccv.org/doc/doc-convnet/)... gg.
    what can I do... work on the master project???
2017/01/30
    I found that taizan the author released his source code.
    The chainer setup seems to be pretty hard to understand for a beginner, but as of now, here are some differences I
    found between my setup and his:
        1. He adds noises to training images and sometimes flip images.
        2. He converts images to YUV color space
        3. He has one extra network: lnet, which takes an image with three channel and output an image with one channel.
        I don't know what that network does. It's not trained, but rather loaded from a file.
        4. The lnet outputs the sketch of the image. He had a loss where he compares the sketch of the generated image
        versus that of the original one.
        5.

    Things I need to do to run his training program:
        1. install chainer (Done)
        2. Convert the file to python 2 compatible ones. (Done)
        3. Produce sketches for the training set. (On-going)
        4. Write all file names to one single file. (Done)
        5. figure out what is lnet. (Done)
        6. Maybe I should use his network to generate the sketches instead of my method... Mine is pretty slow and his
        uses gpu.
        7. Figure out what's the difference between train_128 and train_x2. The loss and the goal all seems to be
        different.


     *While I'm trying to convert images to sketches, I found one problem: image_to_sketch only works when dtype is not
     uint8. It has to be like np.float32. So the preprocessed images looks different. They're not really sketches...
     That's one of the reason why my algorithm never works! ...

     How can I prevent bugs like this in the future?

     Regarding unicode encode/decode, here's a nice summary: http://qiita.com/yubessy/items/9e13af05a295bbb59c25

     It seems like the lnet is a cnn that extracts the sketch out of the grayscale image. But I thought the sketch was
     generated from opencv code. I guess he thought using opencv might be slow for training so he decided to use a
     cnn? That's the only explainataion.


    #Plan: I'm not done with preprocessing using the cnn yet. maybe using the 128x128 image after resize to generate
    sketch is a better idea than generating it first then resize in the cnn case.
    ==> It turns out that actually resizing all of them to the same height/width at 512x512 before turning them to
    sketch might be better since otherwise I would have to resize the image to be divisible by 16 or the unet will
    complain about the size of layers not matching for concatenation.

2017/01/31
    I finished running the old preprocessor using my imtosketch function. I then ran the command below:
    python train_128.py --dataset="/home/ubuntu/pixiv_downloaded_chainer_128/" --image_names_list="/home/ubuntu/pixiv_downloaded_chainer_128/image_files_relative_paths.txt" --gpu=0

    I tried the preprocessing through the provided cnn in chainer. It was slow... It takes around 260000 s, which is
    more than 4 days, to convert all training images to 512x512. Using the opencv method takes 1/3 of that time.

    I was refactoring my tensorflow code so that it corresponds to the chainer version. I implemented the weight decay,
    (which is not what I thought it was), modified the network so that it now generates images in yuv color space
    instead. I didn't think color space was important but in fact it is. I guess YUV makes it easier for the network to
    separate lumination and color. One thing I did not implement yet was the sketch loss. I found that in order for
    back propagation to work properly, I must either find a way to generate sketch from colored image in tensorflow
    using the operators it has, or I need to train a cnn, like what taizan did.

    # Plan: One more thing. The sketch I generated was the opposite color of the sketch generated by taizan's cnn. I'll
    # fix that... But wait a sec. Does that mean I need to rerun the script again?
    # Ah I'll have to write a script to reverse the color. or is there command line tool to do this.
    # But I'm already training the chainer ....Ah... I probably should stop it soon.
    Stopped the chainer. planning to restart once I've inverted all images... which will take hours again.


    I will run my sketch generator cnn along with the chainer since chainer only takes around 6 gigs of memory. Or is
    there a easy way to convert chainer weights to tensorflow? They're essentially using the same parameters.

    It seems that I can. Here's how:
        Load the npz file
        The parameters in batch norm are:
            gamma => scale in tensorflow
            beta => offset in tensorflow
            avg_mean => not used in tensorflow. It is obtained real time instead. Try if I can get away with using it.
            avg_var => not used in tensorflow. It is obtained real time instead
            N => not used in tensorflow.
        The parameters in convolution and deconv are:
            W => weight
            b => bias
            Note that in chainer the input is always 4d.

    I tried to convert all variables to tensorflow ones. It didn't work so well for some reason that I have yet to
    figure out. I first ignored the avgmean and avgvar, but the result was not what I wanted, so I tried adding them
    and now it seems to be even worse, so I'm suspecting that I should not have the two variables, and the network
    is different from the chainer one. I will work on it tomorrow I guess, after I'm done with other stuff.

2017/02/03
    Training does not work. The chainer trained for 2 epochs already and it's still spitting out images that look just
    like the sketches one fed into it. There are some colors but not more than 5%...Maybe it's my training set's
    problem after all.

    Trying to load the pretrained chainer model into tensorflow again.

    Shit I'm really screwing up preprocesssing... The color folder should contain COLORED images, not black and white
    ones. I wouldn't have noticed if I have not checked my local output by accident... I should really really check the
    preprocessing output...

    Ok the import into tensorflow was successful. Now I need to import the rest.

    Importing the unet had a little bit more trouble than I thought. It had the same structure as lnet, so that was not
    a problem. The output was messy first and I end up realizing that using the CNN sketch generator instead of the
    old opencv sketch generator was crucial to reproducing at least some acceptable result. It was still not exactly
    the same as the original one though.
    I am suspecting that it was because the tensorflow im2sketch output is not exactly the same as the chainer one.
    Let me try to manipulate the sketch directly and feed in the sketch generated by the chainer code.

    *I found that with the same image of different size, the chainer code outputs different result?? I have no idea why
    though. Every image is resized to 128 then to 512 anyway... Probably the resize of opencv preseved more info than
    that of ubuntu convert? Yep it turns out that it does make some difference for some reason.
    *Another thing I found that was crucial: the height/width fed into the network must be multiples of 16... Otherwise
    the auto resizing takes over and mess with the output. In chainer they did not have that issue because they did
    not implement nn-resizing when the height and width is not divisible by 16. They resize the image instead.
    Interesting
    * Another really wierd observation was that the input to cnn128 was different from that of cnn512. That itself was
    not strange. I know he trains 512 and 128 differently. But the hint he feed into cnn128 whenever he calls
    get_example is just 3 copies of the sketch itself rather than real hints... Why would he do that? And that is only
    done during generation, not during training. During training the hint is set to -512,128,128. I observed that his
    code was pretty inconsistent on this so I probably should just leave the hint as it is like -512 128 128.
    * The input of the 512 network was the output of the 128 network... What?


    Seems this thing is way more complicated than I thought. Too many unexplained parts...

    The network transferred from chainer to tensorflow is sort of working. it is producing some artifacts though.

    Plan:
    1. Get the input size right
    2. Get rid of the artifacts (maybe it's the deconvolution. It's something that has nothing to do with the variables)
    3. Make sure I'm not missing anything from the chainer version. We're using the same weights so the outcome should
    be largely the same, if not perfectly the same.

2017/02/04
    Chainer ready and training.. Even slower than last time for some reason. This time it says it will take 30 days to
    train 20 epochs.

    Still trying to get PaintTensor to work. Now I'm stuck on not being able to overfit a single image on unet_color.
    Using elimination to get rid of possible sources of bugs one by one. I now know it's not hint/adv/probably not
    weight decay. Hum it doesn't seem to be related to anything but the unet_color itself. I double checked sanity
    check using 'backprop' to make sure it has the correct inputs and outputs.
2017/02/5
    sanity check with low lr, weight decay and sketch loss failed. (No hint or adv net was used). The result did not
    look anything like the original image. testing without weight decay and sketch loss gave better results, but still
    not nearly indentical to the input image. The color is off.... and the loss does not seem to correspond with the
    loss I eyeballed from the last five pixels.

    I again tried to make the pretrained chainer model work in tensorflow... I double checked that the lnet works.
    But unet still does not work well. The color is just off... It's almost there though so perhaps I can explain this
    away with difference in ways of calculation between model...

    * I studied the structure of cnn512. It turns out that he used a very smart trick to train the hint network using
    the output of cnn128. He expanded the output to 512 and then fed that as hint to the 512 image. The 512 image still
    receives the normal sketch input as its first layer. The loss is then calculated with only l2 loss. So the second
    part can use the information from the 128 part and improvise on its own a little while having the guide from 128.
    Incidentally(maybe?) this corresponds well with the "colorful image colorization" idea that we only need some
    hints of what color to use from smaller image sizes.

    In the mean time, training with chainer is not going well... The output only faintly resembles the sketch, but no
    color and not even shades.I'm suspecting it's the dataset I was using so I'm trying to switch back to pixiv_full..

    Plan for tomorrow:
    Plan: read pix2pix paper. Try finding a tensorflow model. This is the most important task in the long run.
    Plan: check result of the sanity check
    Plan: check the result of chainer code
    Plan: I'm currently stuck on sanity check and my model can't even overfit a single image.
    Plan: if possible, switch to spot instance to save money

2017/02/06
    Sanity check last night with low lr and no weight decay or sketch reconstruction loss still did not get
    the output I'm expecting. Why? I think it used to work fine before I transferred the project to the new repo.

    THe chainer code is working a little i think. Training 30000 iterations made a big difference compared to 20000
    iterations. It now can at least generate some colors as opposed to no color at all.

    I also converted pixiv_full training dataset to 128x128 with sketches last night. It finished this morning with
    some warnings, but I ran a short piece of code to check if they can be read and have the right size. They all do,
    so it should be fine.

    Regarding the sanity check... I found the reason. The reason is I had the wrong input... I had a color-reversed
    sketch for training input... gg. After I fixed this, it worked fine. Now i'm going to add on weight decay and
    sketch regen loss. (PS. the lr i used was 0.01 and it got to a pretty good result after 100 iter)

    The result looked bad after adding the sketch reconstruction loss and the weight decay. I think the weight decay is
    causing the problem... Nope after I set the weight decay param to 0 the output is still a mess. So it's the sketch
    reconstruction loss. I double checked by setting the sketch recon param to 0 and the output looked normal again,
    although training too longer than the previous run (which is understandable). Still trying to figure out why
    adding the sketch reconstruction destroys the output...

    I found out why. It was because I did not set the variables in lnet as untrainable. After I made that change,
    everything became fine. Sanity check on everything after that passed. The only thing is it produces dead pixels
    from time to time.

    Now trying to run the whole 128x128 setup on real data. Hope it will at least produce something to debug if not
    crash...

    Plan: 1. Code the 512x512 version in the PaintTensorflow project
    I'm almost done with it. Double checking the chainer code. Some differences:
        Resize images in cv2 versus in tensorflow. I used nn resize. In taizan's code he used cv2.INTER_AREA
        The noise he added for 128 versus 512 is a little bit different. I haven't changed mine because I don't know
            why he used a different noise for them.
        The original code added a non-deterministic probability for each channel to use the color from the 128x128
        original color image (not generated). That probability is 0.2. The rest of the time it uses the color
        channel from generated 128x128 image as hint.

2017/02/07
    The result of PaintsTensor was mediocre. It learned the sketches fine but it did not learn any good looking colors
    yet. The l2 loss plateaued at 5000 iterations at around 20. The generator adv loss was still too high compared to
    that in chainer(3 times higher).
    The chainer result wasn't looking too good either. It color one to two specific images pretty well but does poorly
    on other ones. It does especially poor on images that was not generated by the sketch generator lnet. It also
    does poorly on images that is generated by lnet but of a different size.

    ... I'm stoppin the instance to switch to spot instance... Spent so much last month.
    Also looking into the pix2pix tensorflow code. It looks pretty reliable. At least It seems it can repeat the
    results from pix2pix paper. I modified it a little so now it can run on my laptop. I can use ec2 to train it
    on gpu.


2017/02/08
    Future direction: I decided to push off coloring sketches for now. It is not worth trying to figure out what the
    author was doing while he is still finishing up pushing his code onto github.

    I read through the note from two months ago. I was hoping that a program can do some creative work like draw some
    paintings with some guidance from human. That was why I am so interested in coloring sketches because it aligns
    with that goal. After seeing people's reply on zhihu, I realized how big of a gap there is between AI and what
    people expect from an "intelligent being". For example the lighting, the shades, the positioning all needs work.

    I need to focus on a problem instead of some fantacy. What problem do I want to solve the most? Well I can't draw,
    and perhaps if there is some simple tool that let me draw easily, it would be great. Now let's break it down.
    I can't draw mainly because I don't know how to turn abstract concepts into shapes.

    Plan:
    I'm interested in GAN so I'm going to look into Wassertein GAN... I think it's along the line of understanding a
    2d-image, and reconstruct an image using abstract concepts. I can then probably add the GAN idea where it
    make the parameters semantically meaningful.

    Other people's blog on WGAN is super helpful. Here are some of them;
    http://musyoku.github.io/2017/02/06/Wasserstein-GAN/


    Plan: modify the pix2pix a little to fit the purpose of coloring sketches.
    The pix2pix code was way more well written than my code. It even has an efficient way to manipulate images using
    gpu instead of cpu. It works great. I should really learn from this repo.

    Plan: explore new projects. Think of one interesting thing each day.
    I thought of auto-color suggestion for web pages or for anyone without UI design background. Input is the sketch
    and the output is colored webpage. I always have difficulty designing a good-looking webpage so I can
    use this tool to help other programmers like me.

2017/02/09
    The pix2pix is working better than the chainer version and it almost works outside of the box. Lol.
    I'll let it run for another say 12 hours and see what happens. In the mean time, I will switch to spot instance
    with freshly installed tensorflow. (It's not urgent and it looks like it will take a while to figure out again.)

    I should focus on mainly WGAN for now. I can use it on pix2pix.

    pix2pix trained for 36 hours now. the l1 loss kept going down while the gan loss only goes up.

    I'll keep it running for another night...
    Plan: I want to add the colorful_image_colorization technique (lab bin) into pix2pix. But that might not be
    necessary... Really think about it before I actually do it.
    The reason I wanted to add the lab_bin technique is I noticed that from the pix2pix trained results, there
    are some images where the network is capable of capturing the skin color fully while it fails on other ones. There
    are also times when it creates a sepia colored image, indicating that l1 loss might not be the best approach.

    Now since the GAN generator refuse to train and have a low loss, I think wgan may save the day and make it a little
    bit easier for the generator.

2017/02/10
    Spot instance is working. Spent such a long time figuring out utf8 problem again and then realized that I already
    fixed it but the previously generated file names are corrupted and that's why ls is still giving me bad-looking
    file names but in fact it is working properly...

    What should I do? I should implement WGAN and merge that into pix2pix framework, but that will take a lot of
    refactoring and I saw online people doing sanity check to make sure their wgan is working as it's supposed to. I
    should definitely do that, but again, so much refactoring on other's code.

    Ah fuck I give up. I was refactoring pix2pix for wgan and it was way more complicated than I expected. First of all
    pix2pix make predictions on individual areas and sort of sum it up at the end instead make one single decision
    using a fully connected layer. Second of all, the input for sanity check is numbers but the input for pix2pix is
    images. Plus the code uses way more functionality in tensorflow than what I am familar with. So I guess the best
    "sanity check" for now is not to repeat what other's have done (the one in chainer code), but to use the existing
    framework and directly modify the loss function. I am thinking that I probably can do something similar
    on individual areas (instead of using fully connected, which will require some changes to the discriminator
    structure). My goal should be not touching anything except the loss and the variable magnitude cap. ...

2017/02/11
    Found a tensorflow repo for WGAN. Adopted things from it and changed the pix2pix a little. Testing wgan on pix2pix.
    I was thinking how to make the output be less close to black and white and more colorful. Learning with l1 or l2
    loss will inevitably fall into black and white because the statistical average is gray. The GAN network forces
    the generator to be more creative but does not allow control over what kind of coloring theme is used. The result
    is that the network learned one uniform theme but the user have no control over that (because unet does not have
    a noise input). The "hint" input solves the problem by providing user an active way to control the input, so
    the output can be pretty diversed based on the user input. But the overall look or style did not change.
    Anyway I don't think adding lab bin is necessary since I already have another way to have more variety as well as
    giving more control to the users.

    A side note: I'm really interested in whether a trained colorization network can be used to change the style of a
    paint. Maybe it's not the colorization network because the input to colorization network is not the colored image.
    So probably a labeling network... For each image, I have its tag info. Maybe I can write a classifier for the tags
    used and use that to change the style of images... Or I can use the discriminator. Yeah using the discriminator is
    easier and may be equally effective.

    So I did. I modified the stylize in neural style and added the discriminator network into that. The only thing is
    that the discriminator actually takes the input to generator network as input as well. So I have to retrain a
    generator/discriminator network by taking out that part.

    Now running a network that has both changes: discriminator takes one less input and gan changed to wgan

    The result so far is disappointing to say the least. The output is way less colorful than the original pix2pix,
    probably because of the change in GAN and also because the discriminator is given a harder task.

    Plan: learn the file reading techniques used in pix2pix
    Plan: continue training the multistyle feed forward network... It's a little bit too slow though.

2017/02/12
    After switching to wgan and modified discriminator, the output became black and white again... I'm guessing it's
    wgan that's causing the problem. Either I should not use l1 loss at all, or the wgan is not helping to generate
    color as it should because I took out one extra channel of information

    Trying to train only the wgan.
    # Plan: read more papers and blogs today.

    wgan only failed with lr = 0.0002 and batch size = 1. It succeeded at first but then the discriminator failed to
    train further and starts to oscillate while the generator kept getting better at beating the discriminator but not
    actually improve the quality of generated image. So if the discriminator keep getting better, it should be fine.

    Too bored so I started implementing a classifier for food images. It's also a practice to understand the
    functions used in pix2pix.

2017/02/13
    wgan only was not working well because the discriminator was not strong enough. I modified the discriminator as
    well as the generator so that now they have the same structure as the chainer one...
    #Plan: I should do sanity check before I run it on larger datasets.
    I spent a lot of time on doin sanity check on the "deep food style"'s classifier part. I later found out that it
    was the maxpool or the vgg itself that was causing the problem, which I did not expect... I then just decided to
    change the structure to home-built functions and got rid of the maxpool, which made huge improvements on the
    sanity check. I'm running it on the full dataset now.

    Good news is I now learned a little bit about how to use a tensorflow file reader, which maybe faster than just
    reading using other modules I guess. I have yet to figure out what the supervisor does.
    #Plan: continue training my pix2pix, but first do a sanity check.
    It's doing poorly on sanity check on 500 images for now.

    For the multi style, I thought there was the old bug again where there were white spots on the output image for
    all styles, but looking at my own repo, I didn't see that problem... So I must have solved it or it is something
    that can be solved by more training... Yeah it wasn't a bug. That's just caused by insufficient training.
    #Plan: continue training on deep food network.

    # Food style is classifying everything as the same category... Doing a large scale sanity check on three types of
    food
    I tried generating styled images using the briefly trained vgg network. It didn't work. There is no style,
    and the result is just a darker looking original image. let me try without content image. Just style loss gave
    noise-like images. I think it is because the network is overfitting on the tiny training set.

2017/02/14
    Food classifier failed. The loss never decreases.
    pix2pix failed to some extent. It never learned how to apply color over 180 iterations over the test set. That is
    actually consistent with what I've seen before with unet.

    Falling back to safety zone
    For pix2pix I am running the original version on test data for sanity check. I want to see if the original version
    is able to produce color, reasonable color. It did have color, but the color does not look reasonable yet. More
    training and hopefully it can get better. I tested with training on one image
    and it didn't have any issue with that. I tried that network on other images and the color it produced looked
    overfitting towards the training images, as expected. The sanity check training is still going on (with size 500).
    Maybe I should increase the lr since the learning curve looks too smooth.
    Increased the lr and it still looks fine. Maybe I should try running it on the larger dataset. The curious thing
    is why it works. I mean it is essentially the same as other modifications I've made...
    Yep it works. It's already getting somewhere after around 15000 iterations. The next thing I should do is to
    implement the optional hint. #Plan: implement hint.



    For food, I am retraining the vgg19 on pretrained parameters (also sanity check). My version of vgg19 did not work,
    but I know the original version must work, unless it had some bugs in the training code. So far it's doing just
    like my version of vgg, which make me wonder if the training code had bugs. Using the pretrained vgg did not help.
    I tried fixing all other layers except the last three and it worked... So the problem was training too many layers
    at the same time? Or was it because of the max pool?
    Now running the network on bigger dataset, still only training the fc layers.
    #Plan: If that succeeded, I will try to  gradually make all layers trainable.
    Yes it is working. The loss is slowly decreasing. Now I'm trying to manually change the trainable layers and
    resume training. Resume training failed for some reason. It is complaining that it can't find the variables...
    It turned out that I was using an old checkpoint. It is working fine now. I've implemented making more layers
    trainable as the network trains more.
    Ok there was actually a bug in restoring. Because I was modifying which layers are trainable all the time, I cannot
    restore the Adam variables which stores the momentum and other stuff. If I start all over again each time, the loss
    get messed up, so I am forced to switch to good old gradient descent now.

    Plan: multistyle, check once per day
    Plan: pix2pix implement hint
    Plan: food classifier: check training result
    Plan: read more papers.
    Plan: investigate "unsupervised cross-domain image generation "

2017/02/15
    Multistyle is still running. It ran for around 2000 batches in one day at resolution 512x512

    pix2pix works, yet to implement hint. I finished that although it was just a rough version. The hint is always
    40 random pixels, which is not true in real world. I did not follow the hint procedure in chainer repo... yet.
    If it works, then I don't need to follow that. If it doesn't, then I can figure something else out.
    So far it's not passing the sanity check... It can't overfit one image..  I ran 20000 iterations and still the loss
    was stubbornly at 0.45. Wait the output seems correct though... It trained slowly (still looks blurry after 5000
    iterations, but the final output looked fine. Was it the loss function? Or that might be because although l1 failed,
    the GAN dominated and it produced good results... I am trying to take away gan training and see if l1 loss can be
    reduced... Turns out it did fine without gan. So it was gan that made l1 training hard?? Let me try not feeding in
    hint to gan, ONLY after the previous l1 experiment succeeded. Hum not feeding hint also failed. There is some
    serious problem here. # Plan: investigate sanity check using original pix2pix without gan (weight = 0)


    food classifier: working. I'll test the style transfer. Style transfer not going well. The retrained vgg just
    doesn't seem to work. The content loss looks unusually low (30 magnitudes apart from the usual level) and I've
    isolated other problems by not using the saver to load retrained model. The previous output might be caused by not
    loading the vgg? But it shouldn't because all weights should be recorded in the saver. Yep I confirmed that it is
    because I did not load the vgg. but why? Dunno...
    Plan: double check that there isn't any bug like I actually did not load the saved model or something, before I run the style transfer after training is done.
    Playing with that now. The outcome is sooo dark... It must be a bug somewhere. I thought it was * 2 - 1 but there's
    no such code in training. Still can't figure out why it's dark. It's not the way I save it, and it's not the tv
    loss. #Plan: change vgg19 so that it accepts different height and width instead of forcing them to be the same.

    Multistyle looked a lot lot better when the output shape is increased to 512x512. It is slow but it is definitely
    worth the time spent on training. #Plan: multistyle, check once per day. Maybe even try 1024x1024 if memory permits

    Plan: read more papers. reddit machine learning also seems to be a good source.
    I read a translated post on reddit about the advantages of chainer. The thing that shocked me the most was
    multi-gpu support. More GPU almost results in a linear increase in speed for chainer. The downside I guess is I
    don't need that many gpus yet now and it is not yet well developed as tensorflow.

    Plan: investigate "unsupervised cross-domain image generation "
    I did. It uses a module called Slim.

2017/02/16
    Multi style
    Plan: multistyle build a webpage...
    1024 x 1024 works, unbelievably... From the result I've seen on 512x512,
    2000 iteration is enough to produce good-looking enough images, so I'm training 1024x1024 CNN for 2000 iterations.
    It will probably take 2 days.

    Food classifier
    Added save mode to save all parameters into npy format. The output is still dark. It's not because of the trained
    layers, or not-trained layers, or even npy format. I tried loading from matlab format and it is still the same.
    It has nothing to do with subtracting the vgg19 mean. I tried adding the mean and it looked too bright.
    Putting this aside for now. This project is less important than the other ones.
    I made sure that it has nothing to do with the vgg file at all. Not even the network construct matters. It also
    has nothing to do with casting or lr or style weights or layers... Tested a bunch of stuff but still don't know why.

    pix2pix
    I was using 128 x 128 training data on 256 x 256 input! What was I thinking...

    Think of other parts of painting, can I generate a network that learns where the shaded parts are?
    The original pix2pix easily overfitted one image within 200 iterations, as I've tried before... I will compare the
    with hint architecture with the one without hint. Can't find any noticeable bug. I will try to add asserts to the expected number of dimensions.
    pix2pix_large sanity check is training slower than pix2pix, but it can overfit.
    I think I know what's going on. I isolated the problem down to the "tf.gather_nd" function. I am suspecting that
    when that function gets called, a new batch is drawn, and it does not corresponds with the old one. There are two
    ways to prove this. One is try not to call this fucntion and see if the image per sec doubles. The other way is to
    disable randomizing training input and it should be able to overfit.
    Yep my guess was right, although one of my way to prove it was not correct. The way to solve it was by adding the
    hint into the tensorflow batch. I didn't understand batch since I didn't use it that often.... Now I do I guess.
    Note: if the large network failed to color things and only produce bw images, it is because the GAN's discriminator
    is too good and (according to WGAN paper) that leads to GAN gradient close to 0.

    More papers:
    no progress

    unsupervised cross-domain image generation
    no progress

2017/02/17
    Multi style
    Plan: multistyle build a webpage...
    Retraining things... Haven't started programming the webpage. I should work on that.
    Done working on the webpage.

    Food classifier
    I isolated the problem to preprocess and unprocess (the step to subtract mean pixel from vgg input). If this step
    is done inside vgg, then the output will be dark. If it is done outside vgg, then the output is normal. Don't know
    why. I tried to fix it and now the color is wierd....
    I'll try again tomorrow. It's not worth too much time looking into this. Low priority

    pix2pix
    Made a mistake yesterday that resulted in hints not being updated... Fixed it and rerunning with hint version now.
    I was also running the larger version of unet and the result was just as I've seen before: purely black and white
    images. The loss from GAN indicates that the discriminator is too good at its job. So my solution is to switch to
    WGAN, which should solve the problem of hard GAN training. GAN is what made the images look colorful. We'll see how
    each goes. check the two url below:
    http://ec2-52-38-177-3.us-west-2.compute.amazonaws.com:8000/pixiv_full_128_w_hint_train/images/
    http://ec2-52-38-177-3.us-west-2.compute.amazonaws.com:8000/pixiv_full_128_large_wgan_train/
    PLAN: check result and make improvements

    More papers:
    Downloaded a paper comparing CNN and RNN in nlp but haven't read it.
    I read it briefly, it says rnn is better than cnn in almost all tasks, which make some sense I guess.

    unsupervised cross-domain image generation
    The repo by "yunjey" is awesome. Concise and to the point. The only downside is it only supports svhn to mnist
    dataset so the first step for me is to make it support other datasets.
    I found that in order to use other datasets, a different network structure and a different "content_extractor" is
    required. The content extractor should be specialized to extract features from the source and then can be trained
    to extract features from both source and target, so it is required that both source and target have the same output
    from the feature extractor and from the discriminator.
    So I need a feature extractor. I have vgg19... I can build a new one. Building something like a classifier for
    different fonts can be done but seems boring. (Fundamentally the feature extractor for digits or letters is just a
    classifier that works on both source and target and map them to the limited set of possible answers. It is shown
    in the original paper that that structure cannot do zero-shot learning. It cannot transform a 3 if it has never
    seen that category in the feature extractor. I want a feature extractor that works more like vgg19 and I don't
    want the output to just be the possible classes. Actually double checking the source code seemed to prove me wrong.
    The output of the feature extractor is indeed one layer before the fully connected layer.)
    I am thinking if i can use this on nlp, like translation between two similar languages. What should be the
    feature extractor? It should read the sentence(or word to make it simpler) and generate a representation of that.
    That representation can then be used to classify some properties of the sentence/word. I can't think of too many
    CNN NLP classifier like things, but there are feature extractors that turns words into word vectors. (it is a
    little different from the original task because the representation is actually a more semantic representation of
    the word, which just consists of letters.)
    Another direction can be turning human faces into animals.
    Downloading dataset... Hopefully it does not take over all available space

    I can also think of other project directions, like improving image to speech or something like that... The topic
    is probably too large for now, but the overarching idea is still: choose a good loss function using some general
    principles.
    # PLAN: work on human face to cat face

2017/02/17
    Food classifier
    I finally found why the color looks wierd. It was because I did processing on uint8. I subtracted the mean pixel,
    therefore making it overflow and going to 256-value... I shouldn't have messed with changing the dtype at all.
    Running a full test on how different vgg affect the output style. My guess is that it does not affect the output in
    a very fundamental way. There will be subtle changes but the overall look won't be too much different. Because as I
    think of it, just because I trained the vgg on food data doesn't mean that the style transfer output of a human
    face will look like a hamburger...

    pix2pix
    The large wgan model still only outputs bw images.
    The hint model is sort of working but still I can't see that it's using the hints.
    Trying pix2pix on lab color space.. Works. Trying to train on full dataset
    WGAN only on full dataset failed when I don't train the discriminator 5 times as much as the generator. That is
    probably because the discriminator failed to get good at its job. I'll try again with training discriminator 5 times more.

    I was trying to visualize the hint and I found that the hint was not updated consistently. Sometimes it is updated
    and other times it is just blank. Further digging into this shows that no matter what the updated hint is, the
    concatenated version of input will always be all -1s.

    More papers:

    unsupervised cross-domain image generation
    Downloading face data. That will probably take a whole day again...

2017/02/19
    Multi style
    Fixed ui, Plan: need to write a function to automatically batch style transfer an image.

    Food classifier
    The result of stylization using the food classifier vgg19 was just as I expected. The style didn't look any better.
    I haven't compared it with the original vgg19 but it might even be worse. Abandoning this project if I'm not
    planning to stylize using original vgg19

    pix2pix
    LAB result didn't look too much different from the normal one.
    with hint training looks a lot better (because it's an easier task) but the result looked aweful without hint.
    I will try to change the program so that 50% of the time the training is done without hint. Did that and resumed
    training.

    I am planning to do wgan again because I think I need the larger network to make things work better, but so far all
    my attempts to modify the network structure failed. I am suspecting that the discriminator is causing the
    colors to appear or disappear, so I left the discriminator as it is and changed the generator. I also made a change
    so that the WGAN only trains after 6 epochs. ( but I made a mistake and it is actually training the discriminator
    only once instead of 5 times for the first 6 epochs. The result is still black and white images.) I will definitely
    figure out what is causing it to turn to BW. TODO: figure out exactly what part is causing it to turn to bw by making small modifications to pix2pix.

    More papers:
    Saw a paper on a prelimary idea to apply style transfer to natural language.

    unsupervised cross-domain image generation
    Took a while to generate the training set pickle, but it works now. I'm just waiting to see the final result. The
    classifier is not doing so well on classifying celebrities. It can overfit the training data easily but it is
    getting a 60% accuracy on test set.
    The final result wasn't looking promising. The output had a lot of artifacts and the cats generated all looked the
    same. Trying with a larger network and see what happens. If it still looked the same, I will first deal with the
    artifact issue, then try to make the generated cats look more diversed. Plan: do the things above

2017/02/20

    Multi style
    Plan: need to write a function to automatically batch style transfer an image.

    pix2pix
    wgan result is ok. The network still output mostly black and white images but it is getting the skin color.
    Hopefully it can be better with just more training.

    hint is also working well. It paints pretty good with hint and not bad without them. It actuall is working along
    with GAN, unlike what taizan described. (I was at epoch 17) I will try to train a 512x512 network, but first I
    need to create the 512x512 training data.
    Running the following command while I also continued training the wgan (loading from a checkpoint may cause the
    WGAN discriminator train 5 times start point to be wrong...)
    python cgi-bin/paint_x2_unet/preprocess.py --colored_image_dir=/mnt/tf_drive/home/ubuntu/pixiv_full/pixiv/ --save_dir=/mnt/data_drive/home/ubuntu/pixiv_full_lnet_512/ --hw=512
    Plan: train on 512 data for hint
    Plan: check wgan again.

    More papers:
    Plan: actually read more, especially think about the style transfer on natural language idea. I need nlp feature
    extractor... but the idea itself isn't the same as image, because word is a different encoding from rgb image.

    Unsupervised cross-domain image generation.
    Still failed. The cat's face did not change too much except from its color. I am suspecting that the dataset might
    bear the blame, because the human faces had different orientations and even takes up different portions of the
    screen whereas the cats (at least the ones generated) all had their eyes and face fixed at the center. I took a
    look at the training dataset and found that the cats had varied orientations but their face is always at the center
    unlike the human dataset. But that shouldn't be the problem I feel like. Most of the human faces are still facing
    forward and cropped to the center...
    By making the content_extractor's last 2 layers from 128 to 512 greatly improved accuracy.
    Plan: I'll see how the training goes now.


2017/02/21
    Multi style
    batch style transfer done. PS. The result of stylizing sketches is very interesting. Some of the results look
    like modern arts.


    pix2pix
    512 data done in 10 hours.. Done. Combining images. # Plan: run 512 with hint.

    More papers

    Unsupervised cross-domain image generation
    It's getting way better with the change in the last 2 layers of content extractor. Not too many artifacts and the
    generated cat faces looked a lot better and more natural. It can definitely correspond skin color with cat skin
    color. It is probably hard to correlate expression on the current dataset of cat faces.

    I downloaded another dataset. I'm curerntly running this command and downloading the face dataset again on a new
    hard drive and crop it to 128.
    python main_cat.py --mode=pretrain --model_save_path=model_cnd_32 --sample_save_path=sample_cnd_32 --target_dir=cnd_32

    I can also train on cat and dog dataset although it is a lot smaller compared to the human face dataset.
    check result after I come back... It looks like a mess. No trace of anything that looks like animal face. Maybe
    I shouldn't reuse the pretrained model or maybe the target dataset is too small. I can try with anime face dataset,
    Plan: check cat and dog dataset result again. The first one failed. The only change I made is to retrain from scratch.

    Also running training on anime images. Plan: check anime images result
    python main_cat.py --mode=pretrain --model_save_path=model_anime_32 --sample_save_path=sample_anime_32 --target_dir=/mnt/data_drive/home/ubuntu/datasets/anime_face_32

2017/02/22
    Multi style
    There is something in stylizing the sketches that touches me. I don't know what it is but some of the effects I see
    make me think that this thing is more useful than just turning one style into another... It works on sketches...
    I'll also program a slow version I guess. I want to see the slow version's effect on sketches. I can work on that
    on the plane. Done... It's not perfect. I did not program showing the progress or emailing the result to the user.
    I did not give user any kind of control over stylization, but it is usable.
    TODO: allow asynchronous processing. maybe... if I have extra time. low priority.


    Wait a second. I thought of one thing. I was thinking that the content loss was too restrictive. It does not allow
    recreating the scene and/or resizing objects. Well I can use a generative model to solve this right? As long as I
    can think of a loss that compares the "content" of two images without encoding too much location information, then
    it can generate the image however it want! It can make the eyes larger if that fits the style image. It doesn't
    have to be constrained to the structure of content image anymore. Think about this...
    The problem with this hypothesis is I don't know exactly how the nn encode "features" and it may not work the way I
    want it to. For example it may encode an eye as four separate feature instead of one, and they have to be
    dependent on each other, so it may be better not to disentangle the four features if you still want an eye to look
    like an eye. CNN cannot encode an "eye" as an individual feature because the eye can be arbitrarily large or small.
    It has something to do with the architecture. I thought of doing font change. Think about that more. That is pretty
    fundamental to style change.

    pix2pix
    512 generation done. Running the following command
    python pix2pix_w_hint.py --mode train --output_dir pixiv_full_512_w_hint_train --max_epochs 20 --input_dir /mnt/data_drive/home/ubuntu/pixiv_full_512_combined/train --which_direction AtoB --display_freq=1000 --gray_input_a --use_hint --batch_size 12 --lr 0.0008 --gpu_percentage 0.45 --checkpoint=pixiv_full_512_w_hint_train
    Plan: check result
    made a mistake not to scale the input. So I was essentially training more on hint.  Switched to the following command
    pix2pix_w_hint.py --mode train --output_dir pixiv_full_512_w_hint_train --max_epochs 20 --input_dir /mnt/data_drive/home/ubuntu/pixiv_full_512_combined/train --which_direction AtoB --display_freq=1000 --gray_input_a --use_hint --batch_size 12 --lr 0.0008 --gpu_percentage 0.45 --checkpoint=pixiv_full_512_w_hint_train --scale_size=572 --crop_size=512
    WGAN on the other hand still did not work after 300000 iterations. The output is still black and white.
    Trying to run wgan only after that because the generator's loss never decreases. but I don't expect it to change anything.
    python pix2pix_large_gen_wgan.py --mode train --output_dir pixiv_full_128_large_gen_wgan_train_then_wgan_only --max_epochs 20 --input_dir /mnt/tf_drive/home/ubuntu/pixiv_full_128_combined/train --which_direction AtoB --display_freq=1000 --gray_input_a --batch_size 12 --lr 0.0008 --gpu_percentage 0.45 --gan_start_epoch=0 --checkpoint=pixiv_full_128_large_gen_wgan_train_then_wgan_only
    Plan: check result

    Unsupervised cross-domain image generation
    cat and dog still failed after fresh training. The generated image does not look like animal and it is almost all
    the same for all faces.
    anime images suffers from the same problem. It looked a little bit more like anime images but it is too homogenous.
    One thing I noticed was that all the generated anime faces are at the same location, regardless of the input image
    and the output. I know that WGAN will solve GAN's collapsing problem (that is all output look similar), but I don't
    know if I can make it work. I haven't succeeded with WGAN once yet. I was reading the author's WGAN code in torch
    and I noticed that for the first 25 iterations and every 500 iterations, it trains the Discriminator 100 times for
    each time it trains the generator.

    Added wgan, but didn't change training setting because the original training setting looks pretty wierd. Plan: check result

2017/02/23
    Multi style
    I want to control the degree of stylization... I capped the normalized weight to 1 but I think I need a setting for
    the magnitude after normalization, like a main volume control. I did that, but the result was not what I expected.
    It did not make the images "more stylized" by tuning up the master weight, or less stylized by tuning it down.
    Plan: Now what happens if I use a random variable instead of a fixed value for the place holder during the training?
    That will force the network to behave like I want it to: if the place holder's value is down, then the output is
    less stylized, and if it's up, then it's more stylized. If it's zero, then the output should be the content image,
    not some distorted version of that.
    I can explore pinterest for more ideas on what to paint...devian art is also good.



    pix2pix
    WGAN result was still mainly black and white. It appeared to have a little bit more color but it was almost
    unnoticeable. I checked the log and the generator loss never decreases (although the l1 loss decreased a little.
    Wait that was not wgan only because the weight for l1 loss was still on.
    Changed the setting to python pix2pix_large_gen_wgan.py --mode train --output_dir pixiv_full_128_large_gen_wgan_train_then_wgan_only --max_epochs 20 --input_dir /mnt/tf_drive/home/ubuntu/pixiv_full_128_combined/train --which_direction AtoB --display_freq=1000 --gray_input_a --batch_size 4 --lr 0.0002 --gpu_percentage 0.45 --gan_start_epoch=0 --l1_weight=0 --checkpoint=pixiv_full_128_large_gen_wgan_train_then_wgan_only

    The 512x512 was not working as well, because I trained it from scratch probably. Also I can't see the hint. I don't
    think the hint was on for some reason... Plan: write code to read from 512x512 data and check the hint...
    I modified the code but I haven;t check the hint is generated correctly or not yet.
    I checked the hint. It seems fine locally and I don't see any reason why it won't work on ec2. It maybe just that
    the number of pixel (40) is considerably smaller than the 512x512 image.
    Plan: check result.
    This project is taking way way way too long....

    Unsupervised cross-domain image generation
    Had a bug where I did not take the reduce_mean for all the logits when I calculate the loss. The model did not
    train. I fixed it and it is retraining now.
    Hum I think after 2000 rounds of training, it's getting somewhere. The generated images certainly are diverse
    enough, although it was kind of blurry and hard to find the relation between the human face and the anime face.
    One wierd thing is that the target d_loss is always 0 no matter what. I think something is wrong.
    It turns out that it is training just as normal, except that the loss from positive and negative just happen to
    cancel each other out almost perfectly with accuracy within 10^-5... Wait this is still too suspicious.
    I uploaded the current code and tried to add extra training for discriminator... I still can't find anything
    I have changed from the non-wgan version.I double checked and still can't find anything. Maybe that's just the
    way it works by coincidence...
    Plan: check the result.


2017/02/24
    Multi style
    Finally made the random one hot vector working. Hopefully it will just work and if not I'll think of something.
    Yay it seems to work! I don't know whether the quality will go down but so far I didn't see a huge difference.

    pix2pix
    wgan only failed miserably. The generator loss did not go down and the l1 loss kept going up from 0.3 to 0.6 and it
    plateaued.
    The 512 with hint result was ok. Some of them got the majority of the color right but there are still some simple
    places where it messes up. I think that's because I don't have the sketch regen loss and the color just overflows
    easily. plan: add that network after I come back from the zoo.
    Uploading before making changes. Adding hints works but wgan failed. I don't know why yet. Planning to add a sketch
    generation network so that the pix2pix can have a sketch generation loss.
    My conclusion on the experiments: 1. anything that messes up with the network itself fails, no matter if it's the
    generator or the discriminator. The network seems to be more delicate than I thought. Maybe I should do stepwise
    improvement. (Like add 2 layers at a time instead of 10).
    2. The network does not seem to be good enough since the loss plateaus at 0.25 ish.
    3. I can't see the obvious advantage of lab, although in theory it should be better.
    4. WGAN does not seem to help in this case.
    My step next:
    1. Move unnecessary files to other places. (DONE)
    2. Combine training 128x128 and 512x512 into one file. (DONE, need testing)
    3. Test with gradually adding layers to both generator and discriminator.
    4. Move the sketch generation network over here or (even better) find out a way to train such a network.
        The way to train it is pretty easy, since i already have the data. I just run the network the other way around
        and there you go. Plan: check training result. Used command:
        python pix2pix_w_hint_512.py --mode train --output_dir pixiv_full_128_to_sketch_train --max_epochs 20 --input_dir /mnt/tf_drive/home/ubuntu/pixiv_full_128_combined/train --which_direction BtoA --display_freq=1000 --gray_input_a --batch_size 4 --lr 0.002 --gpu_percentage 0.45 --scale_size=143 --crop_size=128
        Plan: add such sketch generator to the loss.
        Sketch training done. It's good enough to be used for losses.
        python pix2pix_w_hint_512.py --mode test --output_dir pixiv_full_128_to_sketch_test --max_epochs 20 --input_dir /mnt/tf_drive/home/ubuntu/pixiv_full_128_combined/test --which_direction BtoA --display_freq=1000 --gray_input_a --batch_size 4 --lr 0.002 --gpu_percentage 0.45 --scale_size=143 --crop_size=128 --checkpoint=pixiv_full_128_to_sketch_train
        python pix2pix_w_hint_512.py --mode test --output_dir pixiv_full_128_to_sketch_test_out_512 --max_epochs 20 --input_dir /mnt/data_drive/home/ubuntu/pixiv_full_512_combined/test --which_direction BtoA --display_freq=1000 --gray_input_a --batch_size 4 --lr 0.002 --gpu_percentage 0.45 --scale_size=572 --crop_size=512 --checkpoint=pixiv_full_128_to_sketch_train


    Unsupervised cross-domain image generation
    It's going back to the old problem again. The image looks pretty homogenous although it is definitely better than
    the one before. I don't know whether it is because I continued training and it will just be like that after too
    many iterations, or if it is my change in training the discriminator extra number of times.
    Trying to restart training from scratch and see if that causes the same problem after 2000 rounds. If it does (
    I don't think it will), then change the setting back would help, although it would not solve the problem of
    seemingly no direct link between the input and the output.

    Well after 2000 rounds of training it still maintained enough diversity although I can see that all the anime
    faces generated seemed to be concentrated at one place, like the training result at 5000 iterations.
    Plan: I want to see the output when the input is the anime faces.
    I did that by changing the source dir during eval. The result was surprising. The training dataset is even more
    diversed than I thought but the model tend to oversimplify what it saw when it tries to regenerate them. The result
    is blurry images. I think that is due to the fact that the network is too shallow. After comparing with the paper,
    I found that my network is actually not shallow except for the classifier part. The generator and discriminator
    part seemed fine. The feature extractor they used is for professional deep learning face detection and i can't find
    that anywhere.

    Plan: I also want to check whether the task of turning male faces into female ones can be done perfectly. If that cannot be done, then it's the network structure's problem, not the dataset.
    command used:
    python main_cat_wgan.py --mode=train --model_save_path=model_human_sex_32 --sample_save_path=sample_human_sex_32 --source_dir=human_32_male --target_dir=human_32_female
    # NOTE that I did not pretrain a model on only males... and the pretraining dataset overlaps with training dataset.

    I decided to switch to vgg face so I need 224 x 224 inputs. I need to :
    convert datasets to 224x224,
    test the model on the new datasets(hope they're not too slow)
    COmmand: # Plan: not done converting 224 yet do not using this command. 224 is too large for pickel so 64 g memory can't handle it.
    python main_cat_wgan.py --mode=train --model_save_path=model_human_sex_32_vgg --sample_save_path=sample_human_sex_32_vgg --source_dir=human_32_male --target_dir=human_32_female --batch_size=1 --hw=224


    More papers/ideas:
        Don't forget the idea about font style. it's important. Look it up when I have time.

2017/02/25
    Multi style
    Training not yet done on the 8 extra images. I've shown before that it is working.
    Done training. Training 9 more style images. The problem is, the output is pretty inconsistent with the slow
    version. It has less artifacts than the slow one, but more blurry, less vivid, and less 'creative'.
    I should probably start at least researching on font generation.
    Plan: research font generation.

    pix2pix
    Done writing code for sketch loss. Now retraining sketch model because the variable scope changed..
    Command used to train sketch gen:
    python pix2pix_w_hint_512.py --mode train --output_dir pixiv_full_128_to_sketch_train --max_epochs 20 --input_dir /mnt/tf_drive/home/ubuntu/pixiv_full_128_combined/train --which_direction BtoA --display_freq=1000 --gray_input_a --batch_size 4 --lr 0.002 --gpu_percentage 0.45 --scale_size=143 --crop_size=128 --train_sketch
    Command used to train sketch coloring
    python pix2pix_w_hint_512.py --mode train --output_dir pixiv_full_128_wgan_sketch_loss --max_epochs 20 --input_dir /mnt/tf_drive/home/ubuntu/pixiv_full_128_combined/train --which_direction AtoB --display_freq=1000 --gray_input_a --batch_size 4 --lr 0.0008 --gpu_percentage 0.45 --scale_size=143 --crop_size=128 --use_sketch_loss --pretrained_sketch_net_path pixiv_full_128_to_sketch_train
    So far it is pretty successful. It showed vibrant colors in less than 2 epochs (around 4 hours) and sometimes I
    would argue that the color choice of the network is even better than the ground truth. More training will do.
    The only thing left to test is I guess larger network (by gradually increasing the depth and/or add dilation).
    OH!! I see why the hint changes everything. Because the hint forces the network to use the color given, not just
    black and white. Deeper network won't do, changing it to wgan won't do. But why my experiment before failed? ..
    Tensorboard graph looks normal. Just more training.
    Plan: maybe make the network a little deeper and compare the result.
    Wrote a network with 3 more dilation layers after encode layer and before decode layer. The downside of this
    network is that it sacrificed depth for more dilation. (because dilation is better when the input size is large
    so that it can collect information at a distance.)
    Wrote another one with each convolution layer half as deep followed by a dilation layer also half as deep, then
    concatenate the two. This way i can get the concatenation without sacrificing the depth.
    Also looking into recombinator networks--turned out that it is just unet.

    WAIT! I forgot to turn on hint??? So the color did not come from the hint but from the sketch loss???? what?
    Interesting... Plan: compare each change and see which one causes the model to be better/have more color etc.
    ... Anyway restarted two experiments at the same time. One with unmodified structure one with modified.
    Plan: check tomorrow which one is better.



    Unsupervised cross-domain image generation
    Turning man into woman is working better than any previous ones, but the output is still blurry and looked a little
    homogenous. For example I can see it maps a smily man to a smily woman, but I can't see it mask say age or glasses.
    The overall layout position is mapped correctly most of the time (if the face is on the left for source, then the
    target will also have the face on the left). This showed the maximum capacity of the network. It has flaws but its
    working reasonably well. I'm hoping to see better results with vgg.
    Command used:
    python main_cat_wgan.py --mode=train --model_save_path=model_human_sex_128_vgg --sample_save_path=sample_human_sex_128_vgg --source_dir=/mnt/data_drive/home/ubuntu/datasets/human_128_male --target_dir=/mnt/data_drive/home/ubuntu/datasets/human_128_female --batch_size=4 --hw=128 --lr=0.00003
    I chose to use 128 because upscaling it should work on the vgg face and it makes training faster (and I don't have to modify the pickel code to generate 224 images)
    One of my worry is since the batch size is only 1, the lr may be too large.
    decreased lr by 10 fold while decreased batch size by 25.
    Tensorboard graph looked a lot more unstable than before--natural given such a small batch size compared to before.

    Better style transfer
        Problem: the style transfer is not good enough. One of the biggest problem is that it relies on matching a
        content image without making changes to the overall layout of objects. I would like the style transfer to also
        learn the semantic meaning of objects and to be able to change their layouts as part of the style transfer.
        My approach: In my mind, human break things into parts. Style is created by applying rules to those parts
        (whether it is as small as brush strokes or as large as the whole face). The current network is not able to
        accomplish that.... No easy solution found yet. All I think about is pretty fundamental so it's hard...


2017/02/26
    Multi style
    Worked on combining trained fnn models. It works but the problem is that the combined version is not as good as the
    not combined one. It is because I only took the shift and scale and did not combine the other conv weights and
    biases. Maybe I should take the average or something... dunno. Or maybe i should not train on those conv weights
    and biases anyway. Maybe I should create a mode where I fix the conv weights and just train the shift and scale...
    Well this is for later.
    Or maybe... it is not idea to only train on shift and scale afterall? I can check by training on a single image
    and compare that result with training it with a bunch of other ones.
    As I think of it, it would be stupid to average the three trained feed forward nn because how do you know they
    have the same content in their features? feature 123 works equally well as feature321...
    So the only solution it seems would be to only retrain the scale and offset while loading the weight and bias
    from previously trained model and keep them constant... Or, just keep them separate. lol.

    Also I think most of the time training on 1024 doesn't seem to give better results than 512 as far as I can see.

    I thought of a way. MRF does an awesome job at transferring a style into another given the semantic masking right?
    So we just need the right kind of feature extractor to give the semantic masking for the program. What would be the
    network that both recognize a normal human face and a transformed one? the unsupervised cross domain image
    generation paper of course. It trained the feature extractor to do exactly that. I am thinking that instead of
    one generator, we can have multiple ones, like one generator per font or something. They might be able to share
    the weight and bias and differ only in shift and scale (in batch norm)
    The feature extractor will still be shared. The discriminator... we'll see if it can be shared. The loss can
    draw inspiration from dtn

    it also would be better to provide some locational information to the generator, otherwise the things it can
    generate would be pretty limited. I can use the interconnected structure and let each generator improvise based on
    the semantic information given. It is free to move the eyes to wherever it wants based on the style it is creating.

    ha. That's something. TODO: think about this. I think it might work. It's not based off of nothing and I can work by small steps. and its potentiall.. if only I can get it good enough so that people are willing to label their style images to explain the semantic meaning.





    Unsupervised cross-domain image generation
    The output of vgg combined dtn is pretty blurry. I think it is weighed too heavily on the f consistency (oh because
    the feature vector is way larger than it previously is). Changed the alpha and beta value to the one suggested
    in the paper (although in the paper they used 256d feature extractor output vector and mine is 4096d, i don't know
    the difference in magnitude.) and we'll see if the blurring gets better. As I think of it, this might also be
    the problem in the other ones as well, like the blurry cat faces...gg
    The f was huge at the very beginning... it should be around 20 at first but now it's 20000..
    Yep the result of changing alpha and beta on was even worse. The output is all the same regardless of the input.

    The difference between previous experiment and the current one;
    previous experiment is able to capture some different eye positions and facial positions whereas this one is
    all one style. Previous experiment did not train discriminator 5 extra times (and of course no 100 times every 100
    iter). Previous experiment has a smaller pretrained feature extractor.
    I will go back to the previous experiment's setting and abandon vgg for now. I want to see the effect of :
        different alpha values
            -- it is said in the paper that higher value will improve consistency at the cost of noise and distortion.
        different beta values
        training wgan discriminator 5 times more.
        Let's first try alpha values.

    Tried different alpha values on the feature extraction trained on MALE ONLY. It failed miserably no matter what
    the alpha and beta values. I will try again with the feature extractor trained on both male and female.


    OH!!!! Now I realize why all the generated images seem to have their eyes and mouths all at the same spot. It's
    because the feature extractor's output is 1d!... fuck. Either the feature extractor learns to encode position
    information or the generator has no option but to take the best position on average! Ah.... fuck.


    pix2pix
    After 50000 iterations, my modified model is worse than the original setting. Both of them is training fine but
    mine had on average a higher loss (0.02 higher) than the original one. I will stop training on my model.
    Tried using the larger generator model again... Maybe now with hint and with sketch loss it will get better results.
    I was forced to reduce the batch size from 4 to 1 and lr from 0.0008 to 0.0002 accordingly because of the memory
    limit. I will compare the result after it trains for a while. I am expecting to see color at a very early stage.
    If it's not showing color then something is wrong and i will stop training immediately.
    It is showing some color around the hints but there's not much... Maybe that network is not good at all... I
    remembered the result I got a month ago. It was like that as well...

    It's getting a little bit better, but I don't see any obvious merit over the original setting yet. I mean the
    training is slow, I was forced to use a small batch size, and it doesn't seem to be better...
    Yep it's not better. The majority of the outputs are black and white. So I guess that is the deathnell for that
    network. I'll keep the with hint and sketch loss version training for a while, since the loss hasn't plateaued yet,
    and see where that leads to. Plan: check result...

    More paper:
    Downloaded a pinterest paper.

2017/02/27
    Multi style
    Plan: train one single style in feed forward mode and compare that with the multi style feed forward.
    Oh shit my setting was that the one hot vector was None when there's only one style meaning that I cannot control
    degree of stylization.. Oh well. Let's see the result before worring about fixing it... It is causing problems in
    the server code. Let me fix it... It turns out I've already fixed the problem in the training code but forgot to
    update it in the server code.

    The conclusion so far is: given identical rounds of training (ie. the multi style feed forward network get to train
    num_of_styles times more than the single style one), the single style is inferior to the multi style one,
    understandably. There is no obvious advantage to using the single style since although its details could be better
    the overall style is still the same as the multi style output. Both are different from the slow version.

    Plan: write code for combining the dtn and the neural style. Start out simple with svhn + mnist feature extractor (take the middle layers) and see if that work.
    But how could that beat the vgg? Or should I use that in addition to vgg as like additional domain specific
    semantic information? That sounds like a better idea than retraining a whole feature extractor... (remember the
    deep food experiment...) So the idea is to use mrf and use additional feature extractors... hum...Sounds like a plan...
    So I think all I have to do now is to add some additional semantic information layers to the mrf supplied by
    feature extractor neural networks?... I can recycle some code.



    Unsupervised cross-domain image generation
    Even main_cat failed. Rewinding to the version that worked... I thought some of the version did work.
    I double checked the code and found that the gradient clipping step was not included during training... Maybe that
    was why.
    python main_cat_wgan_old.py --mode=train --model_save_path=model_human_sex_32_alpha_15 --sample_save_path=sample_human_sex_32_alpha_15 --source_dir=human_32_male --target_dir=human_32_female
    python main_cat_wgan_old.py --mode=train --model_save_path=model_human_sex_32_alpha_1 --sample_save_path=sample_human_sex_32_alpha_1 --source_dir=human_32_male --target_dir=human_32_female --alpha=1.0
    Now it looks a little bit normaler. no exploding d loss or g loss. Plan: check result.

    pix2pix
    128 images looks pretty good now and the loss has plateaued after 2 days of training. Going to train 512x512.
    Plan: check result.
    It's working a little bit better than I expected. I'm one step closer to the target... The next problem is to
    completely solve the diversity problem. I can probably use the color bin technique. I have the code for it and I
    know how and why it works.:)

20170228
    Multi-style
    While I'm waiting for training to finish, I can code my idea up. Plan: code my idea.

    Unsupervised cross-domain image generation
    Finally some close to satisfactory result... The generated images are blurry but clearly the network has a
    understanding over the age, position, and lighting information in the male image and it preserved them in the
    female ones, which is superb. All I have to solve right now is to generate clearer images. I can think of making
    beta larger... that will force the generator to at least adapt to the female images and make them as not blurry as
    possible. If that does not work, I have to add a loss function that make the generated image "sharp"
    I compared beta = 1 with beta = 45. Both of them are a little bit worse compared to beta = 15 so I'll keep it like
    that. I guess one way to have cleaner image is to connect the output layers with the input ones, like unet.
    Experimenting unet... Plan: check result tomorrow

    pix2pix
    The result is really good (given that I've tried this for the past 2 month...). I can make improvements
    on the model's color in the mean time to make it even more colorful. TODO: make it more colorful. Priority.

    Found a potential bug in sketch loss... It should've been discovered long time ago. What's going on with the current training?
    ... basically I grabbed trainable variables for sketch generator instead of global variables... I don't know why the current training even worked.
    ... One other suspicious thing is that the sketch loss can propagate through the bin_to_rgb process... I'm not
    sure it will work like I intend it to... but we'll see.
    Command I'm going to run:
    python pix2pix_w_hint_512_more_color.py --mode train --output_dir pixiv_full_128_wgan_w_hint_sketch_loss_more_color --max_epochs 20 --input_dir /mnt/tf_drive/home/ubuntu/pixiv_full_128_combined/train --which_direction AtoB --display_freq=1000 --gray_input_a --batch_size 4 --lr 0.0008 --gpu_percentage 0.45 --scale_size=143 --crop_size=128 --use_sketch_loss --pretrained_sketch_net_path pixiv_full_128_to_sketch_train --use_hint --use_bin

    Neural style improvement:
        What I need: a mrf-based  neural style program. a new feature extractor. combine two feature extractors...
        the mrf loss is separate for eacy feature extractor layer, so in principle I can just create two feature
        extractors, compute their mrf-losses for their middle layers, apply different weights to the loss, and that's
        it...

        It's working on my laptop Plan: try on gpu with trained feature extractor...


2017/03/01
    pix2pix
    The color bin program produced NAN loss in the middle of the training. The lr is normal. everything looked normal
    up to that point.
    After waiting for 512 to train for quite a while, I think that maybe the network is not deep enough to handle the
    complexity... but whenever i make the network deeper it will be bw again... maybe I should just try to increase number of channels instead?
    Plan: check result and if it's not working, think about how to make network deeper without generating bw images.

    Unsupervised cross-domain image generation
    The output is still blurry after the unet. I can't say which one is better than the other.
    Or i can just use the image superresolution... i can't think of a good way for now...
    Plan: I can look it up online more later about why the output is blurry.

    Let me try the cat.

    The cat was a success, considering the dataset I have. Since all cats are facing forward, the best the network can
    do is to find the cats that are the least "forward facing" and match them with the not-forward facing ones. It can
    recognize expression, color, eyes, but it can't "create" smile or glasses. (cat's don't have them...)
    Why success? by comparing with the original svhn to mnist code.... Plan: summarize why I succeeded.
    The reason why this model succeeded was as follows:
        1. I made all networks deeper (double depth) so that it is complicated enough for faces.
        2. I changed gan to wgan, which helped the output to be more diversed.


    # Plan: And by the way the cat image did not look so blurry (which could be because it's hard to tell whether cat hair is blurry or not)


    The female to anime generation was a failure. the output was blurry and all the images tend to have their features
    at the same spot. That could be explained two ways: one, the target dataset is too diversed, so
    the network is unable to use the same feature extractor on generating the new images. Two: the network was
    not trained properly. -- which is kind of wierd because this setting worked for other datasets. I think it is
    the feature extraction network. Anime images are just too different from natural human images I guess. I can
    train a network on recognizing both human and anime images I guess and see how that does on the generation task.
    OR there might be just too few of them? The anime dataset has around 14000 images compared to 70000 of the human
    dataset.
    So far pretraining with anime dataset added seems to caused a 10% decrease in the test dataset accuracy (
    80%->70% while the training dataset accuracy is just 1.0)
    Plan: check the result of combining human and anime together to pretrain an extractor.


    Neural style improvement:
    command I'm using:
    python neural_style.py --network=imagenet-vgg-verydeep-19.mat --content=stylize_examples/5-content-128.jpg --styles stylize_examples/5-style-128.jpg --width=128 --height=128 --content-weight=10.0 --style-weight=30.0 --use_new_features --new_features_save_path=model_pretrained_human_32 --use_mrf --output="output/sanity_check_use_mrf_use_new_features_128.jpg" --checkpoint-output="output_checkpoint/sanity_check_use_mrf_use_new_features_%s_128.jpg"
    python neural_style.py --network=imagenet-vgg-verydeep-19.mat --content=stylize_examples/5-content-128.jpg --styles stylize_examples/5-style-128.jpg --width=128 --height=128 --content-weight=10.0 --style-weight=30.0 --use_mrf --output="output/sanity_check_use_mrf_128.jpg" --checkpoint-output="output_checkpoint/sanity_check_use_mrf_%s_128.jpg"
    python neural_style.py --network=imagenet-vgg-verydeep-19.mat --content=stylize_examples/5-content-128.jpg --styles stylize_examples/5-style-128.jpg --width=128 --height=128 --content-weight=10.0 --style-weight=30.0 --use_new_features --new_features_save_path=model_pretrained_human_32 --output="output/sanity_check_use_new_features_128.jpg" --checkpoint-output="output_checkpoint/sanity_check_use_new_features_%s_128.jpg"
    python neural_style.py --network=imagenet-vgg-verydeep-19.mat --content=stylize_examples/5-content-128.jpg --styles stylize_examples/5-style-128.jpg --width=128 --height=128 --content-weight=10.0 --style-weight=30.0 --output="output/sanity_check_128.jpg" --checkpoint-output="output_checkpoint/sanity_check_%s_128.jpg"
    python neural_style.py --network=imagenet-vgg-verydeep-19.mat --content=stylize_examples/7-content.jpg --styles stylize_examples/7-style.jpg --width=32 --height=32 --content-weight=10.0 --style-weight=30.0 --use_new_features --new_features_save_path=model_pretrained_human_32 --use_mrf --output="output/sanity_check_use_mrf_use_new_features.jpg" --checkpoint-output="output_checkpoint/sanity_check_use_mrf_use_new_features_%s.jpg"
    python neural_style.py --network=imagenet-vgg-verydeep-19.mat --content=stylize_examples/7-content.jpg --styles stylize_examples/7-style.jpg --width=32 --height=32 --content-weight=10.0 --style-weight=30.0 --use_mrf --output="output/sanity_check_use_mrf.jpg" --checkpoint-output="output_checkpoint/sanity_check_use_mrf_%s.jpg"
    python neural_style.py --network=imagenet-vgg-verydeep-19.mat --content=stylize_examples/7-content.jpg --styles stylize_examples/7-style.jpg --width=32 --height=32 --content-weight=10.0 --style-weight=30.0 --use_new_features --new_features_save_path=model_pretrained_human_32 --output="output/sanity_check_use_new_features.jpg" --checkpoint-output="output_checkpoint/sanity_check_use_new_features_%s.jpg"
    python neural_style.py --network=imagenet-vgg-verydeep-19.mat --content=stylize_examples/7-content.jpg --styles stylize_examples/7-style.jpg --width=32 --height=32 --content-weight=10.0 --style-weight=30.0 --output="output/sanity_check.jpg" --checkpoint-output="output_checkpoint/sanity_check_%s.jpg"

    Well, the output for 128 was quite clear. the original version without mrf is the only one even close to a success.
    Using mrf+new features gives the style image while using only new features gives a slightly worse-looking content
    image. 32x32 gives quite different results from 128x128.
    32x32 images were just too small to see... Wait. is that why the 32x32 images look so blurry? Because it is
    enlarged for viewing purpose? but wait then the source image should be processed the same way and they didn't look
    blurry so it's not possible.
    TODO: I want to adjust the weights around and see what happens--to the 128x128 perhaps. 32x32 is just way too small...
    TODO: also check the only mrf results.


2017/03/02
    pix2pix
    the rgb color bin was working really well--until it became nan...
    I still can't figure out why it's nan and I'm doing random guessing.. which is not good for debugging. i
    need to reproduce the problem and find the cause. Plan: check result. hope it's not nan again and if it is at least it should be terminated before saving.

    Unsupervised cross-domain image generation
    By pretraining the classifier on both human and anime data, the generator got a lot better. Although all the
    generated anime faces are still at the same spot, their features now somewhat corresponds to the human features.
    The model mistakens the hair with the background of the generated anime images, but you can tell if the hair is
    white, then either the background or the hair of the anime character is white. So the real problem is dataset I
    guess. I need a huge dataset of anime characters labeled with their face regions and also other characteristics.

    Plan: split the human dataset so that it is small enough to be fit into the memory. Then try to train 128x128
    Command used:
    python main_cat_wgan_old.py --mode=pretrain --model_save_path=model_male_to_female_128_alpha_1 --sample_save_path=sample_male_to_female_128_alpha_1 --source_dir=/mnt/data_drive/home/ubuntu/datasets/human_128 --target_dir=/mnt/data_drive/home/ubuntu/datasets/human_128_female --alpha=1.0
    cp -r model_male_to_female_128_alpha_1 model_pretrained_human_128
    python main_cat_wgan_old.py --mode=train --model_save_path=model_male_to_female_128_alpha_1 --sample_save_path=sample_male_to_female_128_alpha_1 --source_dir=/mnt/data_drive/home/ubuntu/datasets/human_128_male --target_dir=/mnt/data_drive/home/ubuntu/datasets/human_128_female --alpha=1.0
    python main_cat_wgan_old.py --mode=eval --model_save_path=model_male_to_female_128_alpha_1 --sample_save_path=sample_male_to_female_128_alpha_1 --source_dir=/mnt/data_drive/home/ubuntu/datasets/human_128_male --target_dir=/mnt/data_drive/home/ubuntu/datasets/human_128_female --alpha=1.0
    # STOPPED the experiment because i wanted to run resnet... pretraining is done though.


    TODO: Find dataset.
    More face dataset:
    https://data.vision.ee.ethz.ch/cvl/rrothe/imdb-wiki/
    Face recognition network is a little bit hard to train. I can start with the simple way... using the existing
    structure with the feature extractor swapped in.
    Also I found an anime face detector that I might be able to run on my pixiv dataset. It's working pretty well.
    There were some positives but it's ok. I can probably manually go through them... or not. But how do I know who
    those characters are though? Or can I just make the classifier classify the tags of the source image from which the
    images are cropped?
    I thought of cropping the faces from the anime videos, that way at least I know the characters that's in there, but
    the program still doesn't know which character it is... Unless I train another program (maybe active learning) that
    learns to recognize the characters... somehow. This is an interesting problem by itself... After I have that
    labeled dataset, I can do lots of things on it. This problem consist of several subproblems: object detection under
    all kinds of environments, object tracking, pixel labeling... TODO: think about this potential idea.
    I guess my adventure with Unsupervised cross-domain image generation has to be paused for the moment. The quickest
    way to improve it is to get dataset and I don't have that. Wait a sec... If only I have a better trained human
    face recognizer, it should be able to be used on other domains, given what the paper has demonstrated... So there
    is something I can do for now: TODO: try to get a better face feature extractor.
    Trying resnet on a gitrepo. It may work... I'd be surprised if it doesn't.


2017/03/03
    pix2pix
    still training. No error yet so far.
    TODO: transfer the sketch generator to pix2pix, so that i don't have to use a whole network to do the job... Or as a second thought maybe that's not worth it.

    Unsupervised cross-domain image generation
    It definitely knows the major features of human faces. For example it knows the location of eyes, faces, mouths,
    and hair color. It can't recognize glasses though. The output is quite good in that sense, but it's blurry as hell.
    This is true when the input is target images (ie. input female output female). The solution I can think of is
    to introduce something like the sketch loss. The generated images' sketch must also pass the discriminator test.
    The only thing is I don't know how to compute the sketch from tf directly (without training a network for it) yet.
    Let me look into that...

    Command used:
    cp -r data/20170216-091149 model_male_to_female_128_resnet_sketch_loss_alpha_1
    python main_cat_wgan_resnet_sketch_loss.py --mode=train --model_save_path=model_male_to_female_128_resnet_resnet_sketch_loss_alpha_1 --sample_save_path=sample_male_to_female_128_resnet_sketch_loss_alpha_1 --source_dir=/mnt/data_drive/home/ubuntu/datasets/human_128_male --target_dir=/mnt/data_drive/home/ubuntu/datasets/human_128_female --hw=128 --alpha=1.0
    python main_cat_wgan_resnet_sketch_loss.py --mode=eval --model_save_path=model_male_to_female_128_resnet_resnet_sketch_loss_alpha_1 --sample_save_path=sample_male_to_female_128_resnet_sketch_loss_alpha_1 --source_dir=/mnt/data_drive/home/ubuntu/datasets/human_128_male --target_dir=/mnt/data_drive/home/ubuntu/datasets/human_128_female --hw=128 --alpha=1.0


    Neural style improvement:
    Plan: Now that I have resnet, I can improve on what I have and try it on larger resolutions!
    I think I need to implement color independent style transfer... Some of the images are just not in the right color,
    especially food.
    I found a bug which made my previous experiment invalid (the style loss was not using the new features for the
    generated image.
    Inception net doesn't seem to work very well with mrf... Oh is that because the patch size is larger than the
    embedding size?
    What should be the input to the inception net? I think it should be uint8 0~255 as far as I can see... yep. So my
    input to it in the cross domain image generation was incorrect... TODO: fix that.. did that. then check the result, compare with before fix.
    also forgot to change the content loss to incorporate the new featuers.
    Finally done with getting things to almost work. The output of style loss only does not make sense,
    so I think there's still things going wrong.


2017/03/04
    Unsupervised cross-domain image generation
    The resnet made things worse. Again I'm pretty sure something is wrong with the way I'm using resnet.
    While I figure that out, I will test the sketch loss (without resnet) to see if that brings any improvements.
    Command I'm using:
    cp -r model_pretrained_human_32 model_human_m2f_sketch_loss_32_alpha_1
    python main_cat_wgan_sketch_loss.py --mode=train --model_save_path=model_human_m2f_sketch_loss_32_alpha_1 --sample_save_path=sample_human_m2f_sketch_loss_32_alpha_1 --source_dir=human_32_male --target_dir=human_32_female --alpha=1.0
    python main_cat_wgan_sketch_loss.py --mode=eval --model_save_path=model_human_m2f_sketch_loss_32_alpha_1 --sample_save_path=sample_human_m2f_sketch_loss_32_alpha_1 --source_dir=human_32_male --target_dir=human_32_female --alpha=1.0

    I thought that it was because I did not do pretraining on the dataset... but why would I... Unless the features
    from other dataset cannot be used on mine?...Dunno. Also the trainable variable just was still there even when I
    set the trainable argscope to false...
    Anyway, I think I'm giving up on resnet... Using others code like this just doesn't work so well...

    Probably I should just train my own... If I'm serious about this project. I do think it would be cool if I can turn everyones face automatically into anime or cat faces.
    TODO: first do some research on the effect of not using vgg19 in stylization. If the difference is small or it doesn't work like what I thought, then I should just stop... It's not worth spending time to train a whole network to detect faces only to find out that it won't work and people have tried it later on.

    So far the one with sketch loss looks promising...
    Ok the result was not what I expected. It did get better, but it's in terms of semantics, not the blurriness.
    The output is still blurry, but with sketch loss it also have some artifacts, which arguably makes it look less
    blurry. Anyway... I guess I should keep the sketch loss for now...
    I thought of one thing. maybe the blurriness is due to the network structure. For example, if the last layer comes
    from deconv of a smaller size, then it should be blurry... (it would be wierd if it's not...) In this case, what if
    I add one to two layers at the end that hopefully can make the output look better? This way because the last layer
    now does not come from enlarged smaller image, it should look a little bit better?

    pix2pix
    I need to focus on this project to just get it done. It's almost there. My experiment on using color bins was a
    failure. Or is it? Or is it because I used t = 1... Otherwise I can;t explain why the images are mostly black and
    white because the purpose of color bins is to avoid averaging all the colors (which results in bw). When I lower
    the t value, the results get a little bit more colorful looking, but not even close to the ones trained without
    color bin. The color conprehension still stops at understanding which parts are human skins. BTW lowering t too
    much (to like 0.1) will ruin the results.
    Anyway, in conclusion I should probably abandon using color bins. It takes too much time to train and does not
    get good results.

    I also want to see the output of the 512 network.
    The output of the 512 network without hint was actually not impressive at all. It likes pink a lot for some reason
    and it rarely gives satisfactory colored results.. With hint is similar. It is a little better than without hint
    but still far away from the actual output. I guess this is because it is overfitting the training data??? Dunno.
    Also check the 128 network...

    I know changing it to lab should improve the colorfulness-- in theory... I also know in theory that wgan has better
    diversity than gan, althought it hasn't been seen in practice for this task (maybe I was training it wrong, because
    in the dtn task it got better results without training extra rounds for the discriminator.)
    I also know that more data is always better (although I don't know the quality of the images I scraped from pixiv.)
    It might be better that I use the anime face detector on the dataset to clean it up, but before that I want to
    train on the scraped pixiv dataset and compare that with pixiv-full dataset.

    I am wondering why I failed... despite the fact that I have access to the source code (although incomplete).
    I should think of this like a product... The users on twitter clearly wants to color humans more than anything else,
    so my dataset should be mostly humans (which can be done through face recognition). The network he is using is
    different from what I am using (his is deeper). He is also using a different color space.

        -dataset
            This is the life and blood for all machine learning tasks. I should have an excellent dataset.
            The dataset should avoid containing bw images, images with little color or mostly one kind of color.
            The dataset should contain a human (or at least a face) which can be recognized by the face detector.
            The dataset should not have a wierd height-width ratio, or I have to use a different way to crop the input
            image- like getting part of it.
        -network
            The author provided the network structure so I should use that.
        -training strategy(aka losses)
            Start from l2 loss and see if it can get at least bw images.
            Then add GAN or WGAN and see if it can get some color
            Finally add hints and see if the output is close to the actual image.
        -general rule:
            Avoid changing more than one thing at a time. Otherwise it will be confusing during analysis.
            Always do sanity check before running code on larger dataset.
            Always check manually the dataset you;re using.



         I've done most of the stuff described in here. I'm lacking in the dataset part, so I should focus on that.


        Plan: first filter out all images that does not contain anime character face.
            I wrote a program that contructs a list of images that contains at least one face.
                Number of images containing faces: 55211 out of 442701
            I combined the color and sketch images to get a training set.
            The output was much more colorful just by changing the training dataset... I should've paid more attention
            to that... Anyway, If I need to download more dataset, I know how. Also if I need to do that, I should
            figure out how to also save additional information about the image, not just the image and the author etc.
            Maybe also the comments, the tags, etc.
        Plan: Add wgan to the model. (do NOT use only WGAN because I tried and it did not work.), also don't train extra for the discriminator for now.
            I did... the wgan loss was a little bit too high and since there is no log, there doesn't seem to have a limit for how much it can grow.
            Yeah at the end the l1 loss just kept increasing instead...
            I'm trying to decrease the gan weight from 1 to 0.01... Well that didn;t work.

            Hum I changed the following code from :
                with tf.control_dependencies(discrim_clips):
                    discrim_train = discrim_min
            to:
                with tf.control_dependencies([discrim_min] + discrim_clips):
                    discrim_train = tf.no_op("discrim_train")
            and suddenly the loss looks normal... I think that is because the clipping must be placed behind the discrim training and before generator training..
            This is the right way: the previous one mixs clipping and training randomly, which is not good...
                with tf.control_dependencies([discrim_min]):
                    discrim_train = tf.group(*discrim_clips)
        Plan: add lab.
            I did.
        Plan:
            Write a program that combines all the steps I took from the raw iamges scraped from pixiv to the
            images I use for training. (Maybe up to color and line is enough for now)

    Reading papers:
        A Discriminative Feature Learning Approach for Deep Face Recognition
        Generative Image Modeling using Style and Structure Adversarial Networks

2017/03/05
    pix2pix
    The result for lab+wgan looks a lot more satisfactory than before. Although it still avoided giving too much
    color for some reason, it clearly knows what parts are human skin. I have yet to run it on the test set but
    training set results looks ok. Yeah the results are mostly black and white...

    Interestingly without lab + wgan seems to give more color than the with version. I think I've seen this phenomena
    before, where the wgan just refuses to put color on things... I'll not use wgan then... Wierd... What's causing it?
    I know from previous experience that switching to lab does not affect coloring... Maybe the weight clip is too
    small for the discriminator to do a good job?
    TODO: test the two ideas: first try to relax  the weight clip to 1 and see what happens.
    Running it on the sanity check and i think the clip mainly dictates how fast can the WGAN loss decrease.
    maybe I should run the discriminator training multiple times... not sure. in the past that did not help though.
    Ok relaxed the weight clip to 0.05 (1 is too large) and I saw some decent color! :0 I'll keep training it and
    see what happens.

    Anyway the one without lab and wgan is still training and I didn't see any sign of it reaching a plateau yet!
    Also the output seems to have enough color... (Although I haven't tested with no hint input yet... so I don't know
    for sure).
    Some existing issues:
        Blurriness
        Color overflowing
            I think the author tried to overcome this by adding the sketch loss.
        Bad control over the use of light
            Is there any classifier for use of light in images???...
            Maybe it can be done by training on data built by 3d models like unity or something.
        No uniform color over the same object sometimes (which ties into the three issues above)


    What I've learned:
        *** Dataset and preprocessing them is super super important to good output.
    Plan:
        Merge the 512 images... Done
        Modify the pixiv downloader program and see what I can get out of it apart from images... Done
        Download more images... Done,
            still downloading. Got 60000 images in the last hour but only 6000 webpages... This is a little bit strange.
        Write code to preprocess downloaded images.
            I was hesitating whether to save the resized version of all images or just those that qualifies.
            I finally decided to save only those that qualifies... to save time.

        Think about how the issues above can be solved.

    The result for 128x128 without lab or wgan was acceptable. It got the color right sometimes (like 20% of the time)
    without hint. The rate was higher with hint (like 40~50% of the time). I'm training it on 512x512...


    Unsupervised cross-domain image generation
    Ok adding an additional layer definitely made things better. The images looked a lot clearer than previously. It
    proved my hypothesis that the network needs some intermediate layer to generate clear final image (instead of
    directly generating from deconv smaller images). I think the sketch loss is making thing worse semantically. By
    comparing the sketch loss version and the non-sketch loss version, I found that the sketch loss version usually
    comprehend less well spacial things like head tilt or head position. I don't know why sketch loss would cause that
    but that's the result... I think it might be better to just take that off.
    Hum... after 2000 iter I can see that the output is still blurry (not as clean as when I have the sketch loss).
    Maybe I should just tune down the sketch loss weight.

    Reading paper.
        The two new papers I downloaded did not seem interesting...

2017/03/06
    Unsupervised cross-domain image generation
    The experiment that ran a version with extra layer but no sketch loss generated blurry results as expected. The one
    with sketch loss weighted less was a failure. It had generated lots of artifacts (probably due to the fact that
    while I weighted the sketch loss differently, I did not also scale the threshold on the sketch loss GAN weights...)
    The artifacts are affecting the output by a lot and the output looked mostly homogenous.
    After comparing all version, I think the one with unet is the most successful one. It is a little bit clearer than
    the other ones and was semantically equivalent to the first version.
    I think this concludes this project until I can have more dataset.

    pix2pix
    Larger clipping value definitely gave the generator a much harder time, but the generator is getting better little
    by little. When I was looking at the discriminator's per pixel judgement, I was surprised to find that it made most
    of the judgement based on pixels near the edge of the image. This is pretty wierd. I mean the result looks nice,
    (At least it is bearable) but the way discriminator was behaving was wierd... Unless there was something wrong with
    the conversion process from not-capped values to image... That could be the reason... TODO: look into that.
    The 512 result wasn't so satisfactory yet... Maybe it was a little bit too early to train 512x512... I should do
    that only after I've solved everything else.
    And I was right about not training 512 so soon, because the wgan + lab version is definitely better than the old
    version! The color of the human skin was much more accurate and the color choice looks more natural. :) Finally I
    can see some images that I really liked. I think it was both because of the lab and the wgan, but I don't know
    which contributed more to the better result.

    Anyway, since now I have a better dataset, it might be worth it to revisit the other attempts I've made. I
    was especially puzzled by why having a larger network end up yielding bw images and why the rgb bin system did not
    work. The rgb bin I can partially explain by its slow training speed (not really because the network should show a
    decent amount of color by around 10000 iter, and if it hasn't by that time, it never will in the future. This is
    just from my experience), but having a larger network resulted in bw image always always puzzled me. So I will
    try both of them again, along with lab and wgan.

    I started to think what can I do after this project. I will have a huge dataset of around 1TB of images along with
    some related information. I can probably build a classifier that will automatically apply tags to images. I might
    be able to generate comments for the images. I might be able to predict the number of bookmarks based on the
    images and the tags (maybe I should not include the tags, or I can include the program generated ones).
    Also, there seems to be a bug in the sketch generator network... It should be untrainable...
        It was in the line:
        sketch_var = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=SKETCH_VAR_SCOPE_PREFIX + "generator")
        #: shouldn't the sketch network be fixed??? Shouldn't this reutrn empty list and we should use global variables instead?

    I then found a bug, where I forgot to add a "trainable" to a batchnorm step in the generator function. I fixed it.
    I decided not to run the 512x512 training yet. As I've mentioned before,
    ***it is better not to spend time on training 512x512 when I still have room for improvements. I can do that after
    I've found the best model I can make. I'm running the rgb bin model. Interestingly I found that the model is
    already learning the skin color a little only after 1000 iterations. I didn't see any other colors yet but that's
    a good sign (bie nai)... The training speed is also faster than I thought. TODO: I should come back and check tomorrow.
    Or maybe it will not afterall. I still didn't see any color apart from bw and a faint skin color after 2000 iter.



    Read papers
    I read some papers on DRL (Deep Reinforcement Learning), which is used largely for game ai or other feedback-reward
    systems. They're interesting but hard to understand... I'll read into them more, especially the
    "Playing Atari with Deep Reinforcement Learning" paper.

2017/03/07
    pix2pix
    I think the rgbbin does not work. Now it still outputs only black white and skin color most of the
    time. It colors the eye sometimes... Also the loss is saturating too fast. I already can see that it began to
    plateau very fast, so I don't think it will reach the quality users are asking for. One other thing that may
    make matters worse is that the hint given is in rgb color space but the output is in rgbbin, so now the network
    can't just take the hint and use it directly...
    I still want to test the larger network again, because first of all taizan used that network, and second is that
    if in the future I have more data, having a larger network helps in theory (but from what I've seen, it does not
    help at all...) Hum... Maybe I can train that larger network along with the 512x512... So in case the larger
    network gives bw images again, the time won't be totally wasted.
    So I basically copied what I had before into the new code, then ran the sanity check. The sanity check failed
    because wgan was unstable... I think the cap also needs to be adjusted with the network, which I didn't know
    before. Why is that though? I changed it back to 0.01.
    Training after 3000 iterations and I still didn't see too much color, which is a bad sign.
    Yeah after 8000 it's still black and white. I'm stopping it. Hum so it has nothing to do with the training data...
    Why both of them only produce bw? They're definitely capable of producing better ones... It's not the discriminator
    because


    I think I also need to start writing code for a pixiv tag classifier. Probably I should use vgg, cuz that is
    already available to me. (but it will train for so long though...) well.
    Preprocessing steps:
        Get path to all image files
        Extract information from txt.
        Make a dictionary with key = image id and value = image info
        Count number of times each tag appeared, then 1. either make a one hot vector for each tag 2. make a random
        variable for each word in the tag, add them to form a vector for each tag... Wait that doesn't work when I
        need to output more than 1 tag... I don't want to end up with a 100000-d one hot vector, but I want it to
        output the correct tags... One option is to accept a second input and have a few more fully connected layers,
        output 0 if the second input vector is a not a possible tag and 1 if it is a possible tag.
        hum... I guess I'll have to check how many tags there are and then decide.

2017/03/08
    # Plan: finish up pipeline for loading pixiv data.
    # Plan: test the sketcher with real sketch.
        Failed. It didn't look as nice as the test set. Why? I think it is because of the way I "generate" sketches.
        The network is only used to those generated sketches and not the real ones.. One solution is to pass the input
        through those generators before feeding them in... Plan: Let me test that.
        Generating the sketches based on the bw images using the chainer tool didn't give me any good results either.
        Lemme try to generate some sketches using colored images.
        Testing on rella's sketches did not give good results either. I can see that the hint is working properly and
        the network is coloring things with the right color with hint. But no hint is just a mess...
        Now what should I do? I mean the training images looks quite nice, and the test images for some reason looks
        reasonably nice as well. It's just when I use real sketches, everything breaks. I am kind of suspecting that
        the test images appeared in the training set but that shouldn't happen... I mean I created the test set twice,
        once for 128 and once for 512. It can't be that both of them was wrong...
        It's not the test set was included in the training set.
        After careful comparison, i realized that the result from the current training wasn't "so good" either. It
        was good when there's hint, but when there isn't, the result was not so good. Maybe the network is too
        dependent on hints...  I can try once without hint.
        I also realized that the difference comes from the input.  The input I was using was too complicated, but the
        ones in the training data are pretty simple. (They tend to have a human taking up most of the space.) So when
        I look at the one that looks similar to the layout in training set, the result was ok. So it's not I was doing
        something wrong. That's just how things work I guess.

        One more thing, I found an article where someone retrained PaintsChainer using a different dataset, which I
        wasn't able to do very successfully. I might want to look into that.
        http://qiita.com/ikeyasu/items/6c1ebed07b281281b1f6#_reference-1c2f00dca1ba5e029fe3
        Hum it seems to also mentioned that the result wasn't so good when used on pixiv data. He used images from
        anime, which is very different from pixiv images. That's probably the reason.
        * Again that shows the importance of having a good clean dataset. It also shows that if the input is different
        from the training data, don't expect it to get good results...

        One side note regarding the on-going training. I found that the learning wasn't so stable so I increased the
        batch size and decreased the lr. Well batch size 4 seems to be the max...

    # Plan: figure out (again) how to set locale to utf8 on ec2.

    More paper
        SeqGAN had some great ideas that combine gan and reinforcement learning. I didn't see any sample text though...

2017/03/09
    I finished coding up functions to load the pixiv files. All is left is to resize them, then train.
    Here's what I'd like to do while i'm waiting for more data:
        1. Try with hint but no WGAN and with WGAN but no hint (lower priority).
            Image with hint but without gan looks pretty dull. So now I'm 100% certain that it is WGAN or GAN that
            causes the image to have colors.
            Didn't try without hint but with wgan.
        2. Try lowering the probability of having hint. It's too dependent on hint now I think.
        (after examining without wgan. I don't want to have two changes at the same time.)
        3. Look into PaintsChainer and try to train using that code.
        	Difference from my code:
        	I don't have weight decay (which I think should be easy to implement...)
        	Their sketch weight is 10x mine. ( I should try to change that. High priority)
        	    Doing that and training for nearly 2 epochs does not give good looking images. The color was just messy.
        	    Maybe training longer could get some good results. I need to train 11 epochs to compare with weight = 1
        	Their batch size = 16 and learning rate = 0.0001
        4. Read more papers before actually running the tag prediction code. I feel that the current version may take
        very long.
            Found one paper unrelated to this but it's about text 2 image. It's pretty interesting so I'm going to read
            that through. It is also related to my project because now I have more text data.

            The paper "Recognizing Image Style" is quite good. It mentioned how to collect data, and how Flickr tags are
            not so accurate and they used Flickr group instead. Pixiv does not have that so I'll have to find another
            way...
            Or I can make classification on author? or even group authors together? Dunno. Readon
            It uses lots and lots of features... But this should be hugely helpful for my project. The learning task
            is much simpler if I can learn one style, not like 10000 mixed together in the dataset.
            Maybe I can cluster the images based on the tags they have... Then train one cluster at a time? but clustering binary vectors is a challenging problem of itself...
            As I think of it, maybe PCA is good enough...

        5 Figure out what's wrong with pycharm.. It can't save my notes.
            Turns out the C drive is full due to ubuntu backups.
        6. Working on changing the larger version to be almost exactly the same as chainer one.
        	I found some huge differences in terms of the order of operation between my code and the chainer code. Maybe that's why
        	it failed so many times before.
        	Done modifying it. Double check before try to train it.
        	It's working to identify and output skin color after 8500 iter, and training speed was compatible with the smaller version.
        	I'm a little bit more hopeful about this one.

        7. Figure out why the output summary was wrong..
            The wierd thing is it looks normal in the "bigger" version. Not a huge deal for now...

2017/03/10
    If it fails again:
        Compare with PaintsChainer. Try to get that code running and see if it produces the same output. If so, then
        it's my data's problem.
        If not, then it's my implementation. Actually I'd be quite happy if the chainer code produces good results...
        Because I haven't seen it produce good results yet.

        The result surprised me. Clearly the training has been not bad at least. The l2 loss was lower than the l2 loss
        by the smaller network. It learns skin color pretty well and it is almost overfitting the training data. It
        learns skin color well most of the time but it's having a hard time coloring other things. It is sucessful
        sometimes at that task but fails other times. The generator and the discriminator trained till the
        discriminator can no longer distinguish between the real and the fake (indicated by the loss). Combining that
        information with the failure in coloring images from time to time, I conclude that the bug is in WGAN. I should
        increase the threshold so that the discriminator can be better at its task. If I need to, I can also train
        extra on the discriminator, but I don't see the need for that now.

        Training with clip = 0.04. I don't see more colors yet after 6000 iter. Check later.
            Yep definitely more colors (at least when it's epoch 5. I don't know if more epochs will make it have less
            color. If so then there's something wrong with the code.
            But... If it doesn't have the same vibrant color without hint, then I'm doomed. One possible thing that I'm
            missing is that the number of hints is fixed. I can do some trick and have like 5 random variables
            representing different level of hints. (instead of one variable representing number of hints, because then
            the compiler will complain.)
            After only 5 epochs the output with hint was awesome. Some of them was almost exactly the same as the
            target. It is the first time that it shows it can fit/overfit on a large dataset. The output without hint/
            was a little bit lack of color, but it had some good outputs and none of the outputs looked bad. (Which is
            preetty impressive). The worst thing it can do is to not color things... :)
            The model performed poor with real sketches, but it's ok... The most of the real images probably doesn't
            look like training data.
            It performed ok on the rella sketches. Again I see that if the image contain faces, then the result will be
            way better than the ones without faces. And again, given hint it will perform better. I guess I'm finally
            coming close... After soooo long, I ended up with a solution almost the same as the chainer one.....
            except the dataset probably played a huge role in the improvement.

        Now, before I dive into 512x512 training, check the chainer code again and see what they did. They had
        different code for 128 compared to 512.
            I still haven't fully tested setting sketch loss from 1 to 10 yet on the new code. - 1/2 a day
            His 512 x 512 code did not have a discriminator.
            His 512 code had a different batch size (4) compared to 128 (4), but the lr did not change. Showing that
                this part did not really matter that much.
            He resized 128 x 128 and fed that as the hint of 512x512, but he also mentioned during our meeting that it
            was just an experiment.
                I personally do think that having a smaller network along with a larger one with the same structure is
                a waste.
            His data feeder was slightly different. (And both his 128 and 512 data feeder was different from mine).
                But wgan claimed that it removed the necessity of having such data manipulation during training.

            Conclusion: I can probably go ahead and train 512x512. It is 11 times slower, but oh well.

        #Plan: I need to figure out what the image summary of the real and fake is doing... It might have some bug.
            Fixed that. It is because the discriminator no longer have the tanh function.

    The utf8 is not working again... The default locale setting was still there. probably something to do with sh?
        Oh that's just the screen. THe screen was created before the change...

    Papers:


    Clustering and preprocessing data.
        PCA is the way to go. 10000 most popular tags. Covariance matrix. Just times the transpose. It will be huge
        so look for existing algorithms in languages like C.
        Done... sklearn has the related code I need.
    Sketch generation
        Input is crucial, so look into this.
        I need a database of sketches... I want to see what real sketches are like first... So probably find the
        corresponding tag

    Preprocess currently have a bug that did not exist before. That was probably due to some change in software
    packages... And how large should the cascade magnification be? I took the code directly which works fine, but I
    probably should know more

    Plan tomorrow:
        0. Download more sketches. I need them to evaluate the network.
        1. Check the result for 512x512... so far so good up to 5000 iter.
        2. Check the result for downloading pixiv images
        3. I now have two options after the pixiv images download is over:
            one is to run the anime face classifier and get a dataset from that.
                Downside is the face classifier can run for super super long on the whole millions of images
                Upside is I know it's going to work
            two is to create a database, form clusters, and resize only the ones in one chosen cluster.
                Downside is I may need time to fine tune the clustering part, like how many tags I should keep and
                how many clusters I should have
                Upside is I may have a cleaner dataset.

2017/03/11
    0.
    Now I remembered why I did not log onto pixiv and download more sketches--- I was afraid that by doing so I'll
    kick myself out in the download program. I figured out how to log in with another account... Downloaded 10 or so
    random sketches with different style. I noticed that 95% of the sketches are for human figures. The rest 5% are for
    scenes and other stuff. That means I should probably keep using the face classifier.
    Plan: convert them to the input format of pix2pix.
    It kind of worries me by now... The real sketches does not look anything like the ones in the training data.
    Yeah it confirmed my doubt... The result with real sketch input looked aweful. I should probably change the
    way I'm generating or treating sketches. I should analyze the histogram of the sketches online...
    Yeah the "real" sketches usually have almost all black or white pixels. There's a little overflow due to conversion
    to jpg, but 99.9% of the pixels have value larger than 245. In comparison, the sketches generated by the sketch
    generator network was unstable. It generate vastly different sketches depending on the size of the input image. It
    also have a huge amount of pixels lying in between 0 and 255. This is unacceptable... Is there any way that I
    can generate clean sketches?
    I can of course just binarize the sketch, but that leaves a large amount of artifacts. This is my next major task...

    I'm trying some traditional approach, but the result wasn't very good. People have tried this a lot of times in the
    past using anime images. The results were not good. I am thinking if I can have a dataset where I have the sketch
    and the colored image. Then I can just train a neural network to do the job....

    There was a paper on cleaning up rough sketch (interestingly, the author who studied at waseda also published a
    paper on automatically coloring black and white images...

    * Tags on pixiv for sketches: 塗ってもいいのよ and 塗り絵. I need to clean up those of course by making sure they're
    indeed bw and converting them to gray scale.

    線画付き might have something good but it's not so popular. 塗ってみた is a good one, and also 塗らせていただきました
    Let me download those two tags. I also need to download the senga version though. It should be referenced in the
    description section.
    Test using this image_id; 61691859
    So my plan:
            Download by the two tags: 塗ってみた and 塗らせていただきました
        For each image downloaded, create a key-value pair where the key is the image_id of the colored image and
        the value is the image-id of the sketch mentioned in the description section. If the description section
        has more than 1 description, I should debug that situation, but if the description section does not
        contain any url, then ignore that image. Then I should download the sketch mentioned as well by its image id.
        Additionally, before we begin analyzing the image url, if the image is already in the database, then
        get it's corresponding txt, and do the same thing without downloading the image again.

        At the end, save the images as usual but also write to a file containing a list of the key-value pairs.
        I have 24 hours to finish this, because after 24 hours the hard drive for the current pixiv downloading should
        be full.

        Overall pipeline:
            For all images in the two tags:
                If image in database:
                    if the image is really there:
                        pass
                    else:
                        treat it as if it's not in the database.

                # Now the image is really in the database

                check the txt file for the image caption (if from database) or directly get a list of urls.
                    Analyze the image caption and get a list of the urls in the image caption.
                If no url:
                    continue
                if one url:
                    Get image id in the url. If no image id, continue
                    Download the sketch image if necessary.
                    Write to file a the (colored_image_id, colored_image_path, sketch_image_id, sketch_image_path) pair.
                if multiple url:
                    DEBUG this situation and see what's going on. Or I can ignore it.


            Done. Output the number of pairs I have.

        But again I'm a little bit worried. What if people with those tags draw differently from the popular tag ones?
        They look quite different after all. Can the network be general enough to work for the other one?...
        Well it'll be at least better than not having it I guess. The dataset is worth gathering. Also the dataset
        might be good for learning the lighting and stuff.

        Done... Faster than I thought.
        Found a bug in my previous code that was referencing a temporary variable... Not too bad but more of this could
        happen potentially and I'll never know....

        Ah I probably shouldn't have coded the part where it bypasses the in-database check.  Now I cannot restart my
        model because it will mean to revisit all the urls again. I can delete that part probably. TODO:delete that part.

        I should change the preprocessing code so that it can deal with transparent background and turn them white.
        I should also change it so that it can accept another sketch instead of generating its own sketch.. Done.



    1.
    There's no need to check the result for 512x512 any further, for one glance at the generator loss told me that
    the training failed. the l2 loss did decrease overtime but the output is as if it did not have WGAN at all. The
    generator loss was decreasing fine until 6000 iterations where it jumped back to where it had started at.
    I think this is due to the lr being too large. I'll try again with a smaller lr.
    Hum... it doesn;t seem more stable with a smaller lr, which is really strange.... But the color so far looks good.
    No after 12000 iterations it became bw again... I think it might be that l2 loss's weight was too strong?
    Another thing i noticed was that even after I've changed the lr, the learning pace still was pretty much the same..
    This is also a little bit wierd.
    After looking back on the 128x128 training result, I realized that it was like that in 128x128 as well.... so I
    shouldn't have blamed 512x512 settings.... I should try with a lower l2 weight. Changed the l2 weight from default
    100 to 20... The purpose of that was to encourage the wgan to produce more color when hint is not present. If that
    doesn't do the job, then I can lower the probability of hint and see what happens. Or I can eliminate hint all
    together and see what happens. Yeah if this doesn't work then I'll turn hint off and let it train for a long time.
    If it cannot overfit the training set without the hint, then something's wrong. If it can fit the training set
    but cannot generalize to test set, then I should try a different technique maybe... I don't think I can blame the
    dataset on that depending on the actual output then.

    With l1 weight = 20, the discriminator failed to distinguish between real and fake images very well, although they
    are clearly different to a human. Wierdly the generator loss never decreased.... I guess I can try sketch loss = 10
    at the same time... One of the motivation is that the sketch is leaky--- that is it also give some hints about
    the color. lol. Shit..... Shit I just realized that when I feed the output and the target into the sketch
    generator, I did not convert them to rgb before doing so.... shit. the sketch generator will probably behave funny
    but it's looking fine so far... Oh fuck it let me just try to increase the sketch loss and see what happens...
    Probably it won't be so different... I just realized that I can't train two models at the same time due to gpu limit
    Plan: either use a different sketch loss or fix this.

    A side note: I found a pattern that the more color the model has, the less realistic the colors
    look... I imagine this is probably due to overfitting... If you overfit more, you'll have more colors but they may
    not be accurate at all on the test set.


    2.
    pixiv images downloading will be done by today. After that creating a snapshot will take a while. And copying it
    over will take a while. Not to mention all the preprocessing. I expect 2 days.
    3.
    I can first try and see how fast the face classifier is-- while I create a database and cluster them.
    But now the other database is under way, I think it might be better to train a good image2sketch network first.
    I have two options. One is pix2pix, and the other is dtn. I think pix2pix probably works better... I haven't seen
    dtn work on larger ones and even if it does, first of all the output is blurry. second it is not using the
    one-to-one or one-to-many relationship I have already. It is probably more suitable for dataset where there is no
    such relation. Probably I can get such a dataset from my database by taking all the ones with "sketch" tags and
    separate them from the ones without.

    Papers:
        facebook released pretrained fastText vectors. I'm curious how they did non-latin based ones...
        https://github.com/facebookresearch/fastText/blob/master/pretrained-vectors.md?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=revue
        But anyway it may be useful for my future projects.

    Tag prediction
        As I think of it, it is probably better to use a structure that can be used as a discriminator in pix2pix?
        I can take the intermediate layer, add some conv layer at the end, and train that as a discriminator?
        I can try vgg first I guess.

    TODO: spend a little time on my master thesis.
        1. check the precision recall and compare with the previous ones.
        2. Check the database and check if there's any


2017/03/12
    Setting l1 weight from 100 to 20 did not seem to do the job. It probably made matter even worse because now the
    discriminator won't work properly. Setting sketch loss to 10 did not seem to make matter better either. The
    training curve looked exactly the same as the one without change, but the difference is where the sketch loss = 1,
    the generator loss actually gets better but here the generator loss stays stubbornly at the same level.
    Regarding the pixiv download, I have 13000 pairs of sketch/colored images. I am creating a snapshot and preparing
    to copy that over.
    In the mean time. I don't have much to do so let me summarize what I've done...

    neural style
    I just realized how to solve a bug... TODO: I can have a "no style" default weight and every time I change the
    style weight, I normalize that along with the default weight so that the gradient can still propagate through when
    all weights are 0. Well... too lazy to make that change now.

    After I got the dataset copied over, here are the steps that I'll do:
        1. Create a dataset of human colored images versus sketches.
            Currently I'm allowing resized image within a certain height/width ratio error range, but that might
            cause some issues due to imperfect alignment between the colored version and its sketch.
            Also there might be some issues with transparent png. I don't know if all pngs will have white in the first
            three channels for the transparent pixels.
        2. Train the pix2pix to convert colored images to sketches on that dataset without changing any settings or
        enforcing that all pixels in the sketch should be either black or white. If that does not work, enforce
        the constraint.
        3. In the mean time, create a database of all images, and compute their clusters.
            - Takes 2 hours to create the database, who know how long to create the clusters... I just hope the memory
            can handle the task.
        4. If all works well, do preprocessing on those clusters without face detection.
        5.

    I changed the size of the data disk and I also started downloading the rest of the pixiv images. It should take
    another good 4 days.

2017/03/13
    Preprocessing of the colored and sketch images is done. At the end I only have 2933 out of the 13532 images that I
    collected... But why do I end up having 17834 images in the output directory...
    I can't start training the image to sketch dataset yet. Here's a list of potential problems...
        1. Inversed sketch image so the sketch is white.
        2. incorrect sketch image and/or corresponding colored image
        3. Maybe padding is a little bit better than cropping
        4. Sketch is not totally either 0 or 255 (if the author say scanned their sketch)
        5. Sketch may have a little bit more than sketch -- that is the sketch might be colored a little or the sketch
        might have shade indicators etc.
        Plan: I'll need to download the dataset, write a program to manually go through all of them.
        Went through like close to 1/2 of them.
    Clustering 20 clusters was probably too little. It blended a bunch of styles together (most of them gay stuff
    surprisingly, it was more than I've thought).
    Clustering them into 100 was ok. I'll use Cluster 11(49831 images), or Cluster 88 (990 images), or Cluster 39(8214 images)

    My priority is generating good sketch first. Without that I cannot do anything... And to do that, I need to clean
    up the database.
    Oh fuck the sketches....
    There must be some smart way to clean up those sketc,whes............
    running this anyway,..... It'll take a while
    python preprocess.py --input_dir=/mnt/pixiv_drive/home/ubuntu/PycharmProjects/PixivUtil2/pixiv_downloaded/ --output_dir=/mnt/data_drive/home/ubuntu/pixiv_new_128/ --size=128 --resize_mode=reshape

    I should write a faster preprocessing program that uses the tensorflow data loading pipeline. That should make
    it a lot faster.Now it's estimated to take 100 hours, which is unacceptable. ... I stared at it for a while, and
    there are just too many points that cannot be incorporated into the tensorflow pipeline. All those if clauses..

    Wow... wait the output of the current framework used on the data clustered (not really working extremely well maybe)
    was way better than I thought. It trained for 10 epochs, and the result on the training set seems pretty good...
    I'll probably run the program on the previous pixiv dataset and see what happens...
    Nah I don't see any reason why I should... It won't work on the sketches anyway... The system is too delicate in
    some way...

2017/03/14
    While my network is slowing down.... I have to do something else. I can try to test without sketch loss and compare
    that with the one with sketch loss (using the old dataset). I can also try to add my own sketch loss function
    instead of relying on a whole neural network to do the job.

    I think the answer by li rui on zhihu is right... sketch does not contain enough information to get good colored
    result. It is better to go from sketch to sketch with shade information, then from that to colored version.
    Also he mentioned that because the sketch in the training dataset is probably not "perfect", the extra shading
    information is helpful and the network accidentally can do a better job using that extra info. This is interesting...
    So maybe I don't need perfect sketches after all. Dunno.

    Finally the dataset is complete... I've gone through everything twice so it should be good enough. One thing I'm a
    little worried about was that the colored version of the sketch usually also adds lots of things to it, like
    shading and background. The sketches are also not so uniform in their usage of shade. It is not even uniform in the
    color they use for the sketch... I'll be pretty happy if the pix2pix can learn anything.

    Without sketch loss the network was not so good at finding the boundary of coloring, (expectedly, because that's
    what the sketch loss is there for).
    Surprisingly after couple hours the effect of sketch loss was not so obvious. The problem that without hint the
    color looks super bland is still there, not surprisingly. Still don't know how to solve that.
    Anything I can do?... I know it is possible to go from bw to colored... But from sketch to bw... That is also
    possible... I thought I had this idea before but it failed? I can test with preprocessing images and turning the
    colored version to bw version, and see if the current network can color bw images... Maybe not... If not, then we
    have a problem...

    I'll train the colored to sketch network. This is the priority.
        Command used: python pix2pix_w_hint_lab_wgan_larger.py --mode train --output_dir sketch_colored_pair_cleaned_128_w_hint_lab_wgan_larger_train_sketch --max_epochs 20 --input_dir /mnt/data_drive/home/ubuntu/sketch_colored_pair_128_combined_cleaned/sketch_colored_pair_128_combined/train/ --which_direction BtoA --display_freq=1000 --gray_input_a --batch_size 4 --lr 0.0008 --gpu_percentage 0.45 --scale_size=143 --crop_size=128 --lab_colorization --train_sketch
    Then I should preprocess the colored image into bw and combine that with colored to form a bw->colored dataset.

    The result of colored image to sketch was better than I thought. The network seems to learn which is the more
    important part of the graph pretty well... I as a human probably can't distinguish between the two. I'm more than
    happy about this result... I should probably generate the sketch based on this network now. I could do so in fact..
    Training from sketch to colored on the 6000 hand picked images was not going well without hint and sketch loss,
    which is not surprising I guess with such little training data, but not so little that it can be easily over-fitted.

    I'm generating the new sketches.... but the problem is they're in the same folder, which means I probably have to
    modify the combining program a little...

    Generating sketches of the old dataset using the new algo should finish by tomorrow morning... In the mean time
    I should probably try training without hint. I've shown before than without wgan the output is gray as hell, so
    now I should see what happens when there's no hint.

    Plan: check the no hint result
    Plan: run the combinator for bw and color. Done.
    Plan: run the combinator for new sketch and color.

2017/03/15
    Without hint, the network learned the color of a face pretty well, since they usually have the same color I guess.
    But it did not learn other things... So both hint and wgan is crucial to having more vibrant color.
    Trying (again) to do bw to colored image while I wait for the combinator.

    Reading papers while I'm waiting for it to train. It seems that the task is way harder than the dataset generated
    by the previous sketch generator due to a lack of shade info. But if it works at all....
    Oh I forgot the way I'm training bw to color is wrong... I should only make it learn how to generate the ab layer
    while taking the l layer from the input... Plan: make that change... (The discriminator should still take lab)
    Plan: check result for bw to color and for new sketch to color. Done. The network learned a little bit of
    color and how to use the hint a little after 2 epochs, but it's not so satisfactory. I'll check the output_ab
    result where the generator only outputs two channels. The result was uh bw mostly... It's even worse than the
    one with new sketch as input. the loss kept decreasing but the generator loss was increasing all the time...
    I guess l1 loss takes the blame... Hum after checking a few more times, I think it does have color half of the time
    but the other half it's just bw. Maybe it's the hint? I probably should also write the hint into the summary...
    The bw to color network is good at skin color but was not good at other things, like the sketch2color... So I think
    they share the same problem... I'm looking into the color bw papers again to get some inspiration...

    THe new sketch to colored result was better than I expected... I thought it would fail to learn anything but it
    succeeded in one output without hint, so that's good I guess.
    After 5 epochs of training, the only color it learned is to color all hairs to blonde and also face color... With
    hint it can do a little bit better but it's not going well. I've seen the same problem before, when my dataset was
    not as good.


    The paper on color distribution was interesting but maybe not so applicable to the current work. Maybe if I can
    extract the color pattern and supply that as a hint? But I haven't even solved the light/shade problem yet.
    "Do We Really Need to Collect Millions of Faces for Effective Face Recognition" was a good paper, but I guess it
    failed to compare itself with other models using more data... Didn't read too deeply into it but it seems to use a
    smarter technique to increase number of training images by manipulation on the 3d manifold.
    Also reading into "Controllable Text Generation" and "Scribbler: Controlling Deep Image Synthesis with Sketch and Color"
    The "Scribbler" paper is very related to the work I'm doing. It has a huge chunk of commonality with sketch
    coloring. It also faces the problem of having not enough corresponding pairs of sketch and colored image. Notably,
    it used not only one but five different ways to generate sketch to incorporate the complexity in real world
    sketches. Maybe I can do so too! Why haven't I thought of it before... I always think that there is one type of
    sketch that is "more real" and closer to what people will draw... but it never occurred to me that it may not be
    the case, even though I've seen the variety with my own eyes when I label the sketch-colored dataset... There are
    mainly three types of sketches: the one with only lines, the one with lines and a little bit black area
    indicating color and/or shadows, and the one where the input is a bw image...
    Plan: try combining the two sketch datasets and see what happens.
    Anyway the paper did not have too much of a difference from my current implementation, except they don't have
    sketch loss and instead have tv loss, and it uses dcgan and I use wgan. Oh and it also uses "feature loss", which
    is just the difference between the relu2-2 layer of the vgg19 network with target versus output as input.


    Another task that I should get my hands on is label prediction... I don't have the data yet but I should start
    working on the pipeline. Plan: work on the label prediction pipeline...
    Done. The lr required for it to work is tiny, and the batch size huge... The training was slow but it's at least
    working a little by little on the sanity check set.
    TODO: maybe swap the sketch loss network to the new one??? But it doesn't seem to affect things too much.

    Plan: check the result of training on the joint dataset.
    TODO: don't just do random experiments... Summarize what I've tried, what works and what not, what I've seen in the
    papers, and decide next what to do. And don't do this when you're sleepy.

    Plan: move the data to us region after copying is over.

2017/03/16
    Combining two sketch datasets was mostly a failure. The two dataset does not seem compatible to each other. The
    program learned to color the old sketches pretty well but failed on the new ones completely. I can't even see a
    a boundary for the new one. I can see that it vaguely have an identity function in place but other than that and
    the occasional coloring of the face, I can't see anything else working with the new sketch when I train both at
    the same time.
    This sort of proved my previous thought, that with the new sketch the task became much much harder and the network
    was unable to do as good of a job as when I train it on the old dataset. Therefore this time when I combine the two
    the network chose the easier task and fitted the old sketch...
    The bw output doesn't seem to be any better than the sketch. In fact it seems worse, only have learned the color of
    the skin....
    I think I'm close but there's just something I haven't tried before that might just work. I know that the WGAN loss
    as well as adding the hint can result in more colorful images. I also know that sketch loss is helpful to make the
    color more confined within its region, but it does not generate color by its own really. I haven't observed that
    yet.
    Now that I've seen no matter how hard I manipulated the training data, it does not seem to help too much.. The best
    I can do is actually using the old sketch, but that does not give me good results when I apply them to actual
    sketches online. That's why I was working so hard to get a good sketch generator, and now I have it. Although it
    is not generating the right color on real sketches yet, at least it's not messing up the whole thing, like what
    the network did when I train it with old sketches.
    I think maybe if I increase the weight of WGAN loss, I can get more colorful images (at the cost of having less
    accurate coloring). Of course if I have the tag classifier working that might also help, as demonstrated by the
    "Scribbler" paper as well as the "Learning Representations for Automatic Colorization" paper.
    I'm running the previous pipeline with 10 times more gan weight. If it's messing up things, I'll try to run that
    by loading a checkpoint trained with normal gan weight. If it still messes things up that time, then I shouldn't
    have such a high gan loss i guess, or my wgan setting is not working (very unlikely)

    I thought of one thing... If an object have multiple possible colors, then why not generate multiple possible
    outputs, calculate the loss for each of them, and take the max as the actual loss? Say output 3 possible answers,
    and whichever that looks the most real will be the output... I can also enforce a distance function so that the
    possible answers do no lie too close to each other. (maybe as a e^(-distance) format). Oh but then I can choose the
    first possible color for the first object and the second possible color for the second object. That means the
    possible answers must be chosen pixel-wise? I guess it doesn't have to because there are correlation between the
    color of one object with the color of another... I can experiment with that if I have time... Hopefully this can
    solve the problem of l1 loss forcing everything to look colorless. TODO: experiment with this?
    When I tried this, I observed that there is essentially only one of the three possible outputs being trained.
    This is understandable... But I want all three of them to be trained but all three of them should be diversed
    enough from each other on some colors ONLY when they need to ...
    # TODO: record the number of times each output is chosen. ALso the output chosen should be independent of batch. that is each batch can have different outputs... I don't thint the current version works like that.
    Dunno. I should probably try training on the new dataset instead for now./



    Plan: write code to take the cleaned sketches list and get the 512x512 version of that.

    The sketch is a mess? what happened. Hum.... It seems like the sketch generator network is messed up.... I'm
    switching to the simpler tf dilation sketch generator. It's much faster and the result is almost the same...
    After the change the results look normal again. Can't believe I trained on the shitty sketch generator but
    never realized it...

    By the way I'm getting a wierd bug in training the classifier network. It says the string buffer cannot hold
    my data basically. I googled it and found only one post. I can't fix it yet, so I can only fit 20000 out of 400000
    items.
    The classifier network stared to work for a little during the first epoch but then the accuracy suddenly decreased
    after that for some reason... The loss stayed pretty much the same. This is quite strange... I'll start with a
    smaller sanity check dataset.
    It never occurred to me that I never read the vgg paper very carefully before I train such a classifier network...
    I should. I don't know how to train such a network when there're so many categories. The network now is really
    unstable even with a tiny learning rate. It tries to label everything as positive even though that increases the
    loss dramatically.
    I read the vgg paper. One thing to notice is that their number of classes is also 1000 and their dataset size
    is similar in size to our dataset (1.3 M for training, 50k for validation, and 100k for testing)
    Using vgg network didn't get things better... I think the lr may be too large, but it's basically the same problem
    as before: the network seemed to have no choice but to put the same label on every input, the most probable ones.
    TODO: Let me try gradual training if I have time.


    The training with 10x wgan weight made the output look more colorful, but not necessarily better. The discriminator
    was failing to distinguish between the real and the fake one... So I have two options, either increase the number
    of features in the discriminator, or to increase the number of times to train the discriminator. I guess I can
    start with the easier-to-implement one-- double the number of features.
    Even after this change the generator still won after 1 epoch... So maybe I should train the discriminator
    intensively every x rounds.. I did that. strangely this does not seem to have an effect??? The discriminator
    is still slowly crushed by the generator... What's going on? I guess it did look a little bit more colorful but
    the color themselves did not make sense. I can keep on training that if I have time. TODO: keep on training if I have time.

    New dataset that passed hw test, bw test, and face test: /mnt/data_drive/home/ubuntu/pixiv_new_128_w_face_reshape
    There's around 150000 images in that dataset (like 8% pass rate from the 2200000 downloaded images).
    generating the sketches for that... Training on the dataset. So far it looks good. Hope it would be better than
    the previous dataset. If more data does not make things better, then we have a problem...

2017/03/17
    Trained with more data, but nah it still is stuck at coloring only faces... I remembered taizan said the hint and
    the gan have conflict.... I'm wondering if I turn the hint off (but leave the channels on), would it get better. Or
    if I use two discriminator networks... It's the same idea. So I am trying to continue training from the previous
    checkpoint but changed the hint probability to 0. I did so because I think the discirminator might can't handle
    two types of inputs: one with hint and one without hint... This sounds stupid but I don't know if it can happen.
    I think it did... The discriminator loss lowered and the generator loss rose after I changed the hint probability.

    When I'm waiting, I also changed the tri code so that it no longer always choose the best output. It now have a
    chance to choose a random one instead.. I'm doing sanity check on a dataset where there are 15 different outputs
    for the same sketch. If it passes that test, then I'm more hopeful that it will work on the larger dataset. It'll
    also be interesting to see how the original code work on that sanity check dataset. Nah it's not working. I think
    it may be that the three outputs are not using the variables the same way, so the other two just looks aweful even
    though they're trained the same number of times.

    Color... Finally color... (At least for the ones in the training dataset, I tried the test set but it's
    giving me wierd outputs for 90% of them and 10% was good. I guess that was because the checkpoint did not save the
    latest one yet.) And the color looked real!... Most of them. So the key to color is not just hint nor GAN, it's
    a combination of them, but just the combination isn't enough. It has to be GAN's discriminator first trained on
    the outputs with hint, learns to discriminate color nuances really well, then trained on without hint... If
    start out without hint, then it will focus on other things instead of color, but because the network can do really
    well with hint, the discriminator was forced to focus on color differences...
    I was thinking what happens if this fails... Well I have other plans ahead, but I did not expect this to succeed.
    I hope it is really succeeding though...lol. I'm probably wrong and there's probably some bug or just some
    other stuff that prevents it to be directly used on real sketches. lol. Who knows.

    I need to repeat what I've done... Exact repeat.
    The first one trained for 1 night (8 hours and 114 iterations on batch size 4 lr 0.0008).
    The second one trained for 2h (30k iter on batch size 4 lr 0.0008)
    The third one  trained for almost 3 h now ( on batch size 16 lr 0.0008, gan weight 5)
    python pix2pix_w_hint_lab_wgan_larger.py --mode train --output_dir pixiv_new_128_w_hint_lab_wgan_larger --max_epochs 20 --input_dir /mnt/data_drive/home/ubuntu/pixiv_new_128_w_face_reshape_combined/train --which_direction AtoB --display_freq=1000 --gray_input_a --batch_size 4 --lr 0.0008 --gpu_percentage 0.25 --scale_size=143 --crop_size=128 --use_sketch_loss --pretrained_sketch_net_path pixiv_full_128_to_sketch_train --use_hint --lab_colorization
    python pix2pix_w_hint_lab_wgan_larger.py --mode train --output_dir pixiv_new_128_w_hint_lab_wgan_larger_cont_no_hint --max_epochs 20 --input_dir /mnt/data_drive/home/ubuntu/pixiv_new_128_w_face_reshape_combined/train --which_direction AtoB --display_freq=1000 --gray_input_a --batch_size 4 --lr 0.0008 --gpu_percentage 0.25 --scale_size=143 --crop_size=128 --use_sketch_loss --pretrained_sketch_net_path pixiv_full_128_to_sketch_train --use_hint --lab_colorization --checkpoint=pixiv_new_128_w_hint_lab_wgan_larger --hint_prob=-1
    python pix2pix_w_hint_lab_wgan_larger.py --mode train --output_dir pixiv_new_128_w_hint_lab_wgan_larger_cont_no_hint_gan_5 --max_epochs 20 --input_dir /mnt/data_drive/home/ubuntu/pixiv_new_128_w_face_reshape_combined/train --which_direction AtoB --display_freq=1000 --gray_input_a --batch_size 16 --lr 0.0008 --gpu_percentage 0.75 --scale_size=143 --crop_size=128 --use_sketch_loss --pretrained_sketch_net_path pixiv_full_128_to_sketch_train --use_hint --lab_colorization --checkpoint=pixiv_new_128_w_hint_lab_wgan_larger_cont_no_hint --hint_prob=-1 --gan_weight=5.0


    It is kind of wierd why it still formed those glitches on the test set... It should've been saved a long time ago.
    Maybe it is overfitting the training set? It can't be... It seems like it has something to do with the batch size...
    uh.
    It is still far from perfect, but I think after training for a few more epochs, it should generate good enough
    outputs... It's still not working on the real sketches, which worries me a little bit...
    Also the glitches... It's wierd. It appears either for all images in the same batch or do not appear at all.

    There seems to be a bug in recording the images. Got it fixed. It was incorrect alignment when I was copying code.

    # TODO: train the thing on all images (that passes bw test and hw test), instead of on the images with faces.
        This one I'll have to wait until the preprocessing on the 512x512 finishes, then I turn them into 128x128...
    One other thing also worries me. It seems like the improvement happened over a very short period of time - in 20
    minutes the loss changed dramatically. generator loss went down, discriminator loss went up, and l1 loss went up.
    What if this process didn't happen for other datasets? Or what happens if the network stopped improving and the
    coloring stayed at the current level? There's much to do and too early to celebrate, but it is getting better,
    I hope. I confirmed by doing the testing again after 2 hours. It is indeed better than the previous one.

    TODO: I can read some more papers...


2017/03/18
    Combining the old dataset with 400k images should be done in 2h.
    Continue training for a night did not yield too much of an improvement. There were some issues with leaking colors.
    I think it is caused by the sketch generator. The sketch generator in place now, the naive one, does not ignore the
    background like the nn sketch generator does. Therefore using l1 loss on the naive sketch generator does not make
    sense, since the algorithm can never have guessed what the background can be, since the information is just not
    there if I use the new sketch as input. One option is to feed it through the trained nn and take the output.
    The disadvantage of that is it greately decreases the training speed. I'll make the change once I made sure that
    using the new sketch to train will yield a better result in general than the old sketch. Previously I saw that
    training on the old sketch is not general enough for the network to automatically extend it to human-drawn sketches.

    I'll try to train the thing on the old pixiv_downloaded_sketches_lnet_128_combined dataset, which uses the old
    sketches as input. I think I still have the pretrained version of that, so I'll just try to take away hint and add
    the gan weight. This will test my hypothesis about why the latest training method sort of worked.

    As to how to improve the model and make it be able to generate different styles, I think I need to use the tags,
    separate out the cheerful ones from the sad ones, the bright ones from the dark ones, and the manga ones from the
    colorful ones... Basically if I have the training data (maybe even provided by a classifier if possible), then I
    should be able to just modify the batch normalization variables to get different styles, just like the
    fast style transfer thing. Well. That is if everything works... lol I still don't think it will based on past
    experiences.

    Yep it works. The method works. The sudden drop in discriminator loss was observed after half an hour of training
    and lasted for 30 minutes. Then it flats out at around -0.05, just like using the other sketch. What's different is
    the generator loss decreased after 30min but then slowly increased again. It shouldn't matter as long as the
    discriminator loss is the same. The difference matters more than the absolute value. The result was still bad on
    real sketches, but the result on the old sketches test set was way better than the previous one. There was much
    more color than the previous version and they look to some degree real.

    One question came to me... Do I really understand the difference between different layer orders? conv batch relu
    vs. conv relu batch for example? huh... After reading the batch norm paper again, it mentioned that the bias b
    is no longer needed after adding batch norm because batch norm's shift essentially can cover the job for the bias.
    That was probably why I did not have the bias in my neural style program for a long time. It should be added before
    the non-linearlity. The reason for that is:
    We could have also normalized the layer inputs u, but since u is likely
    the output of another nonlinearity, the shape of its distri-
    bution is likely to change during training, and constraining
    its first and second moments would not eliminate the co-
    variate shift. In contrast, W u + b is more likely to have
    a symmetric, non-sparse distribution, that is “more Gaus-
    sian” (Hyvärinen & Oja, 2000); normalizing it is likely to
    produce activations with a stable distribution.
    It's good to review this from time to time.


    Before getting on plane tomorrow:
    # Check tar status. It seems I have no choice but to use AWS Import/Export... School network is slow, and exporting
    data through the internet costs money. I tried compressing the thing, but since a majority of them is in jpg, which
    is already a compressed format, even using 7z only yielded a compression rate of around 5%. So it's not worth the
    time.
    https://aws.amazon.com/snowball/disk/
    # Train sketch generation
    python pix2pix_w_hint_lab_wgan_larger.py --mode train --output_dir sketch_colored_pair_cleaned_512_lab_wgan_larger_train_sketch --max_epochs 50 --input_dir /mnt/data_drive/home/ubuntu/sketch_colored_pair_cleaned_512_combined/train/ --which_direction BtoA --display_freq=1000 --gray_input_a --batch_size 4 --lr 0.0008 --gpu_percentage 0.55 --scale_size=572 --crop_size=512 --lab_colorization --train_sketch
    # Keep training on 400k dataset with new sketch loss weight 10

    If I got bored on the plane, maybe I can work on the master thesis... I have the data.

    Shit... The sketch generator was broken again. It must have something to do with loading variables and probably
    those variables are initialized again after being loaded... So my training using a nn sketch generator all failed
    for that reason.

2017/03/19
    Maybe on the plane I can write a faster preprocessing program now that I've found the decode_image in tensorflow1.0
    I can also work on the classifier. I don't understand why the current configuration does not work. I don't know
    how long it takes to train usually, but training overnight and still getting the kind of result where it labels
    everything as the most possible output is unacceptable.
    I found the inception net tutorial on tensorflow. It was awesome. It's probablye one of the best tutorials one can
    find on training image recoginition systems. It talks about how to build the training set, how to setup the network
    to do training from scratch or to just do training on the last layer. One thing I'm worried about that was, what am
    I going to do with the network afterwards... So yeah I can predict the tags if I'm successful, but so what. My
    ideal situation would be that I can separate all those different styles. Then with that I can train different
    coloring networks to get different effects. But is that information already in the tags? I should find out about
    that. (Because if it is not in the tag, then the inception net probably can't help very much, and if it is in the
    tag, then what do we need the net for?) No the tags don't usually contain info about the style of the image. I have
    to use some more conventional image analysis tools I guess. Maybe I can analyze the color choice and separate them
    based on that? Think about how audio styles gets identified. There should be some similar trick...
    Nah, there seem to be less well engineered features. I can perhaps use the vgg net or inception net, extract the
    feature layers, compute the co-relation matrix of the feature layer (just like neural style) and maybe cluster on
    that matrix (after the matrix is normalized with respect the the size of the image). But how to cluster a
    say 64x64 matrix? I can flat it out (because it does not resemble the spacial positionining or the size of the
    image, I could do that) and do the same thing like the tags?
    Sounds like a plan... Tried it ... It made sense to a very small degree but it can just be random... Will it
    work better on a larger scale? Dunno.
    I found that the problem lies in my clustering method. PCA is not a clustering method... Plan: find another clustering method. M
    aybe do pca to reduce dimensionality. sklearn.cluster.KMeans support sparse matrix though.

    Anyway let me put the url to the inception net here: https://research.googleblog.com/2016/03/train-your-own-image-classifier-with.html

    Surprisingly, using the correct sketch loss did not change things too much. Nor did the sketch weight change from
    10 to 1. From the loss graph they all looked the same.

    I trained the network for the whole day. The result didn't look much better than the small dataset. In fact I think
    it actually looked worse during the first stage due to lots of averaging... I'm testing with 1% probability of
    having hint so that the hint can still work, but the gan should work as well. I changed gan weight to 3 so that the
    color can look a little bit more realistic at the cost of being a little bit bland.
    Also I'm worried to see that it's still not working on the real sketches. Maybe I should have a bit more variation
    in the sketches I use to train? But I tried to combine two types of sketch before, which was just a failure...



    Maybe I can add a random shift in the image. Maybe I can apply a random kx function (mapping 0~1 to 0~k then
    add a threshold at 1) for all the inputs... That should let the network be more flexible in terms of the sketch it
    accepts as input.
    Maybe I can also filter the images with manga tags.
    plan: fix sketch generator in sketch loss

2017/03/20
    The network behaved pretty different on the new setting where I changed the hint prob from 0 to 0.05 and the gan
    weight from 5 to 3. The discrim network never just went close to 0. The sudden increase in discrim loss is mainly
    caused by the hint probability going from 0.5 in the previous training to 0.05. It's not caused by the generator
    network doing a great job at crushing the discriminator...
    The output on real sketches still failed. The output on old sketches didn't work so well. The output on new
    sketches worked a little bit better than on the old one but not as good as the previous version, where I trained
    the network pixiv_downloaded_128_w_hint_lab_wgan_larger_cont_no_hint_gan_5 (this is a little bit better than
    pixiv_new_128_w_hint_lab_wgan_larger_cont_no_hint_gan_5, but it trains on the old sketches, not on the new ones.).
    I think I'll try two things: train on the large dataset using the previous setting, and training using random shift
    in the input sketches.


    For the experiment using the old setting, the jump was observed only after 6 minutes and lasted for less than 3
    minutes. After 3 hours of training, I think the previous result still wins, that is, the result I got
    from training on a small dataset with face detection. The current one also have face detection, but it's just not
    working as well. The color is a little bit too much... Tricky. Yeah got more color but the color looked wierd.
    For the random shift, ... I can't really tell until the whole thing finished training after like 1-2 days.
    I tried to test it on the sketch_combined dataset. The result looked different from all other ones I've seen so far.
    I can't really make any judgement at this point.

    Fixed the sketch generator. That was because the supervisor auto-initializes everything when it gets called...
    Fixed the clustering method... ah pca cannot do clustering... I still found it surprising. I'm using kmeans now.
    After thinking about it for awhile it made sense why pca can't do clustering. Just imagine 10 clusters on 2d plane.
    PCA can only output 2 features since it's 2d. Even if there's only 2 clusters, imagine pca's output. It won't be
    the two clusters at all! It would just be the two axis that has the most variance... Anyway I was dumb. I'm looking
    forward to see the result from the k-means clustering. It's been running for 30 minutes now.
    The clustering worked. :) Now I have a hundred different themes that I can use, each on average containing 10k~20k
    images. I can do lots of interesting things with it...
    Note that these doesn't necessaily correspond to "style" of the image. It's more about content most of the time.

    btw, preprocessing on the 512 is done.
    Number of images preprocessed in total: 2245207. In which 1918286 passed hw ratio check,
    1164396 passed black-and-white test and 1164396 passed face test and was saved.
    I did not generate the sketches yet because I want to train the sketch generator on 512 first. Another option is
    to just use the naive generator, but then I don't need to explicitly output the sketch again. I can just generate
    that during training. I'm surprised that only half of them passed the bw test...
    Maybe I can improve the preprocessing speed? if I have time... But I don't see the point. I've done most of the
    preprocessing already...

    As for the "tri" experiment, I think the problem is caused by outputs sharing too many variables. I should maybe
    make them only share the conv weights, but not batch_norm vars. I finished the modification, but strangely all
    three of them is giving me the same input no matter what I put in... A different output is chosen each time but why
    do all of them look the same?? I tested by changing the onehot vector and the result looked different after that.
    So I guess it was natural that all of them looked mostly the same at the beginning because the scale and offset was
    not fully trained to resemble their different distribution yet. I guess the thing is ready... Maybe I can add a
    counter for the number of times each output is chosen but I'm lazy now... I'll have to create three separate
    scalars to do that...
    The tri experiment worked to some extent on the sanity check. One of the possible output has a dark background
    while the other two had bright background. All other parts looked pretty much the same. I'm trying to raise the
    probability to choose the best output over a random one.
    I still have to separate out the batches if I want to use this for training.
    It works. After raising the probability to 0.9, it can separate out three types of output. So all I have to do is
    to allow different instances in the same batch to make different choices.

    Experiments :
    1. Run the new sketch loss. (running)
    2. Run the tri (running)
    3. Run the second half of the shift. (running)
    4. (Low priority) continue training on the huge dataset and just hope it can get a little less wierd.
    5. Do something with the clustered data.
    6. Train the sketch extractor for 512x512

2017/03/21
    Thoughts last night:
    compare between resize then gen sketch and gen sketch them resize
    because the sketch combined is mostly from the latter. i think that is the reason to difference in performance.
    the unrealistic color is caused by two things. one is the dataset. If the dataset is easy then the quality will go
    up. on the other hand, the more fundamental reason is the discriminator is doing a poor job. I think i ve tried
    increasing the number of features. check that. then i can try to increase the number of layers. i can t think of
    anything else that can help the discriminator at the moment... it is wierd that the discriminator might be working
    differently for with hint and without hint.

    unless i change the architecture of the discriminator? pix2pix has a different architecture...
    dunno

    Results from running the following three experiments:
    1. Run the new sketch loss.
        Didn't seem to change too much. It's working properly I guess. Have to do the second part.

    2. Run the tri
        The three possible results were separated into basically three types of background color. This shows that first
        the method works to some extent, second the background is there part where the information is lacking the most.
        When the background is gray (vs dark or bright), the characters tend to have the most skin color...
        Have to do the second part.

    3. Run the second half of the shift.
        It was disappointing to see that adding random shifts did not improve the result on sketch combined dataset.
        It also did not work so well on the new sketches dataset. I guess adding random shift made the task harder???
        Or maybe I have to change the settings a little. I'm not sure yet why it had failed.
        Interestingly the result on old sketch was a little better than both of the outputs above. The coloring looked
        wierd but it did not have too many glitches.
        The failure in training may also be caused by me changing the batch size and the lr. ( from 16 to 4 and from
        0.0008 to 0.0002)

2017/03/22
    I checked the result for the two experiments, which ran for around 40 hours. The results are pretty good.
    The change in sketch generator improved things a little, but that could also be due to extra training. The result
    was still not yet comparable to using the old sketch, but the old sketch cannot work with the real sketches whereas
    the new sketch can sometimes work with the old one.
    The wierd glitch still appears from time to time... I haven't figured out why is that yet, because I've never seen
    them during training. But I'll look into that...
    The tri experiment was less successful. The three different modes basically was mainly working on the background,
    creating three different types of background for each image. From time to time the color of other objects changes
    as well, like the color of hair etc, but overall it just looked less natural than the other one, and there are a
    lot more glitches. One curious observation is that the glitches can appear on say only one to two out of the three
    outputs.

    I combined the tri and the sketch, running the tri with 16 outputs and see what happens. If the change is still
    limited to background, then I'm pretty disappointed.
    But yeah the sketch version worked pretty well. The only disappointments are the glitches and that it doesn't work
    so well most of the time. There are few that are particularly well, but other than that, most of it does not seem
    too impressive. TODO: I need to improve the overall quality by a little bit more...
    TODO: I also need to think about how to use the clustered data, and or how to cluster based on style, not tags.
    for style I think adding more layers might help.
    I'm hesitant to make machine predict human ratings because it just has too many factors...

2017/03/22
    Shit found a bug... I was still running output = 3... Well at least I can compare the results between tri and tri
    with new sketch loss I guess.
    So I compared among tri, tri+sketch, and sketch. Among the three, the most successful one seems to be tri+sketch in
    terms of l1 loss. The tri by itself was good as well, but it didn't train for enough rounds for some reason. Let me
    compare their actual outputs without hint.

    I'm going to do two experiments... Because now I don't know what affected the output's color. It could be either
    decreasing hint probability, or it could be increasing the gan weight, or it could be both.
    So in one experiment, I'll try to decrease the hint probability but leave everything else the same. In another
    I will try to increase the gan weight but keep the hint probability.
    I also need to run with the original setting with tri and sketch combined version, but that can be pushed till
    later.

    In the mean time, I need to work on clustering based on style, and I also need to look into chainer code. I
    was uncertain whether their discriminator has the same structure as mine...

    Ok, by comparing the result from the three different settings, I now know that it is the gan weight that made the
    output colorful. Furthermore, it seems to be actually worse when I take away the hint. After careful comparison, it
    became hard to tell which one is actually better. The one without hint seems more colorful, but sometimes less
    realistic, while the one with hint seems less colorful but sometimes more realistic. It's hard to tell which one is
    more realisitic than the other.

    I'll train the num_output = 16 remotely while I train the network with a slightly lower gan weight locally (without
    the tri)

2017/03/23
    I summarized what I've done so far. I can't believe that I spent soo much time on this project but accomplished so
    little. It is hard working on project like this, where there are just so many possibilities where things can go
    wrong.

    Plan: make a network that works on both old and new sketch (run mix, check pixiv_new_128_w_hint_lab_wgan_larger_sketch_mix)
    TODO: find the right training parameters where the color is just enough to be pleasing but not too much so that
    it looks unnatural. (pixiv_new_128_w_hint_lab_wgan_larger_sketch_w_10_cont_gan_1_l1_50)
    Plan: get rid of the manga in the training set. (Done, but not perfect, 10% are manga.)
    Plan: If I have time, test the multiple possible outputs. pixiv_new_128_w_hint_lab_wgan_larger_tri_sketch_sketch_w_10_o_16_no_hint
    TODO: check result of pixiv_new_128_w_hint_lab_wgan_larger_sketch_w_10_cont_gan_3

2017/03/24
    Both the "mix" experiment and the "l1_50" experiment were successful. In the mix experiment, although it did not
    produce too much color, it successfully worked on the sketch_combined dataset, which consists of several different
    styles of sketches. In the l1_50 experiment, the coloring looked a little bit more realistic than the gan_5 version.
    It is definitely more realistic than the gan_3 version trained for like 6 hours.
    Maybe I can increase the l1 weight by a little, like to 75 or something.
    I'll try to run the second half of the l1_50 experiment with l1 weight 75.
    The multiple possible output experiment was a failure. After 12 hours of training, it is still unable to produce
    outputs that had enough color. The difference was still mainly in the background. I think after reading the paper
    on real-time style transfer of any style, I am lacking a fundamental understanding of why instance normalization
    works, and it is not strange that the method did not work in the current setting. TODO: re-read that paper. It is
    pretty inspiring to see someone of my age publishing good papers like this.

    After reading the real-time style transfer paper I realized that I was not doing instance norm for the "tri"
    experiment. I was using batch norm. You never realize this kind of thing until you re-read the papers and examine
    your code again.
    What if we use instance norm instead of batch norm? (Not just in tri but in other programs as well) In theory the
    instance norm will learn to normalize the "style" of each input sketch, instead of normalizing all input sketches.
    And in theory this sounds like it will improve things.

    I fixed the test settings in sketch_mix file. Maybe that was causing the glitches, because I did not set the
    hint_prob to 0. I changed it so that when user specifies a user_hint, the hint prob is 1, otherwise it is 0.

    By the way the l1 weight = 75 is working pretty well for the old sketches, but not for the new ones (comparatively).
    The old one sometimes generate almost perfect results. i want to find more sketches to test with.
    It failed on real sketches... Why... Is that because I generated the sketches after resize?... I don't know...
    It's also not doing so well on the old sketches generated from real colored images

    And there were the glitches again... why. Maybe that is because the gpu ran out of memory??? Yeah that seems to be
    the cause. It occurs randomly whenever the gpu runs out of memory.

    NOTE: currently the mix condition does sketch g                 eneration BEFORE the image is resized. That was the reason why it
    did not work on the full sized version of the colored images I collected! Could that be the reason why it did not
    work on the resized real sketches? That seems like a plausible explaination. The sketches would be way too
    complicated for sketches generated from a 128x128 image.... Unless I train it with sketches generated on full sized
    images and then resized to 128x128. Ah... fuck.
    I was right. After using the old sketch algorithm on the sketches I've collected (I know it sounds
    counter-intuitive, why would anyone generate sketch based on sketches), it got a little better. And after I use the
    resized the collected sketches first (outside of the algorithm), it got even better. Now that's pretty wierd. Why
    would resizing inside the algorithm versus outside make a difference. TODO: look into this.
    That's all because I did resizing before I generate the sketches!... Or is that really the cause?


    I should try to write a program that takes unresize images and train on old sketches.
    Modified the current program (sketch_mix version) so that the flag single_input combined with train mode and
    mix_prob = 1 can take any image as input, first convert it to sketch using dilation, then train on the cropped
    version.
    I also modified the decode_image function so that now it can decode using the extension from the file name
    instead. I'm still a little bit worried about corrupted image files stopping the whole training process though.
    I don't want to add a big try clause outside, because if I do that, then I am having the risk of not discovering
    some bug that affects the training.

    I would imagine that sketch before resize would perform worse than sketch after resize, because all the training
    images are first resized, then the old sketches extraction method was applied with 0.5 prob. (But curiously, as I
    think of it, all the training data for the new sketch generator was trained on resized sketch images -- equivalent
    to first generate sketch then resize both sketch and colored. so the two sketches are generated in a different
    order, the newer one sketch first, and the older one resize first).
    And my experiment proved that. Generating the old sketch after resizing had way better results than generating
    the sketch before resizing. So, I guess in order to work with real world sketches where it is equivalent to
    first generate sketch then resize (which results in more details in the sketch generated), I have to have
    the training data configured accordingly.

2017/03/25
    Refactored the preprocessing code. It should be more efficient now. I also wrote the test cases for the bw
    detection code. I am going to take a look at the code overall to see if there's any other bugs/room for improvement.

    What happens if the current sketch (generated before resizing) does not work?
    In that case, I'll look into why exactly does resizing change things that much. Specifically why does resizing
    first to 128 then inside the program to a square different from just resizing from the original image directly to
    a square. Plan: I should look into this no matter what.

    Nah I dont think it is working on the current dataset... Maybe I can try to use the current sketch extractor on the
    full sized image? I think it will fail, but who knows.
    Plan:support new sketch with single_input option. -- I stopped training the old sketch's second half (l1=75) and
    instead started to train the new sketch with single input.  I did that because I don't think the old sketch result
    was good enough. The old sketch was way too messy. But wait then the sketch is generated after resize...
    So I wrote a program that can take arbitrary image shape, generate the sketch, and then resize it. But it turns out
    that some images are just too large to handle, so I have to resize them to like 1024 x 1024 or something before I
    can generate the sketches. So I thought it's even better to just generate a bunch of 512x512 sketches and use the
    old pipeline. That's easier. But I'm still not sure whether the sketch generator will extend to 512x512 easily.
    So far it seems like it does...

    The problem with the previous experiment was that the sketches were way too detailed for a 128x128 image. Maybe I
    should start with 512x512, resize it to 128x128, then train on that? Is the bug really caused by real sketches being
    too detailed after resizing? ... Or is it because I was training on datasets that was passed through the face
    detector, which limits the variety of the dataset?. Plan: really think about this before you proceed.

2017/03/26
    After looking back on the test results, I think actually adding "mix" where both old and new sketches are used
    might not be a good thing in terms of the quality of the result on real sketches. There was still a trade off
    between vivid color and being realistic, but I think no matter how colorful or not-colorful it is, it will
    always color things wrong when it don't know what do color. For example, if it's sure something is human skin,
    then the color will look fine no matter how vivid it is. But if it is not sure, then the non-vivid version will
    just leave the area grayish while the vivid version will color it either randomly or according to the preference
    color of the network.

    The latest result is I think the best one I've seen so far.

    I think the reason why resizing in python and resizing using "convert" command is different is because they use
    different algorithms for resizing. The convert tool results in outputs that are a little bit more blurry than the
    resizing function in the program. As to why that results in results that looked a little bit better, I don't know.
    I guess that is because the training sketches are blurry as well so the network learned to work with that kind of
    data.
    *A little bit blurry sketches give better result.
    Isn't it kind of bad that such things can yield vastly different results? Now what about the brightness/darkness of
    the sketches?.... TODO: test that

    I tried using crop instead of changing height width ratio and the result was much better. So height width ratio
    is important... The convolutional network will probably take much more effort to remember the same object with
    different height width ratio.

    TODO: run the command to turn of hint during training to see the effect of that.
    TODO: add weight_decay by switching to slim. (MAYBE with weight decay I do not need to do clipping. I've seen such
    results in some blog: http://musyoku.github.io/2017/02/06/Wasserstein-GAN/)
    I;ve found a bug (maybe) in the slim code. The conv2d_tranpose's bias's trainable argument is not set. So i
    made two generator networks, one for sketch and one for the actual output. (I was close to making another bug but
    discovered it 30 mintues after training started. I forgot to modify the discriminator as well.)

    The slim version with only weight decay did not work. The discriminator loss just kept increasing. I added back the
    clip but increased the clip value. we'll see if the clip together with weight decay will work.
    On the other hand, setting hint prob to 0 and using l1 loss = 75 did not work. It only colored skins and faces.
    I'm trying to set the l1 loss to 50 and compare that with the same l1 loss weight but with hint turned on.
    TODO: examine code. double check everything.

2017/03/27
    I'll call the current result "working" to some extent. The results are good enough, although it is having a
    hard time coloring the background. The hint is not working, but hey the result looks pretty good. I'll turn on
    the hint again and see what happens. If everything goes well, then all the better. If not, then I have to change
    the code so that it only trains the gan when there is no hint.

    Here's a record of the list of command I used to train. It took quite a few steps and I was not sure whether they
    are all necessary, but I think it should be sucessful to reproduce the same result on 512x512 data.

    /usr/bin/python pix2pix_w_hint_lab_wgan_larger_sketch_mix.py --mode train --output_dir checkpoints/pixiv_new_128_w_hint_lab_wgan_larger_sketch_mix --max_epochs 20 --input_dir /mnt/06CAF857CAF84489/datasets/mnt/data_drive/home/ubuntu/pixiv_new_128_w_face_reshape_combined/non_manga_images.txt --which_direction AtoB --display_freq=1000 --gray_input_a --batch_size 4 --lr 0.0008 --gpu_percentage 0.26 --scale_size=143 --crop_size=128 --use_sketch_loss --pretrained_sketch_net_path checkpoints/sketch_colored_pair_cleaned_128_w_hint_lab_wgan_larger_train_sketch --use_hint --lab_colorization --sketch_weight=10.0
    /usr/bin/python pix2pix_w_hint_lab_wgan_larger_sketch_mix.py --mode train --output_dir checkpoints/pixiv_new_128_w_hint_lab_wgan_larger_sketch_mix_cont_l1_75 --max_epochs 20 --input_dir /mnt/06CAF857CAF84489/datasets/mnt/data_drive/home/ubuntu/pixiv_new_128_w_face_reshape_combined/non_manga_images.txt --which_direction AtoB --display_freq=1000 --gray_input_a --batch_size 16 --lr 0.0008 --gpu_percentage 0.6 --scale_size=143 --crop_size=128 --use_sketch_loss --pretrained_sketch_net_path checkpoints/sketch_colored_pair_cleaned_128_w_hint_lab_wgan_larger_train_sketch --use_hint --lab_colorization --sketch_weight=10.0 --l1_weight=75.0 --checkpoint=checkpoints/pixiv_new_128_w_hint_lab_wgan_larger_sketch_mix
    /usr/bin/python pix2pix_w_hint_lab_wgan_larger_sketch_mix.py --mode train --output_dir checkpoints/pixiv_new_128_w_hint_lab_wgan_larger_sketch_mix_cont_l1_75_no_hint --max_epochs 20 --input_dir /mnt/06CAF857CAF84489/datasets/mnt/data_drive/home/ubuntu/pixiv_new_128_w_face_reshape_combined/non_manga_images.txt --which_direction AtoB --display_freq=1000 --gray_input_a --batch_size 16 --lr 0.0008 --gpu_percentage 0.45 --scale_size=143 --crop_size=128 --use_sketch_loss --pretrained_sketch_net_path checkpoints/sketch_colored_pair_cleaned_128_w_hint_lab_wgan_larger_train_sketch --use_hint --lab_colorization --sketch_weight=10.0 --hint_prob=-1 --l1_weight=75.0 --checkpoint=checkpoints/pixiv_new_128_w_hint_lab_wgan_larger_sketch_mix_cont_l1_75
    /usr/bin/python pix2pix_w_hint_lab_wgan_larger_sketch_mix.py --mode train --output_dir checkpoints/pixiv_new_128_w_hint_lab_wgan_larger_sketch_mix_cont_x2_l1_50_no_hint --max_epochs 20 --input_dir /mnt/06CAF857CAF84489/datasets/mnt/data_drive/home/ubuntu/pixiv_new_128_w_face_reshape_combined/non_manga_images.txt --which_direction AtoB --display_freq=1000 --gray_input_a --batch_size 16 --lr 0.0008 --gpu_percentage 0.45 --scale_size=143 --crop_size=128 --use_sketch_loss --pretrained_sketch_net_path checkpoints/sketch_colored_pair_cleaned_128_w_hint_lab_wgan_larger_train_sketch --use_hint --lab_colorization --sketch_weight=10.0 --hint_prob=-1 --l1_weight=50.0 --checkpoint=checkpoints/pixiv_new_128_w_hint_lab_wgan_larger_sketch_mix_cont_l1_75_no_hint
    /usr/bin/python pix2pix_w_hint_lab_wgan_larger_sketch_mix.py --mode train --output_dir checkpoints/pixiv_new_128_w_hint_lab_wgan_larger_sketch_mix_cont_x3_l1_40_no_hint --max_epochs 20 --input_dir /mnt/06CAF857CAF84489/datasets/mnt/data_drive/home/ubuntu/pixiv_new_128_w_face_reshape_combined/non_manga_images.txt --which_direction AtoB --display_freq=1000 --gray_input_a --batch_size 16 --lr 0.0008 --gpu_percentage 0.45 --scale_size=143 --crop_size=128 --use_sketch_loss --pretrained_sketch_net_path checkpoints/sketch_colored_pair_cleaned_128_w_hint_lab_wgan_larger_train_sketch --use_hint --lab_colorization --sketch_weight=10.0 --hint_prob=-1 --l1_weight=40.0 --checkpoint=checkpoints/pixiv_new_128_w_hint_lab_wgan_larger_sketch_mix_cont_x2_l1_50_no_hint
    /usr/bin/python pix2pix_w_hint_lab_wgan_larger_sketch_mix.py --mode train --output_dir checkpoints/pixiv_new_128_w_hint_lab_wgan_larger_sketch_mix_cont_x4_l1_40_with_hint --max_epochs 20 --input_dir /mnt/06CAF857CAF84489/datasets/mnt/data_drive/home/ubuntu/pixiv_new_128_w_face_reshape_combined/non_manga_images.txt --which_direction AtoB --display_freq=1000 --gray_input_a --batch_size 16 --lr 0.0008 --gpu_percentage 0.45 --scale_size=143 --crop_size=128 --use_sketch_loss --pretrained_sketch_net_path checkpoints/sketch_colored_pair_cleaned_128_w_hint_lab_wgan_larger_train_sketch --use_hint --lab_colorization --sketch_weight=10.0 --hint_prob=0.5 --l1_weight=40.0 --checkpoint=checkpoints/pixiv_new_128_w_hint_lab_wgan_larger_sketch_mix_cont_x3_l1_40_no_hint

    I guess now is time for me to:
    1. train on 512x512
    2. clean up the code
    3. support input of any dimension for testing
    4. Find solution for getting the background color look nicer.
    5. Think about future directions: like whether I can have multiple styles, whether I can also somehow encode the
    shadowing information, and what should I do with the dataset I currently have.

    Plan： compare training with and without hint. Then decide whether I should take the GAN off for ones with hint.
    After comparing the two results, I think without hint is definitely better (of course when hint is not present).
    So I'll go ahead and write the program for taking GAN off when hint is on.
    The results of taking GAN off when hint is onwas not so satisfying. It did seem to do better by a little on
    the real sketches but it did worse on the test set.

    Found a bug in preprocessing. It was caused by using the same variable name twice. Had to resize it again...

    I looked at paintschainer again. I am now sure that it uses some kind of trick or database that makes the coloring
    biased to the skin color. So even if it goes wrong, the result did not too bad. What I don't understand is why it
    just worked on the real sketches, even though its training data did not look anything like the real sketches.

2017/03/28
    The new sketch with hint was not coloring things so well... If I use the old sketch, then it just doesn't work on
    real sketches at all.
    (Preprocessing resizing works really bad on sketches by the way. The lines become discontinued after resizing by
    preprocessing.py)
    I know the current model works pretty well on old sketches. It works decently well when trained using new sketches
    on real sketch without hint. Is there any way to supply extra information and turn the real sketches into the
    old sketches (created by dilation), with all the information on shade and stuff?
    I can use a neural net to turn one into another I guess.
    I tried that. Apparently the discriminator is not good enough to tell the difference between the new sketch and
    the old one. The optimal result for l1 loss is to have a slightly darker background - also a sign of failing to
    recognize anything meaningful in the data provided.
    In the mean time I'm also trying to increase the depth of the network.
    I thought I was so close to succeeding.

    I don't know why Taizan successfully trained with inputs completely different from real sketch and still got
    good results.


    PS. Think about writing a software that can speak like a seiyuu. There are some training data (sort of), there
    are methods to realize my plan, and there will be interest in that. I thought about this when I was reading the
    manga and I thought it would be easier to understand if it was read out loud for me.
    After researching into that a little, I found that wavenet result was not really real yet and baidu's deep voice
    was not published yet (understandable, because they did a lot of hardware level optimization.)

    Maybe I can try the newest rnn for image recognition.

2017/03/29
    I tried to run the mix again with larger network. Did not work. The training seemed smoother, but without hint
    result on real sketches are just a mess.
    What I learned from experiments in the past two days:
    1. Only using old sketch is guaranteed to fail on real sketches
    2. Using hint seems to be incompatible with GAN, what I mean by that is it seems like you can't train the two on
    the same network. (Which is a little bit different from what taizan was using, he was using the same network with
    shared parameters for both with and without hint, but he did mention the incompatibility between the two.).

    I'm trying to turn off both hint and mix and see what happens.

    During the training, I saw the discrim loss spiked a few times, and each time that happens, the output became much
    more coloful until the loss falls back. So I thought it might be super easy for the generator to get colorful
    results... It usually takes only 5~20 minutes for the discrim loss to spike from min to like 0.5 or 0.2 or
    something. So maybe... Well I don't know why my tri experiment did not work yet (maybe that was because I did not
    use instance norm correctly?). Is there any way I can avoid falling into the statiscal average solution?
    I can avoid that by not using l1 loss. But then the discriminator became too weak and too easily exploited by the
    generator. I need to research what to do in that case, when the generator crushes the discriminator.
    Tips: https://github.com/soumith/ganhacks Some of them looks like voodo.
    Hum I haven't tried adding random noise as part of the input right? I could try that.
    Also try switching to leaky relu instead of relu
    Add back the dropout: according to the pix2pix paper it functions as a noise vector, but it also admits that how to
    capture the full spetrum of possibilities is an open question.
    (https://arxiv.org/pdf/1611.07004v1.pdf)

    By the way something completely unrelated: I read this blog http://distill.pub/2017/research-debt/ and it's quite
    interesting about the price to pay to explain things.

    It is interesting to see how the current experiment setting makes the GAN loss oscillate. It's like the Generator
    is trying to break free but then lose in the competition with discriminator and l1 loss. It happens after some
    training for both freshly trained model and model loaded from other checkpoints.

    The problem is just how to let the generator work without crushing the discriminator too much that the coloring
    looks unnatural. I'll try two things. One is to use a larger batch size coupled with smaller learning rate and
    slightly smaller l1 weight to get the discriminator loss slowly decreasing.
    The other thing I want to try is to take off the l1 weight entirely, but from past experiences I think the
    generator might just exploit the discriminator and generate images with artifacts. Nah that's probably a bad idea.
    lol

    The new sketch sort of works on some of the real sketches but it can only do skin color. I'm finding a way to
    train the model so that the discriminator loss can stably increase (meaning that the generator is doing a better
    job at generating realistic images and fooling the discriminator). So far I failed to control that.

    Some random thoughts:

        I think the best repo might be this one
        https://github.com/endernewton/tf-faster-rcnn

        The training data requires images as well as labeling 20 categories of objects as well as providing the bounding boxes in each image. In total there are 9,963 images, containing 24,640 annotated objects. It would be very time consuming to do the same thing for anime. The best I can hope for is to use some video analysis technique to extract the different layers in anime videos.

        Object tracking things:
        An overview of algorithms used in opencv:
        https://www.learnopencv.com/object-tracking-using-opencv-cpp-python/
        It is also available in aftereffect. It seems to be pretty mature technology, at least for real life images.

        I also looked into using 3d simulators to train neural nets. Currently there has been a lack of effort being put in that area. I saw one paper using data extracted from gta5 by gathering info in graphic cards. They've shown that by doing that they can train a successful car detector for the real world. But a lot has to be done if I want some well annotated 3d simulated objects for object detection training... There is just no framework currently that supports the task. I thought it would be as easy as changing the texture of the model but I didn't get too far in that direction because I don't know too much about 3d softwares. I don't want to spend too much time gathering raw data again... So let's stick with what I have now.

        I thought of image captioning, so I looked into the "Show and tell" model's corresponding github code. It mentioned that the whole training will take a month on a single gpu, that's a no... The biggest problem I have with this plan is two-fold. One is I do not have a pretrained classifier for anime images. Two is I do not have really "image descriptions" but rather comments on the image. The comment is very likely to be not so related to the details of the image at all. It might just say "oh it's beautiful" or something along that line.

        So it might be better for me to predict ratings. I don't know how good that will turn out... I think the ratings are related to too many things and not just the image. But it seems to be a more feasible task than generating comments for pixiv images...
        So what should be the structure? Just some simple deconv layers followed by fc layers and output a single number at the end? ... I don't think that kind of naive approach will work, unless I initialize the network with some kind of pretrained feature extractor. Even then it is likely that the network will just output the statistical average score. A better approach would be to separate the scores into buckets, and let the program predict which score bucket does the image belong to. That way it can share the same code as predicting which cluster does the images' tag belong to.


        Another thing I can do is: I can extract the sketch for all images, and try to find all groups of images that are near duplicates of each other. That would be quite time consuming as I think of it... (remember my summer internship project). And the result would just be the images that has been repainted by others... Probably not worth the time.

        What if I can cluster images based on their color histograms? Oh that sounds like a better plan. If I can cluster them based on their color patterns it would be super helpful for my sketch to color project. Ah and the distance function between two color distributions would be something like earth moving distance or kl distance or something.
        I can look into that. Automatic color pallete extraction or just google "cluster images based on color scheme"

        Oh then i can use condition gan to provide that extra coloring information to the network!

        I tried the object tracking thing. It will not work on anime. There are just too many switches between scenes
        that appears to be evident to human but not at all to a program. It will only abe able to track things over
        several frames. Tracking does not work on anime...


2017/03/30
    I think the current setting is promising.
    /usr/bin/python pix2pix_w_hint_lab_wgan_larger_sketch_mix_cond_refactored_dilation_deeper.py --mode train --output_dir checkpoints/pixiv_new_128_w_hint_lab_wgan_larger_sketch_mix_cond_refactored_dilation_deeper_contx2_no_h_no_m_no_l1_75 --max_epochs 20 --input_dir /mnt/06CAF857CAF84489/datasets/mnt/data_drive/home/ubuntu/pixiv_new_128_w_face_reshape_combined/non_manga_images.txt --which_direction AtoB --display_freq=1000 --gray_input_a --batch_size 32 --lr 0.0008 --gpu_percentage 0.7 --scale_size=143 --crop_size=128 --use_sketch_loss --pretrained_sketch_net_path checkpoints/sketch_colored_pair_cleaned_128_w_hint_lab_wgan_larger_train_sketch --use_hint --lab_colorization --sketch_weight=10.0 --l1_weight=90 --hint_prob=-1 --mix_prob=-1 --checkpoint=checkpoints/pixiv_new_128_w_hint_lab_wgan_larger_sketch_mix_cond_refactored_dilation_deeper_cont_no_h_no_m

    I don't think training from a checkpoint is necessary because the learning curve will eventually look the same
    regardless of your starting point.
    I think I should keep training because the learning curve has not plateaued yet. I think it is getting better.
    I am hopeful. Even if I have to sacrifice the hint, I shall make the no-hint part work. It is not yet as colorful
    as the previous versions, (because I did not tune up the gan weight, but if I want to, I know how.) but it seems
    to be getting better. Plus this is the best version I've seen so far without any glithches. It is resistant to
    all kinds of sketch inputs.

    Also in the future version the training data should probably not be resized. I should only use crop...
    I don't know how important is keeping the height width ratio. The reason I chose not to crop it in the past is
    because it will crop away say part of the face or something that will make it hard to tell what it is. The reason
    I am supporting it now is I saw PaintsChainer seems to use crop.

    Training more with hint and mix off doesn't seem to help too much although it seems like from l1 loss it is having
    a steady decrease. The color actually got less overtime. I tried taking off l1 loss and increased the sketch loss
    from 10 to 50 briefly but that seemed to generate greenish lines around the sketch, probably the network learned to
    minimize the sketch loss that way. Now I'm trying to only take off the l1 loss. That is actually not as bad as I
    thought it would be. The color looks wierd but it's not having serious problems like artifacts or overflowing or
    things like that. I'll try to train discriminator more by setting a big chance where the gen loss is set to 0 so it
    cannot train the generator.

    TODO: think about how to cluster palatte. It's not as simple as I thought it would be.
    TODO: I really like the result when the discriminator is half on the way of being crushed. (so loss around -0.6
    when before it was -2.5) If there is some way to prove that at that point the result is what I wanted,
    then I can find a way to hold it there.
    Yeah it seems to get bad around -0.1, or maybe it's just random chance, cuz I saw some good ones again later.

2017/03/31
    I trained the network with l1 weight = 10 for a whole night. The result is ok, but it could be more colorful.
    I noticed that in the training set output, the network was colorful at one point (420k) but slowly lost its charm
    afterwards, when the l1 weight dominated again and the discriminator was not able to discover the difference.
    The output is still not robust, the coloring changes a lot when some processing is done on the input.
    Also I noticed that the network is overfitting the training data. When I did the experiment where I set the l1
    weight to 0, the output of training data was awesome but test data was a mess.
    I want to try to train it briefly on the Pixiv_year dataset.

    # TODO: the preprocessing resize code has bug. The resize does not work very well

    Two projects using the color palatte data:
    one is to modify the current code to take in additional palatte data for each input image. It will be fed in as
    additional layers
    Another is to cluster the palatte data (but how to cluster palatte?)

    I have another thought, since the color palette should not affect the semantic understanding of the sketch,
    why don't I append it at the end of the conv layers instead of appending it to the input?

    It's probably too late, but I realized that method leading to more color does not lead to natural looking color.

2017/04/01
    The best result I had was: pixiv_new_128_w_hint_lab_wgan_larger_sketch_mix_cond_cont_x4_l1_40_with_hint
    the second best was either pixiv_new_128_w_hint_lab_wgan_larger_sketch_mix_cont_l1_75 or
    pixiv_new_128_w_hint_lab_wgan_larger_cont_no_hint_gan_5

    A common characteristic of them was: the relative GAN weight was increased to produce more color, and the
    improvement came only after the jump in GAN loss.
    Hint can be used. It is also ok to mix with sketches from other source. Acutally I think it is better to use
    hint and use sketches from other source. It seems to be so far impossible to control the jump in GAN loss, and
    it is not guaranteed that with enough training the discriminator will reach the optimal solution, or to even
    tell apart real images from some shitty looking outputs.
    Larger batch size seems to increase training speed in terms of the number of images it goes through per second, but
    I cannot increase the learning rate because it will make the GAN loss saturate too fast. (Even if I decrease the
    GAN weight, which only affects generator training. So it might be that the discriminator failed to train when I
    increase the learning rate.) The effect of switching from nn-backed sketch loss to simple dilation sketch loss
    was unknown, since there was no direct comparison between the two. The result of having a deeper network was also
    unknown. Conditioning the GAN on whether hint was used seemed to make the images without hint a little better.

    So here are my tasks:
    1. Test the effect of using the dilation sketch loss. (Completed. Worse.)
    2. Test the effect of using a deeper network, with dilation sketch loss. (Completed. Worse.)
        So far it looks like with the same training time (5.5h), the deeper network result was worse, with speed
        decrease at around 20%.
    3. Generate another kind of "sketch". (Failed)
        There doesn't seem to be other very satisfying colored-to-sketch methods other than what I currently have
        and style transfer. I am worried that style transfer on anime images will not work, because the network
        was not trained for anime images, but as a second thought it works with paintings, so it might work with
        anime images as well.
    4. Play with palette model. It's too slow now.
    5. Test increasing the overall learning rate but keep the learning rate of discriminator synced with that of
    the generator. I think the discriminator might be unstable.
    6. Fix the preprocessor.
    7. Other future experiments, like palette clustering, like incenption net feature vector extraction.
    8. I realized that the tri experiment, maybe I should not co-relate the weights of the generator networks, but
    first I should try to switch to instance norm (real instance norm). Then I should try to disentangle the
    weights and the rest of the training should be the same. I believe it should work. There is no reason why three
    separate networks cannot learn three separate patterns. First test it on the sanity check to see if it can learn
    three completely different patterns, not just in the background or the overall color.
    (Because the overall color change can be learnt simply by changing the last layer's scale, so that it
    can become darker/brighter etc. That is too easy.) (Failure, no improvement over previous ver)

    Comparing new sketch loss vs old sketch loss vs old sketch loss + deeper net.
    With same amount of iterations, new sketch loss had the lowest l1 loss, followed by old sketch loss with difference
    0.1. The difference bewteen deeper net and no deeper net is 0.6.
    Maybe simply with more training, the network will get better.
    But for now I want to test how the previous setting work on old sketch loss (with or without + deeper net)

2017/04/02
    The first two tasks are done. The method works but tends to overfit and produce artifacts . Deeper network
    looks a little bit worse than the non-deep version and it has lots of artifacts. I would actually call it a
    failed training because it seems to ovefit the training data too much without generalizing to other images.
    It is interesting that the discriminator never gets crushed during the training. It's loss is slowly decreasing.
    This is unexpected, as I thought the discriminator now has to do more work telling apart all sorts of possible
    outputs. But it seems that it does better in that environment. Both of them did worse than the
    pixiv_new_128_w_hint_lab_wgan_larger_sketch_mix_cond_cont_x4_l1_40_with_hint
    This might be caused by 1. difference in training method. 2. Difference in the sketch loss I used. Maybe dilation
    sketch loss is not good enough.

    What are the artifacts..... I still don't get why they always appears in batches. There should be something that I
    should have turned off for test but did not that is causing the problems. The artifacts appear on the same
    image if I repeat the run, so it's not entirely random. Yeah the artifacts are probably caused by overfitting.


    Trying to train directly from l1 weight 100 checkpoint to l1 weight 40 checkpoint and compare that with
    the 4 step training... (PS. both nn sketch loss and dilation sketch loss stopped at around 160k iterations for
    their first stage of training. I compared the two but there does not seem to be too much of a difference between
    the two sketch loss. )

    I am also trying to create new sketches using style transfer. nah it's not gonna work... after 10k iterations
    the result was barely recognizable.
    I downloaded the arbitrary style transfer repo. The result was amazingly good for normal style transfer, with
    super fast speed and decent results. Unfortunately, the style transfer does not work on anime images, as expected.
    I tried a few sketches and then I realized that it was the content iamge that was affecting the result mostly. If
    the network cannot recognize an eye, it will not be able to retain the sketch when it does style transfer.
    So I cannot generate more training sketches from style transfer. It's a shame. The arbitrary style transfer
    works so well though. I tried the reverse--using sketches as content and colored as styles--failed of course.

    Modified the tri code so that now all generator weights are multiplied by the one hot vector. Now I'm worried that
    after 500 epochs on the sanity check, it still cannot learn the output perfectly and the loss plateaued. I
    think that was caused by the inherit ambiguity in the 15 possible outputs. The outputs can't cover them all.
    But one thing that worries me more is the inherent flaw in the design. Let's say network A generates red hair,
    and network B generates blonde ones. Now I have a character with blonde hair and green eyes and another character
    with blonde hair and red eyes. How can the model learn them both? Now let's say we have 100 pairs of possible
    hair color - eye color. How would the 3 possible outcomes using nearest neighbor capture all of them? The
    answer is it can't. It would be better off than just one possible outcome of course but at the cost
    of triple training time. But how better off would it be? (Consider 1 million possible output color
    combinations instead of 100)

2017/04/03
    The training was slow. Not only does it have to compute 2 times more than the standard version, each one of the
    three outputs get to train 1/3 of the time, so that makes training 9 times slower. l1 loss decreased to 0.14 at
    30k, which is usually the value achieved at 10k for the normal version. There should be an easier way than this.
    Some way to make it share the same weights but have different coloring.
    So far the output is just the same as previous versions of the tri experiment -- different overall color but
    the coloring is uniform. that is if the output is dark, it will be dark for every output. I'll call this version
    a failure.

    I was working on the palette experiment and I found that the palette extraction code can be used to detect
    mono-colored images! I didn't expect that because I thought it would always output x colors.

    The result was good on not-real sketches. For both the old sketch and the new sketch, it works really well,
    almost as good as using hints. The problem is for some reason it's not working on real sketches. It's just
    not recognizing the semantic meaning behind these real sketches. The non-palette versions definitely is
    capable of recognizing some of them. I don't know why in this version it can't. Maybe that is because I
    took away the concatenation of the last and second to last conv layer? I'll let it train for a night.

    Reading the CycleGan paper by Alexei (Alyosha) Efros. He seems to be pretty well known in the field that
    I'm currently doing. He's the one behind pix2pix and colorful image colorization and
    "Generative Visual Manipulation on the Natural Image Manifold" He has been doing texture transfer since 2000s.

2017/04/04
    After 15 hours of training, the palette network was getting better on test sets, but not on real sketches.
    It seems like maybe it is exploiting the details in generated sketches that does not exist in real sketches.

    TODO: replace batch norm with instance norm
    TODO: try to generate more sketches if possible. (Using the trained pix2pix network)

2017/04/05
    I compared the result of training with mix 0 and mix 1 for a whole night. The l1 loss of mix 1 is lower, expectedly.
    The result of mix 0 looks a little bit better because it has less artifacts - or that it is less colorful than mix
    1. Not that that is a good thing, because the coloring of mix 1 is pretty much all wrong. It colors the face red
    for some reason.
    BTW the training speed of mix 1 is 2.5 times slower than mix 0.

2017/04/05
    The instance norm version is less colorful than the previous version for some reason... TODO: review the
    difference between batch and instance norm mentioned in the arbitrary style transfer paper.

    i realized why there was artifacts!!!!! It was because of the batch norm! I remember in some paper it says that
    when in testing, the batch norm statistics is replaced by an estimate of the training set instead of calculating
    the stats from the test data. But I didn't do that in my network! Oh... I can prove that by changing
    batch size to one and run the test again. It should not have artifacts and even if it does, that should not
    affect other images in the same batch.

    I also found out that during the last two training for the instance norm version, I did not set the l1 weight
    although I thought I did. That was why the output looked so bland. I'll try without mix and train with l1 weight
    equals 40 again. Yeah that was it. Plan: I need to read the batch norm and instance norm paper again.
    I also need to check other git repos to see what others did.

    I didn't see any artifacts in the instance norm version. The problem with instance norm is that
    it is doing ok on with hint version but without hint it's doing pretty bad. Maybe that has to do with the
    instance norm -- Although it does not normalize the input, it normalizes the first layer after the input on
    the height and width direction. (DOUBLE check)
    Yeah I never knew that batch norm works that way. * Master the basics.

    In the first paper that proposes instance norm by Ulyanov, it says " Intuitively,
    the normalization process allows to remove instance-specific contrast information from the content
    image, which simplifies generation.". But in subsequent paper by Huang, it points out that the normalization
    happens in the feature space instead of color space, so it should be normalizing the features in some way instead
    of the contrast of the image.
    So I should modify the batch norm test step. I need to calculate the statistics for each layer.
    Or not... The tensorflow's tf.contrib.layers.batch_norm does that for me when I set is_training to True/False.

    Continue training pixiv_new_128_w_hint_lab_wgan_larger_sketch_mix_cond_refactored_ins_contx3_l1_40_no_mix if I have
    time. I want to retrain batch norm version first.

    One small note: I found that the test set's images actually appeared in the training set for some reason, at least
    one of them did.

    I was worried to find that the sanity check failed.
    I think that is because the scale is different for training and test. I could do random resizing in addition
    to random cropping...
    But then i found out by re-enabling flipping and cropping for testing that the output still looks less colorful
    for some reason. I found that it is because of the is_training setting. (by manually setting that to True).
    I think it is fine. I've seen people say that it takes a while for batch norm to warm up. I will test that
    hypothesis by training sanity check for a long time. After training for 20000 epochs, it proved my hypothesis.

    And the training got 3 times faster for some reason after I switched to slim. That's wierd...
    It's not because of slim. I finally figured out how to do batch norm during test time with some hacky way.
    I'm trying dilation sketch input and sketch loss. The output was ok after I train it for 20 rounds, but still
    not good enough sometimes. I was doubting that the generated new sketches were not good enough and that was
    affecting the results in some way.
    I don't know why the training is three times faster for pix2pix_w_hint_lab_wgan_larger_sketch_mix_cond_refactored_bn
    It's wierd. There is no difference between them apart from the sketch generator for sketch loss.

    I need to implement validation during training. That is I use a dataset outside the training set, ideally
    real sketches I collected along with the colored version. After x iterations, it evaluate the validation set,
    calculate the losses, and output the results. It should make it clearer how training progresses. But it's not
    so crucial. Do it if I have time.

    TODO: figure out why the previous code was significantly faster than this version...
    No not only it's the previous code. I found that the current code is also pretty fast -- if you run it as a
    second instance... (so start one instance and the second is 2 times faster than the first one)

04/07
    The result was still not so good for the real sketches. Maybe I can try to generate another type of sketches
    by training a little bit less so that the background is still in the sketch.
    Or maybe I can set a threshold, and any pixel above the threshold will become 0.
    Actually, why do I think that it's the fault of the input sketch? Or that it is because the input sketch does not
    match the real sketches? I never tested changing the intensity of the sketch... So maybe I should.
    TODO: make a dataset where the intensity of the input sketch changes over 5 levels and see what happens.

    I tried to retrain two sketch generators. Using the sketches they generated, the results looks good. But
    if I use the real sketches, the result was aweful. So I'm thinking maybe there is a way to turn one type of
    sketch into another...

    PS. I can't directly use the 128x128 sketch generator on 512x512. Too many artifacts.
    Plan: after sketch conversion training is done, convert the sketches, and use them as the input.
    sketch converting worked... lol. The output was a little bit homogenous but it's better than I've seen in the
    past. it always produces red eyes and blonde hair, even with previously trained model. It improves stability.
    The best result is still from pixiv_new_128_w_hint_lab_wgan_larger_sketch_mix_cond_cont_x4_l1_40_with_hint

    I'm wondering if I can achieve the same result not using one extra neural net but through something like
    back propagation or something. Because I know that the result will be better if I modify the input a little.

    The training commands for the best result so far were:

    /usr/bin/python pix2pix_w_hint_lab_wgan_larger_sketch_mix.py --mode train --output_dir checkpoints/pixiv_new_128_w_hint_lab_wgan_larger_sketch_mix --max_epochs 20 --input_dir /mnt/06CAF857CAF84489/datasets/mnt/data_drive/home/ubuntu/pixiv_new_128_w_face_reshape_combined/non_manga_images.txt --which_direction AtoB --display_freq=1000 --gray_input_a --batch_size 4 --lr 0.0008 --gpu_percentage 0.26 --scale_size=143 --crop_size=128 --use_sketch_loss --pretrained_sketch_net_path checkpoints/sketch_colored_pair_cleaned_128_w_hint_lab_wgan_larger_train_sketch --use_hint --lab_colorization --sketch_weight=10.0
    /usr/bin/python pix2pix_w_hint_lab_wgan_larger_sketch_mix.py --mode train --output_dir checkpoints/pixiv_new_128_w_hint_lab_wgan_larger_sketch_mix_cont_l1_75 --max_epochs 20 --input_dir /mnt/06CAF857CAF84489/datasets/mnt/data_drive/home/ubuntu/pixiv_new_128_w_face_reshape_combined/non_manga_images.txt --which_direction AtoB --display_freq=1000 --gray_input_a --batch_size 16 --lr 0.0008 --gpu_percentage 0.6 --scale_size=143 --crop_size=128 --use_sketch_loss --pretrained_sketch_net_path checkpoints/sketch_colored_pair_cleaned_128_w_hint_lab_wgan_larger_train_sketch --use_hint --lab_colorization --sketch_weight=10.0 --l1_weight=75.0 --checkpoint=checkpoints/pixiv_new_128_w_hint_lab_wgan_larger_sketch_mix
    /usr/bin/python pix2pix_w_hint_lab_wgan_larger_sketch_mix.py --mode train --output_dir checkpoints/pixiv_new_128_w_hint_lab_wgan_larger_sketch_mix_cont_l1_75_no_hint --max_epochs 20 --input_dir /mnt/06CAF857CAF84489/datasets/mnt/data_drive/home/ubuntu/pixiv_new_128_w_face_reshape_combined/non_manga_images.txt --which_direction AtoB --display_freq=1000 --gray_input_a --batch_size 16 --lr 0.0008 --gpu_percentage 0.45 --scale_size=143 --crop_size=128 --use_sketch_loss --pretrained_sketch_net_path checkpoints/sketch_colored_pair_cleaned_128_w_hint_lab_wgan_larger_train_sketch --use_hint --lab_colorization --sketch_weight=10.0 --hint_prob=-1 --l1_weight=75.0 --checkpoint=checkpoints/pixiv_new_128_w_hint_lab_wgan_larger_sketch_mix_cont_l1_75
    /usr/bin/python pix2pix_w_hint_lab_wgan_larger_sketch_mix.py --mode train --output_dir checkpoints/pixiv_new_128_w_hint_lab_wgan_larger_sketch_mix_cont_x2_l1_50_no_hint --max_epochs 20 --input_dir /mnt/06CAF857CAF84489/datasets/mnt/data_drive/home/ubuntu/pixiv_new_128_w_face_reshape_combined/non_manga_images.txt --which_direction AtoB --display_freq=1000 --gray_input_a --batch_size 16 --lr 0.0008 --gpu_percentage 0.45 --scale_size=143 --crop_size=128 --use_sketch_loss --pretrained_sketch_net_path checkpoints/sketch_colored_pair_cleaned_128_w_hint_lab_wgan_larger_train_sketch --use_hint --lab_colorization --sketch_weight=10.0 --hint_prob=-1 --l1_weight=50.0 --checkpoint=checkpoints/pixiv_new_128_w_hint_lab_wgan_larger_sketch_mix_cont_l1_75_no_hint
    /usr/bin/python pix2pix_w_hint_lab_wgan_larger_sketch_mix.py --mode train --output_dir checkpoints/pixiv_new_128_w_hint_lab_wgan_larger_sketch_mix_cont_x3_l1_40_no_hint --max_epochs 20 --input_dir /mnt/06CAF857CAF84489/datasets/mnt/data_drive/home/ubuntu/pixiv_new_128_w_face_reshape_combined/non_manga_images.txt --which_direction AtoB --display_freq=1000 --gray_input_a --batch_size 16 --lr 0.0008 --gpu_percentage 0.45 --scale_size=143 --crop_size=128 --use_sketch_loss --pretrained_sketch_net_path checkpoints/sketch_colored_pair_cleaned_128_w_hint_lab_wgan_larger_train_sketch --use_hint --lab_colorization --sketch_weight=10.0 --hint_prob=-1 --l1_weight=40.0 --checkpoint=checkpoints/pixiv_new_128_w_hint_lab_wgan_larger_sketch_mix_cont_x2_l1_50_no_hint
    /usr/bin/python pix2pix_w_hint_lab_wgan_larger_sketch_mix_cond.py --mode train --output_dir checkpoints/pixiv_new_128_w_hint_lab_wgan_larger_sketch_mix_cond_cont_x4_l1_40_with_hint --max_epochs 20 --input_dir /mnt/06CAF857CAF84489/datasets/mnt/data_drive/home/ubuntu/pixiv_new_128_w_face_reshape_combined/non_manga_images.txt --which_direction AtoB --display_freq=1000 --gray_input_a --batch_size 16 --lr 0.0008 --gpu_percentage 0.40 --scale_size=143 --crop_size=128 --use_sketch_loss --pretrained_sketch_net_path checkpoints/sketch_colored_pair_cleaned_128_w_hint_lab_wgan_larger_train_sketch --use_hint --lab_colorization --sketch_weight=10.0 --hint_prob=0.5 --l1_weight=40.0 --checkpoint=checkpoints/pixiv_new_128_w_hint_lab_wgan_larger_sketch_mix_cont_x3_l1_40_no_hint

    I don't know how many iterations did each one train. All I know is it trained for 160 iterations on the first step.
    The last step was 300k iterations.

    Things I need to do remotely:
    1. Crop and resize all pixiv images I've downloaded (two drives)
    2. Download both the resized and the original colored_sketch dataset.
    3.
04/08
    A few things I'd like to re-mention. I think I'm kind of stuck now. The results is ok, but it's not nearly as
    good as the official paintschainer version. I'd like to rerun my dataset on the paintschainer code and see
    if it is still doing poorly. If so, then it's the problem of either the training parameters, or the dataset.

    Secondly, I noticed that the output is very different given slight changes in the input. As a result, the coloring
    looks pretty inconsistent. Therefore I was thinking three ways to combat this. One is to add a variance loss (diff
    between neighboring pixels). Another is to use back propagation and allow the network to modify the input so
    that it can get the best possible result in the eyes of the discriminator, while making sure that the sketch loss
    is low. The third way is to add noise to the input and force it to have the same input regardless of the noise.
    ( I just realized that paintschainer code added random noise to both the coloring and the sketch... I did not do
    that in my code because I read the wgan paper and it mentioned that by adding wgan it no longer needs to add
    noise to the input. I should try to add noise. I noticed in paintschainer the adversarial network loss is set to be
    the same as the l1 loss, and sketch loss 10 times that. The lr was 0.0001. So maybe I should try that setting.
    But notice that the l1 loss is for 0~255, so I should time that weight by 255/2. sketch generated is 0~1 I think.
    I don't know. Maybe my setting was about right afterall.

    I had to admit that I did not have enough courage to look at paintschainer code. It reminds me how bad I am at
    deep learning and how I failed and am still failing.

    I found one more thing in the paintschainer code. There is a piece of code where it randomly loads other sources
    of sketches. There are a total of 3 sources used. One is called cnn and one is called b2r. According to
    https://github.com/pfnet/PaintsChainer/issues/88 that is historical code used for data augmentation. Taizan seemed
    to have tried using inputs of different line widths.

    TODO: retrain paintschainer (In progress)
    TODO: test adding random noise (In progress, needs to start training)
    TODO: test using back prop
    TODO: Test adding variance loss.
    TODO: Test adding weight decay using slim.

04/09
    Paints chainer training did not give me a good result yet after training for 3 epochs (30k iter with batch size 16)
    The whole training will take 2 days, so do that when I have time.
    I want to start testing on adding random noise and varying the sketch width.
    Ah I trained for too few rounds. The result wasn't clear enough. Also I should probably start with lower batch size
    (or higher lr?) The result is still unclear. One thing that I found is that after I added the noise, the new
    sketch may become too dim -- the sketch is not really clear. The problem shouldn't occur with the old sketch
    because they're usually pretty clear - much clearer compared to new sketch. Let me test with the old sketches.

04/10
    I accidentally ran only three epochs, but I can see that adding noise might work - it works a little bit better
    than changing the width of the sketch lines.

04/11
    I ran the program on old sketches with the two settings: adding noise and changing the width of the sketch
    lines. The result wasn't so good and I suspect that was because I was not using the nn generated sketches.
    The problem with nn generated sketches is that it is not as clear as dilation generated sketches. So
    to compensate for that I added a different noise level for each type of sketch (dilation sketch has more noise).
    I also combined that with changing the width of the sketch, but I decreased the max width from 8 to 6. 8 is probably
    too much for 128x128 images. I also added a feature to decrease the l1 weight after each epoch. Because I found
    that most successful runs comes from manually decreasing l1 weight after training for a few rounds. Hopefully
    this current setting will work.
    Lastly, I double checked the instance norm,  because I thought it makes more sense to normalize each image since
    each image has different "contrast level". (Although this is not what the real time style transfer paper suggested
    the instance norm layer was doing.) Also I thought for some reason that the instance norm did not work. I looked
    back on the results and found that sometimes it works, so it might be the way I'm training it or something.
    I also need to test the weight decay but I don't think that will change the output by a whole lot.

    I'm kind of worried that the correct batch norm does not work properly with hint. Because batch norm test time
    uses moving average of the scales and standard deviation of each layer, but the input can either contain hint
    or does not, so how batch norm deals with that is kind of hard to say. The moving average would be the average of
    with and without hint, but the input can only be either with or without hint, not in the middle.
    The output after 100k iterations contained lots of artifacts. The ones without artifact
    was ok, but not the best I've seen. I was thinking to switch to instance norm.

04/12
    The result of instance norm without hint was bad. It was almost without any color. I'm trying to train only
    on without hint ones, but maybe the whole idea of normalizing the "features" does not make sense for generating
    colors. Maybe I should only use instance norm on down convolution? TODO: try that if I have time.
    Also trying to train by replacing relu with lrelu.
    If that still doesn't work, try weight decay... And if that still doesn't work, try clustering training set
    on its palette and select ones with similar palette to form a smaller training set...

04/12
    lrelu results in less artifacts but less color as well. Or maybe that was because of a bug in decreasing the
    l1 weight. Yeah. The l1 weight was only decreased to 81 while it should decrease to 50 a long time ago. The
    should() function has some bug. The frequency is always three times lower for some reason. I think that's because
    the slim.learning.create_train_op automatically grabbed the global step. LOL.
    Now I found out why all of a sudden training seems to be faster after I switched to slim. It's just triple
    counted the number of steps.

    The result of the last training was good. no artifacts, vibrant colors although a little bit unreal sometimes.
    I think the key is really decreasing l1 weight overtime. I think one paper mentioned that the discriminator was
    not so good at the beginning, so they did some trick or something. Other papers also mentioned decreasing learning
    rate overtime. My modification serves the same purpose.
    I'm planning to add on the weight decay, hoping to force the model to be more general.

04/13
    After I added a tiny weight decay, the model only learned to output black and white. I double checked by
    setting weight decay to 0 and it looks normal, so it's not caused by changing to using slim.
    Maybe the weight decay was too large, but that didn't look large to me and it was the setting used by
    paintschainer. Anyway I will decrease it further.

04/16
    Even with l2 regularizer term = 1e-8, the generator still cannot beat the discriminator. It can finally
    learn the skin color but there were lots of artifacts. It is pretty wierd why the term needs to be
    so small.

4/18
    I was summarizing the pix2pix paper and then I realized that maybe I should not use the patch based
    GAN network, because it makes such an assumption: "Such a discriminator effectively models the image as a
    Markov random field, assuming independence between pixels separated by more than a patch diameter."
    Maybe I should try a different discriminator.

    I also fixed the preprocessing bug. The bicubic resize method was not working well so I'm using Area isntead.

4/21
    Changing the discriminator is not solving any existing issues. I'm waiting for the larger dataset.

4/23
    I tried using mix=  1 on the larger dataset. It turns out that it still does not work on the real
    sketches, not surprisingly.
    It will take me a while to combine the datasets into one final version, which will probably contain around
    800000 colored images. One serious problem though is that some of the images are mono-colored. I might
    need to get rid of those as well. TODO: write program to detect mono-colored images.
    According to https://www.quora.com/What-is-the-most-reliable-algorithm-to-detect-if-an-RGB-Image-is-monochrome
    it can be done by transforming images to hsl, getting rid of the ones where l are close to min/max, and
    detect whether all h falls within the same range(probably by calculating average h and do l2 distance from that).
    If so, then the image is monochromatic.

4/24
Maybe I should try the began, which is supposed to work even better for one-to-many mapping...

Maybe I should try turning the GAN back on for the instances that are provided with hint... I think so because
it will make the discriminator trying harder to tell apart the colors, thus hopefully produce more
colorful images. Now it's biased towards one type of color.

4/27
Another problem I realized when I was doing sanity check on multiple possible outputs. How does the model
decide which one to output? Afterall the current model is deterministic. Unless the model can be conditioned
on something... I don't know. How does other gan model solve this?

Plan: FIRST PRIORITY: I really need to add a validation set to detect overfitting during training.
Finished this. There is a huge gap between test and train data loss.

TODO: implement back propagation on the input sketch. See what happens.
I tried that. The result is what I expected. Very noisy outputs with lots of colors but the colors don't
make sense. Maybe the discriminator is not good enough structual-wise.

4/28
I am double checking current code. Didn't see any bugs.
TODO: try modifying encoder decoder network
There are a bunch of networks here https://github.com/igul222/improved_wgan_training/blob/master/gan_64x64.py
TODO: try taking away the conditional gan (I tried that, it did not work that well so that means this will
not change the output by a whole lot.

In the scribbler paper, it mentioned that conditioning on input image tend to destablize training. They also did
not have fully connected layer and did not use batch norms.

I just realized that in scribbler, there are also two stages... The first stage gan weight is 0 and the second stage
pixel loss is 0. How come I did not notice that. My god.

TODO: rerun the PaintsChainer code on my dataset.
TODO:re-read the two papers at least twice.

I just realized that in my code the discriminator loss is not linked to gan weight... I won't change it
for now because it will save me time trying to train the discriminator after I finished the first stage...
BUt maybe I need to change that later on.
The huge gap between training and validation dataset also worries me.

5/2
Other experiments I;ve done: training paintschainer did not yield good results. Training on new sketch works
better than paintschainer generated sketch.
Conditioning on noise did not help, I suspect because there is already hint.

Changing to other discriminator didnt work probably because of my hyperparameter settings.

What is my last resort? My last resort is to turn sketches into formats similar to the dilation sketch.
What do you mean by format? I mean texture? or local statistical patterns. Anything that makes a generated
sketch looks different from the real one.
How? Maybe through style transfer... Maybe through other techniques, more classical ones. But first I
want to quantify the difference.

Thoughts:TODO: what if we use the humanface to dog face project's idea. That is: we generate the sketch
from black and white images, and that's our source. Then we have the target. Then we train two functions simultaneously
using the Unsupervised cross-domain image generation loss function? That'll be a little slow but maybe the
generated sketch will look closer to real sketches.

2017/05/05
I thought what if I use the validation dataset to train the slim batch norm's shift and scale? That way the
normalization will be done using the actual sketch data. .. Let me try this.

2017/05/10
I spent most of the past few days converting 512x512 sketches and downloading new sketches from pixiv.
I think this will be my last attempt. If I can't convert one sketch into another, then even though the
training works well on the training data, it can't work in real life and therefore has no use.

Oh, and I need to take out the monochrome images. TODO: not working really well yet. Try to make it better.

I am still very puzzled about why

2017/05/13
Experiment that tries to convert normal sketches to dilation sketches did not work very well. There is not
much positive change seen after using the conversion.