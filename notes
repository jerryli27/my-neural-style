This is the notebook for recording experiment results, discoveries, and next steps.

It is crucial to have a low enough learning rate. If you have a high one and decrease it over time, it still won't work.

MRF is confirmed not to work with feed forward neural network. But mrf is not really absolutely necessary. I might need
it for semantic related masking stuff. But it has also been shown that masking works with gram loss and feed forward network.

Here are two unsolved problems:
The size of the features/styles. The image genearted when input is 100 x 100 versus 1000 x 1000 is drastically different.
The generated image is limited to the structure of the original painting. In art, you can draw something larger or
smaller than usual to achieve some effect. There's no such thing here because the loss is localized
(You can't just change the absolute position of objects in content image.)


Two ideas: artists "zoom in" to a spot when they want to paint it in more detail. Is it possible to do so in our architecture?

Second idea: if the generator network can generate styled images, why can't it be used to reformat the feature layers?
Turn the feature layers into something that look like other artforms (without loosing too much originality)
and generate ... But how's that different from just use the generator network to generate the image? Not really

I want something other than difference squared loss. I thought about nearest neighbor loss of the normalized feature
layers. That one is invariant to translation.


Trying two things now: one is to directly observe what is going on in each loss function for each feature layer.
Another is to feed multiple styles into the network while modifying the same parameters.

simply feed 15000 shirobako images as style didn't work.

I tried the mask again using hand-segmented shirobako mask (mrf loss). Surprisingly it's sort of working. Maybe I don't need the
content loss. I can just create a drawing from scratch. Maybe that would be too hard.
One more difficulty if I choose to go on this direction. I don't have hand-segmented masks for me to train.
Even if I do, I don't know how to incorporate say 10 masked style images together. Simple nearest neighbor matching on
the 10 images would require 10x memory in the gpu.

One idea for automatically label images: start with example image and a hand-labeled mask. Now for each input
we compute the nearest neighbor of say conv4-2 (any high level layer) and assign the nearest neighbor's mask
to the input layer. Then add the constraint that nearby pixels should preferrably have the same label.
Then do deconv to get pixel-wise label for the original image. (Sounds like a plan, but this requires the
high level layers of the style and content to be similar, which may well not be the case. 20161122)

Texture and placement of objects are two different things. Texture is given a mask, how to fill in the colors
so that the style loss is the least. That is already solved. Placement of object is another issue. How to
place objects relative to each other so that it is the most probable.

I tried the nn loss. NN is not differentiable so it has no gradient. I should've realized earlier. Now I probably have
to get around this problem.

One thing is, the reason why mrf can perform better is because it has those patches that captures info about nearby
cells. Gram simply multiply them together and add all cells. There's no interaction between a cell and the cell on its
side. On the otherhand, the mrf is expensive to calculate because of the nn step.

After some thinking, it comes down to knowing the feature layers first. (like each of the conv layers, when they're
activated and compare two inputs, content and style, see when their conv layers look similar.)

Adversarial generative network may be worth investigating into. But don't do that just yet. I still have to finish up
this experiment.

We can make the additional semantic masks learnable. Just add a loss for too much deviation from original (more
complicated loss can be added later)

Things learned from the experiment: the conv layer won't look close to the original image. So we can't hope to morph
one image into another in the conv layers. Also, making semantic masks learnable will need further refinement.

I thought maybe finding the spatial correlation of semantic layers might be helpful. Now it is only finding the
correlation in-place (that is, the same pixel times the same pixel in another layer). What happens if we shift it by
say half of the width and find the correlation? The hope is that , for example we have two eyes, one on the left
and one on the right. By finding the correlation after shifting, we may find: ah whenever there's an
eye here, there will be on on its right with distance half screen away. That's my hope.


I was correct that correlation after shifting one layer encodes the relative positional information. Now the problem is
1. It was not perfect. I hope the problem can be solved after adding a content loss.
2. It was spatially too similar to the style image. ie. no shifting etc. There was some shift for subfeatures, like
the mouth was on the right first and slowly came to the center. But overall the head position was still the same, the
position of everything was the same as in the style. This is not what we want. We just want to modify the content image
a little so that the eyes become bigger, or the nose become less noticeable and things like that.
I want to have two very simple images as input and see how it goes.

Not going well so far.

Side projects during thxgiving:
Run multi-style feed forward network.
Shit.. I realized it actually doesn't make sense in the current frame work. I have to separate each style by itself
instead of feeding them in as batches. ( Otherwise I can't set the image placeholder so I can't train scale and offset
individually. I'll try to fix this tomorrow by going back to the master branch and merge the two..
I don't know if I should keep the batch style... Maybe I should.

Tested overnight on claude monet's paintings. Now I'm sure I can't just merge styles like that. It doesn't work.

Other future directions: feed forward neural doodle.


As I think of it, maybe we don't need the content image at all? There are many things that are hard to find in real life
but exist in drawings. I know that a feed forward network with masks can be trained as long as I have enough training
data. The only problem is where to get that semantically labeled training data. I can get that from 3d model... I think
I can look into that direction. No content image. Content loss comes directly from thousands of style images. Details
of how content loss can come from those may be filled in later but I think this is the right direction. We don't have
to worry about distorting content image into the style image, which is a big challenge.

Before that, let me modify the current code so that it can accept only semantic style images and semantic masks and
learn them using feed-forward nn. the loss... we can use content loss or style loss. try both I guess. I need to
copy the previous code into a new file because the change will make the code incompatible with the previous version.
(since there no longer need to be a content image). Actually, I can use the same file... Since I may use that framework
to learn auto generating textures.

Basically I will implement feed forward neural doodle, but better than what they've done. I need not only rgb mask, but
any number of masks for each style image.

2016/12/09
I implemented fast neural doodle. Testing it. It is running super slow for some reason... I don't get it. I should test
running the normal version... (without masks and doodles.)

2016/12/10
Fixed some bugs, but the thing is still super slow, even if I don't turn on masks. Was it like that from the very
beginning? I think so because it takes the whole night to train around 20000 rounds, so it's like one round per second.

Also the loss for style is unnaturally high only after I apply masks. I found that was because I was doing dot product
so each number is dotted with 255. lol.

But the result was still unsatisfactory.

2016/12/11
What can I do that will have some originality instead of repeating what people has done? What is the limit of the
current system?

If working on masked images:
    You must first have a mask, whether it is based on real images or not.
    The mask cannot be too detailed, or it will use up too much memory (grows linearly) and time it takes to create
    such a mask is prohibitive.

If not working on masked images:
    The model clearly does not have an idea of scale. If you train it on a 256x256 image, you can't just put that
    on a 1024x1024 image because the patterns created will be too small.
    It has no understanding of the relative size/location of objects (unless the two objects are close enough that they
    become in the same cell after convolution. That is a problem for vgg, but since we're using gram matrix, the
    problem is also applicable in style generation. For this reason, it is still impossible to tell it to "paint a cat"
    and expect it to just work.
    It also has no idea about different style between different objects. See comparison between mask and no mask.
    It

--
The thing is not working, at least not on johnson network... I don't know what's going on, but my guess is that johnson
might not be the best choice for texture generation with masks. I'll try the pyramid network...

Planning to try pyramid network
read blog and source code on feed forward network and see why mine did not work.
The johnson feed forward network is definitely working. It just takes a while to train. It get's faster(to a tolerable
level) when I decrease the batch size from 8 to 1. I'll try that on the masks.

2016/12/12
on going work:
Trying pyramid network without masks. Just normal content input.
Planning to try johnson network with style generation only. No masks. -- It worked!

I'll try to replace input to "johnson network with masks" with random
noise instead of the masks themselves. It probably will get better after intensive training... Usually generating image
with random content/mask takes a while to train. Ok there's definitely something wrong with the masks. The masks
changed their meanings in the middle of training, so we're definitely loading them wrong. If I'm lucky, it might just
be the output but not the training (judging from the quality of generated images).
I tested the output. The output's order is correct. So it's the training process... But I don't get what part went
wrong. I tested getting the files. That part was correct. ... TODO: debug this. I don't know what it will look like
after more training, but now it's not so satisfactory yet.
It is interesting to see how each mask acts. when value of a mask is 0, it doesn't mean that it will be black. Instead
since it indicates that it is not a certain mask, it becomes the color of other masks.
But when only one mask has all 255s and the rest 0s, the result just becomes the same as no masks for some reason..
I don't understand why, but it seems the network is doing something wierd.


Also working on finding out why pyramid generator did not work so well. The image generated was just too blue...
I first thought it might be that I forgot to preprocess the image or something like that. But I checked and it's fine.
So I thought it might be the generator net. I want to test that with simple "style-only" task. That failed ... old
problem.

2016/12/13
Re-reading the feed-forward paper and trying to figure out why my network did not work.
Differences:
    1. They used a uniform noise distribution instead of the whatever distribution numpy offers. (fixed, changed all to
    uniform random)
    2. according to the original paper, normalization is done also before concatenation layer. (while in another paper
    it says otherwise. Personally I think it is not necessary to have normalization there, but I want to at least
    reproduce the result of the author. (fixed)
    3. Initializations (fixed)

Humm... Where's the problem???
Maybe it's not in the generator network??
Alright I give up... I don't know why it doesn't work. Pushing this off to tomorrow.

Task 2: Johnson network with masks:

Task 3: improving slow version.
    I was comparing the texture generation network (with and without mrf, with and without masks). I found that the key
    lies in the loss function, ***so if I want to make something new, I need to invent a new loss function. ***

    I tested style generation (no content image) for mrf loss and gram loss. It turns out that both are effective,
    although mrf loss is better at preserving local structure and information. gram loss knows little about the
    information in the current pixel compared to say five pixel away. mrf handles that pretty well (maybe too well, I
    suspect that it is most of the time just copying instead of learning).

    I then tested semantic masks
    mrf failed when I simply append the mask to each layer. I think it is because the magnitude of the image (0~255)
    and the vgg layers are different, so nearest neighbor didn't work so well (that's why the author said it is ok to
    simply pass the mask into the vgg as an image. Since this will guarantee that they have the same magnitude as
    all other feature layers.
    I tried to dot the feature layers with the mask in mrf. That also worked, but then the gram matrix and the nn
    convolution becomes pretty large. Both make sense though. Dotting means that: each semantic mask is independent to
    other layers: the style of the eye has nothing to do with the style of say the hair. On the other hand, appending
    means that: it is still possible in nearest neighbor that: although one pixel belongs to another semantic mask,
    it can use the style of the other semantic mask if vgg thinks it is similar enough to another pixel... Considering
    the inaccuracy of human labeling and the ram, the second one is more preferrable. #TODO: I will make the change
    and basically delete resizing. (before that, I need to test if appending is really preferrable to the dot product.)

    Another problem is: Since we can't use nn in feed-forward nn (It's probably too hard for a generator network to
    learn nn), is there a better loss function or any modification I can make to the gram loss function? I came up with
    the "gram stack" and showed that it can reconstruct local structures to a good extent, given enough stacks. Can I
    use that? (like have stack = 5, 2 left, 1 center and 2 right?)

1. Fix resizing/dot/feed into network and append. Compare the three throughly.
2. Find alternative loss function that gets the good side from both gram and mrf.

Tasks not done yet:
1. pyramid generator... still doesn't work. it should be something simple that I'm missing
2. Johnson network does not work with masks.
3. A better loss function.

2016/12/14

1. Johnson network:
    I looked at the network itself. THe first layer has a filter of size 9. That is huge. That's probably why the
    result looks so blurry. And that size is the limit to the size of the features. Notice the image it generated
    (without masks) when I train it for texture generation. The features are always of a certain size, not larger nor
    smaller. That is probably one limit of the network.
    I also guessed why the network no longer works when I added masks. The input I used was noise + masks, basically
    appending the masks as additional features. That didn;t work and I guessed that it is probably too hard for the
    network to learn the dot product. So I thought I can just do it for the network. So now the input would be
    dot(noise, masks) and the dimension would be W x H x (3 * num_masks). Let me try if that works.

    So far it looks a lot better than before... With more training it can probably get even better.
    Uh. it's sort of working but not really. It just looks ugly as hell and the houses layer doesn't look like houses
    at all.

2. Alternative loss function.
    I modified the "gram_stack" so that it now calculates the gram between the original layer and the shifted layer
    where the shift is applied within a window (so if it's a 3x3 window, there will be 9 gram matrices)
    This works, to some extent. The image is definitely better than before. Now we can clearly see the houses and the
    doors whereas before it was just a blur. The feature size is limited to the size of the shift though...

    Also a side note, this might be way down the road, but how do we make sure the sky looks consistent? I mean the
    swirls are going everywhere in different directions, but in original graph they are consistent in one direction.
    Should I do something like add tv loss to the feature layers? ( Not just the final output) But Again that might not
    be a good idea if the feature layers are more like an edge detector (it would have huge tv loss). It's worth trying
    though.

2016/12/15
1. Johnson network:
    The network just doesn't behave like what it should be doing. That's why the style generated looked so poor. When I
    feed in different masks, the output doesn't look like it contains only the content that is supposed to be in that
    mask. Also the output changes when I add say a bunch of zeros in the upper left corner. As I mentioned before, it
    seems like the mask is just remembering whatever is not inside the mask. That's why when I give it zeros, it will be
    filled with the texture not in the mask, instead of just blank.

2. Alternative loss function.
    I compared the style generated by mrf versus my new loss function. It seems that mine is overfit on the small
    details. I thought maybe by changing the style layer that we're fitting it to, the result can get better.
    I did it. By changing the style layers to the one mrf uses (relu3_1 and relu4_1), and by changing shift size to 4,
    I got a much better style regeneration than the previous ones. It was able to recover some of the objects in the
    original style image. One small problem is: the layout seems to resemble the layout of the original style image
    a little (not noticeable unless you pay attention to it). Like the buildings are always on the left side,
    resembling the original style image.
    Tried to fix normalization but ended up using the current one.. It works good while the other ones don't.

    I want to try this on content images. Style/texture regenration was successful.
    Oh shit I was using the wrong masks.. I also need to increase the weight of the styles.

3. Pyramid genration network.
    It seems like the author did not use the pyrammid generation network in his github. He used a conv-deconv model,
    similar to Johnson, but with small modifications. He added a skip layer and appended those after each conv step.
    He also added a lot of noise layers along with each skip layer. The downsampling layers have 8 layers but the
    noise layers have 16...

    That one is guaranteed to work, but I still would like to figure out why mine didn't. Let me try to increase the
    number of noise layers... Don't know if that will help.
2016/12/16
Alternative loss function
The loss with respect to the shift size is kind of wierd...
6:1.80673e+08
5:1.41603e+08
4:1.05345e+08
3:7.28106e+07
2:4.48457e+07
1:2.26033e+07
I guess if we keep the shift size constant, it's not a big problem.
Yesterday when I compared my new loss with the old one, I think I had some bugs
and I should compare them again.

My implementation results in image that is too detailed. (No interesting features on
a grand scale, just local ones). I then tried to change the lr and it worked... I don't know why
but the features now became of a larger scale. What I mean is the brushes looks larger and more
coherent. This may be just an accident, because I don't think the system is designed that way. I need
to find a way to make coherent brushes.

I tested my new loss function. Although there is noticeable difference when I do texture generation,
when I added the content image, there was essentially no difference between the new loss function and
the gram loss. Maybe that is because we're using layer 1-5 and layer 1 is just too detailed? (So if we fit a layer that
looks almost like the original image, then the result would look like the original image as well...) I'll try the
layer 3-5.
... Nah it is just hard to tell the difference. The old problem still exists and the variance in the result each time
is just too large. The biggest problem is still the inability to recognize objects and paint them accordingly. If I
want to paint the sky in some way, I don't want to paint the ocean in the same way as well (unless I want to do so
intentionally) because now the person will have no idea where the boundary is. But that can be solved by adding masks,
and the more detailed the mask is, the better the painting would be. One question would be: what if the thing i want to
paint is not in the style image? Using neural network sort of solves that image but using masks won't. Also relying on
a single image almost guarantees that you can't draw something that is not in that image.
So the solution I can think of:
1. Use a hierarchy of objects. If I want to paint a human, but there is no human in the kb, we find the next closest
level: mammals, animals etc and paint the human like that. But that would require a kb of how to draw different things
as well as their semantic relations.
2. Enable training on lots of masked images, not just one. This is essentially building a kb. But I don't know how to
even train one feed-forward network on one masked image yet... So this now becomes my priority...


Feed-forward texture generation with masks.
    I read the source code in torch and compared against my code. One big difference I found was that it had one gram
    matrix for each coloring. I had one huge gram matrix for all the coloring. Theoretically mine should not affect the
    result (because no color overlaps so most of the gram matrix would be 0), but I'm going to rewrite mine just in
    case.

    Link to the torch source code: https://github.com/DmitryUlyanov/fast-neural-doodle/blob/master/fast_neural_doodle.lua

2016/12/17
1.Testing gramian_with_mask
    It works fine in back-propagation mode.
    The feed forward is still not working after training for 5000 rounds. This is fustrating...
    check the result for texture generation without masks and contintue training? It seems to be working. More
    training will result in better result.
    I will code the new feed-forward network and hope that saves the day.
2. New loss function.
    Ok why my loss function didn't work? It worked perfectly without masks. Then it starts to break down a little after
    I added the masks. Then it completely becomes the old gram loss after I added the content images. What happened in
    between?
3. Refactoring the code: Went till line 224 in n_style_feedforward_net.


2016/12/18
1. Testing gramian with mask
    Continued training from yesterday.
    Hum.. the network also sort of failed... The result just looks super wierd.
    Trying to repeat the experiment using content images...
    Ok The skip_noise_4 network is working. At least it is working when it generates the style without masks or content
    images. *

    Conclusion:
        johnson network sort of works to generate textures. The boarder looks wierd though and the features are small,
        probably because I trained using 128x128 instead of 256x256.
        TODO: Train using 256x256 without masks or content.

        skip_noise_4 generate textures successfully (tested two times by accident)
2. New loss function.

2016/12/19
1. Testing feed-forward with mask
    I don't understand what's going on. The new skip_noise_4 network now had the same type of errors as johnson. The
    masks seem to switch from training to training. The network applies this texture to this network during this round
    and the other texture to the other network in the next round... I'm really lost and I don't understand what is
    going on.

    Tried to have input mask be constant... Still the texture generation didn't work. That doesn't make sense...

    Style generation seems to work. But it looked different from the ones online. Maybe the style loss was too low.
    I set it to 25 because 100 seemed too high. Plan: test using style weight 50.

    I guess it's time to download some repos online and see what they have.

    No online repo found on fast neural doodle, except for the Dmitry one. I am planning to read his whole source code.
    I found that the noise he feeds in has three parts: mask, mask times uniform noise, and noise. I only fed in noise.
    That is fed into the network's very first layer.. He also have some additional noises in skip layers. I'm not sure
    how those work yet. I assume that they're all uniform random.

    Still didn't work after 30000 iterations... GG. There;s something else to the network.


2. New loss function
    No work done.

3. Back prop MRF with style as real images.
    According to https://github.com/jcjohnson/neural-style/wiki/Similar-to-Neural-Style it seems to work well when
    style is real images. Trying that... That worked pretty well when the style weight is kept low and when the content
    and the style image have similar contents (Mostly because then you can't tell if I replaced one object in the
    content with another object in the style image. Again, mrf does mostly copying but not learning.)

    Trying to stylize some other image. I found that there were some wierd horizontal lines in the generated image.
    I may need to fix that. I first thought that it was because of the resized style image. Then I resized them
    manually and the problem was still there. So it must be caused by something else. Maybe the dimensions were not
    divisible by 2?

4. Future direction
    I was thinking about what to do next. I thought before that I can train a feed-forward network on multiple style
    images so that it can draw a wider range of stuff that might not be present in one single style image. But then I
    realized that there are limits to the masks. I mean you can say "A man facing the sun" is a kind of mask and "a
    man facing away from the sun" is another kind of mask. But then that requires too many masks in total. So I need
    some extra parameters to train... I need a semantic tree and I need a smarter way to represent the same object in
    different environments.

    Maybe also reimplement multiple styles feed-forward. But that's not so necesssary.


2016/12/20
1. Feed-forward with mask
    Only works on the boarder line. Don't know why.

2016/12/21
1. Feed-forward Style genration with johnson network.
    style weight = 50 seems to work fine.

2. feed-forward with mask
    I thought it was working... I replaced .jpg with .png masks and at first it seemed to be working... Then it just
    stopped becoming better after round 10000. The sky looks a little bit like the given style but the rest didn't
    look anything like the original style image. I think I've met the same problem before when using johnson network.
    So it has nothing to do with the network or the inputted mask images. Interesting. I tested feeding in masks
    where only one mask is 255 and all others are 0. It outputted the same result as before when I used johnson
    network...

    If this still didn't work... I don't want to get stuck at the same place for a week. So I'm going to move on.
    Maybe I should change the output so that it has not 3 channels, but 3 * num_masks channels and I manually dot
    the output with each mask and combine the final result.
    ...
    Wait, how do I calculate the loss again? I dot the gram matrix with each mask... Hum... There must be a problem
    there. That's one of the two possiblity. The other one is I'm not feeding in the right noise.
    Nah there can't be a problem there because I'm using the same loss function in the slow version.

    I just realized that the version online didn't use any other input except the mask themselves! The code is there
    to support additional noises and noise dot mask but he didn't use them (he set the parameters to 0)... gg


3. Back prop slow version
    I want to try the non-mrf version and compare them. The biggest problem with mrf is: it is too aggressive and it
    does not recognize objects well. It is agressive because it usually either does not change the object at all or
    completely changes it to another objecct in the style image. It does not recognize objects well because, well, the
    limits in vgg network. It does a poor job recognizing, say a black contour of a human compared to that of a tree.
    We're helpless unless we label each part. But then most of the time the images are too complicated to mark what
    objects are in there.
4. Future direction
    Since the feed-forward part is not going so well, I'm thinking things I can do with the slow version.
    I thought of attention network. So I paint one area at a time or something like that. That can replace the need of
    masks because masks are essentially manually added information on object recognition (which can be done by the
    network itself in theory if the network is good enough.)
    Now the bottle neck is that the vgg can't recognize objects well, and if it can't do that, I don't have any way to
    improve the current model except to feed in manually labeled data...

2016/12/22
2. feed-forward with mask
    I noticed that I did not normalize the gram matrix by the mean of mask values. maybe that is causing the problem
    (how do you expect two gram to be the same when you don't even know the number of pixels marked by the mask?)
    That's why the feed-forward network works without masks...
    Yep it's finally working... Plan: I still need to change the input back. The model probably isn't using the extra
    input anyway.

2. Future direction
    I thought about the style of an image. What does that mean? To me its meaning is two-fold. The first one is local
    statistic relations. This one is well captured by the gram matrix. The other one is more logical. If I always paint
    a man's face like that, then it must have some logic behind it (or not) and we can also call that "style". That's
    why the current neural style need a 'content image' whereas in real life people don't need a real image and make
    change on that. One is more on a small scale and the other one on a grand scale.

    What I can then improve? I implement removing the grid-like noises in the generated image. That has been done
    before.

    I can improve the gram loss so that it is no longer just on a local scale. That had some progress but I stopped. I
    should maybe continue that. I need to understand the gram matrix more. I mean I know how mathematically works, but
    why it works? I have no idea.

    I had an idea of how to generate masks for any image (as long as it is moving). There are object tracking
    techniques that can mark one object in a stream of images. Ah I can think about this later. It depends largely on
    object tracking. *

     I know my final goal: to make programs that can draw like humans, with some instruction from human.

    TODO: further test my new loss function. Make sure I understand why and when it works or doesn't work. 