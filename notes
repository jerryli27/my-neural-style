This is the notebook for recording experiment results, discoveries, and next steps.

It is crucial to have a low enough learning rate. If you have a high one and decrease it over time, it still won't work.

MRF is confirmed not to work with feed forward neural network. But mrf is not really absolutely necessary. I might need
it for semantic related masking stuff. But it has also been shown that masking works with gram loss and feed forward network.

Here are two unsolved problems:
The size of the features/styles. The image genearted when input is 100 x 100 versus 1000 x 1000 is drastically different.
The generated image is limited to the structure of the original painting. In art, you can draw something larger or
smaller than usual to achieve some effect. There's no such thing here because the loss is localized
(You can't just change the absolute position of objects in content image.)


Two ideas: artists "zoom in" to a spot when they want to paint it in more detail. Is it possible to do so in our architecture?

Second idea: if the generator network can generate styled images, why can't it be used to reformat the feature layers?
Turn the feature layers into something that look like other artforms (without loosing too much originality)
and generate ... But how's that different from just use the generator network to generate the image? Not really

I want something other than difference squared loss. I thought about nearest neighbor loss of the normalized feature
layers. That one is invariant to translation.


Trying two things now: one is to directly observe what is going on in each loss function for each feature layer.
Another is to feed multiple styles into the network while modifying the same parameters.

simply feed 15000 shirobako images as style didn't work.

I tried the mask again using hand-segmented shirobako mask (mrf loss). Surprisingly it's sort of working. Maybe I don't need the
content loss. I can just create a drawing from scratch. Maybe that would be too hard.
One more difficulty if I choose to go on this direction. I don't have hand-segmented masks for me to train.
Even if I do, I don't know how to incorporate say 10 masked style images together. Simple nearest neighbor matching on
the 10 images would require 10x memory in the gpu.

One idea for automatically label images: start with example image and a hand-labeled mask. Now for each input
we compute the nearest neighbor of say conv4-2 (any high level layer) and assign the nearest neighbor's mask
to the input layer. Then add the constraint that nearby pixels should preferrably have the same label.
Then do deconv to get pixel-wise label for the original image. (Sounds like a plan, but this requires the
high level layers of the style and content to be similar, which may well not be the case. 20161122)

Texture and placement of objects are two different things. Texture is given a mask, how to fill in the colors
so that the style loss is the least. That is already solved. Placement of object is another issue. How to
place objects relative to each other so that it is the most probable.

I tried the nn loss. NN is not differentiable so it has no gradient. I should've realized earlier. Now I probably have
to get around this problem.

One thing is, the reason why mrf can perform better is because it has those patches that captures info about nearby
cells. Gram simply multiply them together and add all cells. There's no interaction between a cell and the cell on its
side. On the otherhand, the mrf is expensive to calculate because of the nn step.

After some thinking, it comes down to knowing the feature layers first. (like each of the conv layers, when they're
activated and compare two inputs, content and style, see when their conv layers look similar.)

Adversarial generative network may be worth investigating into. But don't do that just yet. I still have to finish up
this experiment.

We can make the additional semantic masks learnable. Just add a loss for too much deviation from original (more
complicated loss can be added later)

Things learned from the experiment: the conv layer won't look close to the original image. So we can't hope to morph
one image into another in the conv layers. Also, making semantic masks learnable will need further refinement.

I thought maybe finding the spatial correlation of semantic layers might be helpful. Now it is only finding the
correlation in-place (that is, the same pixel times the same pixel in another layer). What happens if we shift it by
say half of the width and find the correlation? The hope is that , for example we have two eyes, one on the left
and one on the right. By finding the correlation after shifting, we may find: ah whenever there's an
eye here, there will be on on its right with distance half screen away. That's my hope.


I was correct that correlation after shifting one layer encodes the relative positional information. Now the problem is
1. It was not perfect. I hope the problem can be solved after adding a content loss.
2. It was spatially too similar to the style image. ie. no shifting etc. There was some shift for subfeatures, like
the mouth was on the right first and slowly came to the center. But overall the head position was still the same, the
position of everything was the same as in the style. This is not what we want. We just want to modify the content image
a little so that the eyes become bigger, or the nose become less noticeable and things like that.
I want to have two very simple images as input and see how it goes.

Not going well so far.

Side projects during thxgiving:
Run multi-style feed forward network.
Shit.. I realized it actually doesn't make sense in the current frame work. I have to separate each style by itself
instead of feeding them in as batches. ( Otherwise I can't set the image placeholder so I can't train scale and offset
individually. I'll try to fix this tomorrow by going back to the master branch and merge the two..
I don't know if I should keep the batch style... Maybe I should.

Tested overnight on claude monet's paintings. Now I'm sure I can't just merge styles like that. It doesn't work.

Other future directions: feed forward neural doodle.


As I think of it, maybe we don't need the content image at all? There are many things that are hard to find in real life
but exist in drawings. I know that a feed forward network with masks can be trained as long as I have enough training
data. The only problem is where to get that semantically labeled training data. I can get that from 3d model... I think
I can look into that direction. No content image. Content loss comes directly from thousands of style images. Details
of how content loss can come from those may be filled in later but I think this is the right direction. We don't have
to worry about distorting content image into the style image, which is a big challenge.

Before that, let me modify the current code so that it can accept only semantic style images and semantic masks and
learn them using feed-forward nn. the loss... we can use content loss or style loss. try both I guess. I need to
copy the previous code into a new file because the change will make the code incompatible with the previous version.
(since there no longer need to be a content image). Actually, I can use the same file... Since I may use that framework
to learn auto generating textures.

Basically I will implement feed forward neural doodle, but better than what they've done. I need not only rgb mask, but
any number of masks for each style image.

2016/12/09
I implemented fast neural doodle. Testing it. It is running super slow for some reason... I don't get it. I should test
running the normal version... (without masks and doodles.)

2016/12/10
Fixed some bugs, but the thing is still super slow, even if I don't turn on masks. Was it like that from the very
beginning? I think so because it takes the whole night to train around 20000 rounds, so it's like one round per second.

Also the loss for style is unnaturally high only after I apply masks. I found that was because I was doing dot product
so each number is dotted with 255. lol.

But the result was still unsatisfactory.

2016/12/11
What can I do that will have some originality instead of repeating what people has done? What is the limit of the
current system?

If working on masked images:
    You must first have a mask, whether it is based on real images or not.
    The mask cannot be too detailed, or it will use up too much memory (grows linearly) and time it takes to create
    such a mask is prohibitive.

If not working on masked images:
    The model clearly does not have an idea of scale. If you train it on a 256x256 image, you can't just put that
    on a 1024x1024 image because the patterns created will be too small.
    It has no understanding of the relative size/location of objects (unless the two objects are close enough that they
    become in the same cell after convolution. That is a problem for vgg, but since we're using gram matrix, the
    problem is also applicable in style generation. For this reason, it is still impossible to tell it to "paint a cat"
    and expect it to just work.
    It also has no idea about different style between different objects. See comparison between mask and no mask.
    It

--
The thing is not working, at least not on johnson network... I don't know what's going on, but my guess is that johnson
might not be the best choice for texture generation with masks. I'll try the pyramid network...

Planning to try pyramid network
read blog and source code on feed forward network and see why mine did not work.
The johnson feed forward network is definitely working. It just takes a while to train. It get's faster(to a tolerable
level) when I decrease the batch size from 8 to 1. I'll try that on the masks.

2016/12/12
on going work:
Trying pyramid network without masks. Just normal content input.
Planning to try johnson network with style generation only. No masks. -- It worked!

I'll try to replace input to "johnson network with masks" with random
noise instead of the masks themselves. It probably will get better after intensive training... Usually generating image
with random content/mask takes a while to train. Ok there's definitely something wrong with the masks. The masks
changed their meanings in the middle of training, so we're definitely loading them wrong. If I'm lucky, it might just
be the output but not the training (judging from the quality of generated images).
I tested the output. The output's order is correct. So it's the training process... But I don't get what part went
wrong. I tested getting the files. That part was correct. ... TODO: debug this. I don't know what it will look like
after more training, but now it's not so satisfactory yet.
It is interesting to see how each mask acts. when value of a mask is 0, it doesn't mean that it will be black. Instead
since it indicates that it is not a certain mask, it becomes the color of other masks.
But when only one mask has all 255s and the rest 0s, the result just becomes the same as no masks for some reason..
I don't understand why, but it seems the network is doing something wierd.


Also working on finding out why pyramid generator did not work so well. The image generated was just too blue...
I first thought it might be that I forgot to preprocess the image or something like that. But I checked and it's fine.
So I thought it might be the generator net. I want to test that with simple "style-only" task. That failed ... old
problem.

2016/12/13
Re-reading the feed-forward paper and trying to figure out why my network did not work.
Differences:
    1. They used a uniform noise distribution instead of the whatever distribution numpy offers. (fixed, changed all to
    uniform random)
    2. according to the original paper, normalization is done also before concatenation layer. (while in another paper
    it says otherwise. Personally I think it is not necessary to have normalization there, but I want to at least
    reproduce the result of the author. (fixed)
    3. Initializations (fixed)

Humm... Where's the problem???
Maybe it's not in the generator network??
Alright I give up... I don't know why it doesn't work. Pushing this off to tomorrow.

Task 2: Johnson network with masks:

Task 3: improving slow version.
    I was comparing the texture generation network (with and without mrf, with and without masks). I found that the key
    lies in the loss function, ***so if I want to make something new, I need to invent a new loss function. ***

    I tested style generation (no content image) for mrf loss and gram loss. It turns out that both are effective,
    although mrf loss is better at preserving local structure and information. gram loss knows little about the
    information in the current pixel compared to say five pixel away. mrf handles that pretty well (maybe too well, I
    suspect that it is most of the time just copying instead of learning).

    I then tested semantic masks
    mrf failed when I simply append the mask to each layer. I think it is because the magnitude of the image (0~255)
    and the vgg layers are different, so nearest neighbor didn't work so well (that's why the author said it is ok to
    simply pass the mask into the vgg as an image. Since this will guarantee that they have the same magnitude as
    all other feature layers.
    I tried to dot the feature layers with the mask in mrf. That also worked, but then the gram matrix and the nn
    convolution becomes pretty large. Both make sense though. Dotting means that: each semantic mask is independent to
    other layers: the style of the eye has nothing to do with the style of say the hair. On the other hand, appending
    means that: it is still possible in nearest neighbor that: although one pixel belongs to another semantic mask,
    it can use the style of the other semantic mask if vgg thinks it is similar enough to another pixel... Considering
    the inaccuracy of human labeling and the ram, the second one is more preferrable. #TODO: I will make the change
    and basically delete resizing. (before that, I need to test if appending is really preferrable to the dot product.)

    Another problem is: Since we can't use nn in feed-forward nn (It's probably too hard for a generator network to
    learn nn), is there a better loss function or any modification I can make to the gram loss function? I came up with
    the "gram stack" and showed that it can reconstruct local structures to a good extent, given enough stacks. Can I
    use that? (like have stack = 5, 2 left, 1 center and 2 right?)

                        # TODO: change it back, or make sure it is better than just concatenating.
        # TODO: don't forget to change it back.


TODO list for tomorrow:
1. Fix resizing/dot/feed into network and append. Compare the three throughly.
2. Find alternative loss function that gets the good side from both gram and mrf.

Tasks not done yet:
1. pyramid generator... still doesn't work. it should be something simple that I'm missing
2. Johnson network does not work with masks.
3. A better loss function.

2016/12/14

1. Johnson network:
    I looked at the network itself. THe first layer has a filter of size 9. That is huge. That's probably why the
    result looks so blurry. And that size is the limit to the size of the features. Notice the image it generated
    (without masks) when I train it for texture generation. The features are always of a certain size, not larger nor
    smaller. That is probably one limit of the network.
    I also guessed why the network no longer works when I added masks. The input I used was noise + masks, basically
    appending the masks as additional features. That didn;t work and I guessed that it is probably too hard for the
    network to learn the dot product. So I thought I can just do it for the network. So now the input would be
    dot(noise, masks) and the dimension would be W x H x (3 * num_masks). Let me try if that works.

    So far it looks a lot better than before... With more training it can probably get even better (TODO: don't forget to continue training, but first change the gram back to original one...)

2. Alternative loss function.
    I modified the "gram_stack" so that it now calculates the gram between the original layer and the shifted layer
    where the shift is applied within a window (so if it's a 3x3 window, there will be 9 gram matrices)
